<Type Name="SpeechRecognizer" FullName="System.Speech.Recognition.SpeechRecognizer">
  <Metadata><Meta Name="ms.openlocfilehash" Value="03a2ecf66a486d776bd07181a5aa2607aa7004ff" /><Meta Name="ms.sourcegitcommit" Value="055a4a82a0b08bfbdc21bd1347fb71f7fe2c099e" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="pl-PL" /><Meta Name="ms.lasthandoff" Value="08/15/2019" /><Meta Name="ms.locfileid" Value="69258126" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognizer extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognizer" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognizer&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognizer = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Zapewnia dostęp do udostępnionej usługi rozpoznawania mowy dostępnej na pulpicie systemu Windows.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aplikacje używają udostępnionego aparatu rozpoznawania do uzyskiwania dostępu do funkcji rozpoznawania mowy systemu Windows. Użyj obiektu <xref:System.Speech.Recognition.SpeechRecognizer> , aby dodać do środowiska użytkownika funkcji rozpoznawania mowy systemu Windows.  
  
 Ta klasa zapewnia kontrolę nad różnymi aspektami procesu rozpoznawania mowy:  
  
-   Aby zarządzać gramatykami rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A>,,, i <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A>.  
  
-   Aby uzyskać informacje o bieżących operacjach rozpoznawania mowy, zasubskrybuj <xref:System.Speech.Recognition.SpeechRecognizer> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>zdarzenia, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> .  
  
-   Aby wyświetlić lub zmodyfikować liczbę alternatywnych wyników zwracanych przez aparat rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> właściwości. Aparat rozpoznawania zwraca wyniki rozpoznawania w <xref:System.Speech.Recognition.RecognitionResult> obiekcie.  
  
-   Aby uzyskać dostęp do stanu udostępnionego aparatu rozpoznawania lub monitorować go, należy <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A>użyć <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>właściwości <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated> <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>,,,, <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> i i, <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred> <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged> i<xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> zdarzenia.  
  
-   Aby zsynchronizować zmiany w aparacie rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody. Współużytkowany aparat rozpoznawania używa więcej niż jednego wątku do wykonywania zadań.  
  
-   Aby emulować dane wejściowe do udostępnionego aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> metod i. <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A>  
  
 Konfiguracją funkcji rozpoznawania mowy systemu Windows jest zarządzanie przy użyciu okna dialogowego **Właściwości mowy** w **Panelu sterowania**. Ten interfejs służy do wybierania domyślnego aparatu i języka rozpoznawania mowy na komputerze, urządzenia wejściowego audio i zachowania uśpienia funkcji rozpoznawania mowy. Jeśli konfiguracja funkcji rozpoznawania mowy systemu Windows jest zmieniana, gdy aplikacja jest uruchomiona, (na przykład jeśli funkcja rozpoznawania mowy jest wyłączona lub język wejściowy zostanie zmieniony), zmiana wpłynie na <xref:System.Speech.Recognition.SpeechRecognizer> wszystkie obiekty.  
  
 Aby utworzyć aparat rozpoznawania mowy w procesie, który jest niezależny od funkcji rozpoznawania mowy systemu <xref:System.Speech.Recognition.SpeechRecognitionEngine> Windows, należy użyć klasy.  
  
> [!NOTE]
>  Zawsze wywołuj <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> przed wydaniem ostatniego odwołania do aparatu rozpoznawania mowy. W przeciwnym razie używane zasoby nie zostaną zwolnione do momentu wywołania `Finalize` metody aparatu rozpoznawania elementów bezużytecznych.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która ładuje gramatykę rozpoznawania mowy i pokazuje asynchroniczne emulowane dane wejściowe, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy.  Jeśli funkcja rozpoznawania mowy systemu Windows nie jest uruchomiona, uruchomienie tej aplikacji również spowoduje uruchomienie funkcji rozpoznawania mowy systemu Windows. Jeśli rozpoznawanie mowy systemu Windows jest w **** stanie uśpienia, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
  </Docs>
  <Members>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognizer ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognizer();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inicjuje nowe wystąpienie klasy <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> klasy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy <xref:System.Speech.Recognition.SpeechRecognizer> obiekt obsługuje oddzielny zestaw gramatyk rozpoznawania mowy.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która ładuje gramatykę rozpoznawania mowy i pokazuje asynchroniczne emulowane dane wejściowe, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy. Jeśli funkcja rozpoznawania mowy systemu Windows nie jest uruchomiona, uruchomienie tej aplikacji również spowoduje uruchomienie funkcji rozpoznawania mowy systemu Windows. Jeśli rozpoznawanie mowy systemu Windows jest w **** stanie uśpienia, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera format dźwięku odbieranego przez aparat rozpoznawania mowy.</summary>
        <value>Format wejściowy audio dla aparatu rozpoznawania mowy lub <see langword="null" /> dane wejściowe do aparatu rozpoznawania nie są skonfigurowane.</value>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera poziom audio otrzymywany przez aparat rozpoznawania mowy.</summary>
        <value>Poziom audio danych wejściowych aparatu rozpoznawania mowy, od 0 do 100.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy udostępniony aparat rozpoznawania raportuje poziom wejścia audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania wywołuje to zdarzenie wiele razy na sekundę. Częstotliwość, z jaką zdarzenie jest zgłaszane, zależy od komputera, na którym jest uruchomiona aplikacja.  
  
 Aby uzyskać poziom audio w czasie zdarzenia, należy użyć <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> właściwości skojarzonej. <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs> Aby uzyskać bieżący poziom audio danych wejściowych do aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> właściwości aparatu rozpoznawania.  
  
 Podczas tworzenia delegata dla `AudioLevelUpdated` zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład dodaje procedurę obsługi dla `AudioLevelUpdated` zdarzenia <xref:System.Speech.Recognition.SpeechRecognizer> do obiektu. Program obsługi wyprowadza nowy poziom dźwięku do konsoli programu.  
  
```csharp  
private SpeechRecognizer recognizer;  
  
// Initialize the SpeechRecognizer object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
    new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera bieżącą lokalizację w strumieniu audio generowanym przez urządzenie, które dostarcza dane wejściowe do aparatu rozpoznawania mowy.</summary>
        <value>Bieżąca lokalizacja w strumieniu wejściowym audio aparatu rozpoznawania mowy, za pomocą której odebrano dane wejściowe.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Udostępniony aparat rozpoznawania otrzymuje dane wejściowe podczas działania funkcji rozpoznawania mowy na komputerze.  
  
 `AudioPosition` Właściwość odwołuje się do pozycji urządzenia wejściowego w wygenerowanym strumieniu audio. Z kolei <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> Właściwość odwołuje się do pozycji aparatu rozpoznawania w przetwarzaniu danych wejściowych audio. Te pozycje mogą być różne.  Na przykład, jeśli aparat rozpoznawania odebrał dane wejściowe, dla których nie Wygenerowano jeszcze wyniku rozpoznawania, wartość <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> właściwości jest mniejsza niż wartość <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> właściwości.  
  
   
  
## Examples  
 W poniższym przykładzie współużytkowany aparat rozpoznawania mowy używa gramatyki dyktowania, aby dopasować dane wejściowe mowy. Program obsługi <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> zapisuje zdarzenia w konsoli programu <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>i <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> , gdy aparat rozpoznawania mowy wykrywa mowę na wejściu.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add handlers for events.  
      recognizer.LoadGrammarCompleted +=   
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
      recognizer.SpeechRecognized +=   
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
      recognizer.SpeechDetected +=   
        new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load the grammar object to the recognizer.  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Recognizer audio position: " + recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Write the name of the loaded grammar to the console.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje po napotkaniu przez aparat rozpoznawania problemu w sygnale audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać ten problem, użyj <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> właściwości skojarzonej <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>klasy.  
  
 Podczas tworzenia delegata dla `AudioSignalProblemOccurred` zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie zdefiniowano procedurę obsługi zdarzeń, która zbiera informacje o `AudioSignalProblemOccurred` zdarzeniu.  
  
```  
private SpeechRecognizer recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera stan dźwięku odbieranego przez aparat rozpoznawania mowy.</summary>
        <value>Stan wejścia audio do aparatu rozpoznawania mowy.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje po zmianie stanu w dYwięku odbieranym przez aparat rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać stan dźwięku w momencie zdarzenia, należy użyć <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> właściwości skojarzonej. <xref:System.Speech.Recognition.AudioStateChangedEventArgs> Aby uzyskać bieżący stan audio danych wejściowych do aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> właściwości aparatu rozpoznawania. Aby uzyskać więcej informacji na temat stanu audio, <xref:System.Speech.Recognition.AudioState> zobacz Wyliczenie.  
  
 Podczas tworzenia delegata dla `AudioStateChanged` zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie użyto procedury obsługi dla `AudioStateChanged` zdarzenia, aby napisać aparat rozpoznawania nowy <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> do konsoli <xref:System.Speech.Recognition.AudioState> przy każdej zmianie przy użyciu elementu członkowskiego wyliczenia.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the recognizer into Listening mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        Console.WriteLine();  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognizer" /> Usuwa obiekt.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognizer.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognizer" /> Usuwa obiekt.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognizer.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing"><see langword="true" />Aby zwolnić zasoby zarządzane i niezarządzane; <see langword="false" /> do zwolnienia tylko zasobów niezarządzanych.</param>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognizer" /> Usuwa obiekt i zwalnia zasoby używane podczas sesji.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuluje dane wejściowe do udostępnionego aparatu rozpoznawania mowy, korzystając z tekstu zamiast audio do synchronicznego rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody pomijają dane wejściowe dźwięku systemowego. Może to być przydatne w przypadku testowania lub debugowania aplikacji lub gramatyki.  
  
> [!NOTE]
>  Jeśli funkcja rozpoznawania mowy systemu Windows jest **** w stanie uśpienia, te metody `null`zwracają.  
  
 Współużytkowany <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>aparat rozpoznawania wywołuje zdarzenia, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> , tak jakby operacja rozpoznawania nie jest emulowana. Aparat rozpoznawania ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako literały.  
  
> [!NOTE]
>  Obiekt wygenerowany przez udostępniony aparat rozpoznawania w odpowiedzi na emulowane dane wejściowe ma `null` wartość dla <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> właściwości. <xref:System.Speech.Recognition.RecognitionResult>  
  
 Aby emulować rozpoznawanie asynchroniczne, użyj <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Dane wejściowe dla operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie frazy w udostępnionym aparacie rozpoznawania mowy przy użyciu tekstu zamiast dźwięku do synchronicznego rozpoznawania mowy.</summary>
        <returns>Wynik rozpoznawania dla operacji rozpoznawania lub <see langword="null" />, jeśli operacja nie powiedzie się lub funkcja rozpoznawania mowy systemu Windows jest w stanie **uśpienia** .</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują wielkość liter i znaki w przypadku stosowania reguł gramatycznych do frazy wejściowej. Aby uzyskać więcej informacji na temat tego typu porównania, zobacz <xref:System.Globalization.CompareOptions> wartości <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> wyliczenia i <xref:System.Globalization.CompareOptions.IgnoreWidth>. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy oraz traktują znaki interpunkcyjne jako literały.  
  
   
  
## Examples  
 Poniższy przykład ładuje przykładową gramatykę do współużytkowanego aparatu rozpoznawania i emuluje dane wejściowe do aparatu rozpoznawania. Jeśli funkcja rozpoznawania mowy systemu Windows nie jest uruchomiona, uruchomienie tej aplikacji również spowoduje uruchomienie funkcji rozpoznawania mowy systemu Windows. Jeśli rozpoznawanie mowy systemu Windows jest w **** stanie uśpienia, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> zawsze zwraca wartość null.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        RecognitionResult result;  
  
        // This EmulateRecognize call matches the grammar and returns a  
        // recognition result.  
        result = recognizer.EmulateRecognize("testing testing");  
        OutputResult(result);  
  
        // This EmulateRecognize call does not match the grammar and   
        // returns null.  
        result = recognizer.EmulateRecognize("testing one two three");  
        OutputResult(result);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Output information about a recognition result to the console.  
    private static void OutputResult(RecognitionResult result)  
    {  
      if (result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Tablica jednostek programu Word, która zawiera dane wejściowe dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla emulowanej operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie określonych słów do udostępnionego aparatu rozpoznawania mowy, przy użyciu tekstu zamiast dźwięku do synchronicznego rozpoznawania mowy i określa, jak aparat rozpoznawania obsługuje porównanie Unicode między wyrazami a załadowanymi gramatykami rozpoznawania mowy.</summary>
        <returns>Wynik rozpoznawania dla operacji rozpoznawania lub <see langword="null" />, jeśli operacja nie powiedzie się lub funkcja rozpoznawania mowy systemu Windows jest w stanie **uśpienia** .</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda tworzy <xref:System.Speech.Recognition.RecognitionResult> Obiekt przy użyciu informacji podanych `wordUnits` w parametrze.  
  
 Aparat rozpoznawania używa, `compareOptions` gdy stosuje reguły gramatyki do frazy wejściowej. Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują <xref:System.Globalization.CompareOptions.IgnoreCase> przypadek, <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Jeśli wartość lub jest obecna. Aparaty rozpoznawania zawsze ignorują szerokość znaków i nigdy nie ignorują typu kana. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy i traktują znaki interpunkcyjne jako literały. Aby uzyskać więcej informacji na temat szerokości znaków i typu kana, <xref:System.Globalization.CompareOptions> zobacz Wyliczenie.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Fraza wejściowa dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla emulowanej operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie frazy do udostępnionego aparatu rozpoznawania mowy, przy użyciu tekstu zamiast audio do synchronicznego rozpoznawania mowy i określa, jak aparat rozpoznawania obsługuje porównanie Unicode między frazą a załadowanymi gramatykami rozpoznawania mowy.</summary>
        <returns>Wynik rozpoznawania dla operacji rozpoznawania lub <see langword="null" />, jeśli operacja nie powiedzie się lub funkcja rozpoznawania mowy systemu Windows jest w stanie **uśpienia** .</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania używa, `compareOptions` gdy stosuje reguły gramatyki do frazy wejściowej. Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują <xref:System.Globalization.CompareOptions.IgnoreCase> przypadek, <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Jeśli wartość lub jest obecna. Aparaty rozpoznawania zawsze ignorują szerokość znaków i nigdy nie ignorują typu kana. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy i traktują znaki interpunkcyjne jako literały. Aby uzyskać więcej informacji na temat szerokości znaków i typu kana, <xref:System.Globalization.CompareOptions> zobacz Wyliczenie.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuluje dane wejściowe do udostępnionego aparatu rozpoznawania mowy, korzystając z tekstu zamiast audio w celu asynchronicznego rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody pomijają dane wejściowe dźwięku systemowego. Może to być przydatne w przypadku testowania lub debugowania aplikacji lub gramatyki.  
  
 Współużytkowany <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>aparat rozpoznawania wywołuje zdarzenia, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> , tak jakby operacja rozpoznawania nie jest emulowana. Gdy aparat rozpoznawania ukończy asynchroniczne operacje rozpoznawania, zgłasza <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> zdarzenie. Aparat rozpoznawania ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako literały.  
  
> [!NOTE]
>  Jeśli funkcja rozpoznawania mowy w systemie Windows **** jest w stanie uśpienia, udostępniony aparat rozpoznawania nie przetwarza danych wejściowych i nie zgłasza <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> zdarzeń i pokrewnych, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> ale nadal wywołuje zdarzenie.  
  
> [!NOTE]
>  Obiekt wygenerowany przez udostępniony aparat rozpoznawania w odpowiedzi na emulowane dane wejściowe ma `null` wartość dla <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> właściwości. <xref:System.Speech.Recognition.RecognitionResult>  
  
 Aby emulować rozpoznawanie synchroniczne, użyj <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Dane wejściowe dla operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie frazy w udostępnionym aparacie rozpoznawania mowy przy użyciu tekstu zamiast dźwięku do asynchronicznego rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują wielkość liter i znaki w przypadku stosowania reguł gramatycznych do frazy wejściowej. Aby uzyskać więcej informacji na temat tego typu porównania, zobacz <xref:System.Globalization.CompareOptions> wartości <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> wyliczenia i <xref:System.Globalization.CompareOptions.IgnoreWidth>. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy oraz traktują znaki interpunkcyjne jako literały.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która ładuje gramatykę rozpoznawania mowy i pokazuje asynchroniczne emulowane dane wejściowe, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy. Jeśli funkcja rozpoznawania mowy systemu Windows nie jest uruchomiona, uruchomienie tej aplikacji również spowoduje uruchomienie funkcji rozpoznawania mowy systemu Windows. Jeśli rozpoznawanie mowy systemu Windows jest w **** stanie uśpienia, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Tablica jednostek programu Word, która zawiera dane wejściowe dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla emulowanej operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie określonych słów do udostępnionego aparatu rozpoznawania mowy, przy użyciu tekstu zamiast dźwięku do asynchronicznego rozpoznawania mowy i określa, jak aparat rozpoznawania obsługuje porównanie Unicode między wyrazami a załadowanymi gramatykami rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda tworzy <xref:System.Speech.Recognition.RecognitionResult> Obiekt przy użyciu informacji podanych `wordUnits` w parametrze.  
  
 Aparat rozpoznawania używa, `compareOptions` gdy stosuje reguły gramatyki do frazy wejściowej. Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują <xref:System.Globalization.CompareOptions.IgnoreCase> przypadek, <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Jeśli wartość lub jest obecna. Aparaty rozpoznawania zawsze ignorują szerokość znaków i nigdy nie ignorują typu kana. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy i traktują znaki interpunkcyjne jako literały. Aby uzyskać więcej informacji na temat szerokości znaków i typu kana, <xref:System.Globalization.CompareOptions> zobacz Wyliczenie.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Fraza wejściowa dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla emulowanej operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie frazy w udostępnionym aparacie rozpoznawania mowy, przy użyciu tekstu zamiast dźwięku do asynchronicznego rozpoznawania mowy i określa, jak aparat rozpoznawania obsługuje porównanie Unicode między frazą a załadowanymi gramatykami rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania używa, `compareOptions` gdy stosuje reguły gramatyki do frazy wejściowej. Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują <xref:System.Globalization.CompareOptions.IgnoreCase> przypadek, <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Jeśli wartość lub jest obecna. Aparaty rozpoznawania zawsze ignorują szerokość znaków i nigdy nie ignorują typu kana. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy i traktują znaki interpunkcyjne jako literały. Aby uzyskać więcej informacji na temat szerokości znaków i typu kana, <xref:System.Globalization.CompareOptions> zobacz Wyliczenie.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy udostępniony aparat rozpoznawania zakończy asynchroniczne operacje rozpoznawania dla emulowanej danych wejściowych.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każda <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> Metoda rozpoczyna asynchroniczne operacje rozpoznawania. Aparat rozpoznawania zgłasza zdarzenie `EmulateRecognizeCompleted` w momencie zakończenia operacji asynchronicznej.  
  
 Asynchroniczne operacje <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>rozpoznawania mogą podnieść zdarzenia, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> . <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> Zdarzenie jest ostatnim zdarzeniem, które aparat rozpoznawania zgłasza dla danej operacji.  
  
 Podczas tworzenia delegata dla `EmulateRecognizeCompleted` zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która ładuje gramatykę rozpoznawania mowy i pokazuje asynchroniczne emulowane dane wejściowe, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy. Jeśli funkcja rozpoznawania mowy systemu Windows nie jest uruchomiona, uruchomienie tej aplikacji również spowoduje uruchomienie funkcji rozpoznawania mowy systemu Windows. Jeśli funkcja rozpoznawania mowy w systemie Windows **** jest w trybie uśpienia, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=   
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="Enabled">
      <MemberSignature Language="C#" Value="public bool Enabled { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool Enabled" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberSignature Language="VB.NET" Value="Public Property Enabled As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool Enabled { bool get(); void set(bool value); };" />
      <MemberSignature Language="F#" Value="member this.Enabled : bool with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia wartość wskazującą, czy ten <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiekt jest gotowy do przetwarzania mowy.</summary>
        <value><see langword="true" />Jeśli ten <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiekt wykonuje rozpoznawanie mowy; <see langword="false" />w przeciwnym razie.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Zmiany tej właściwości nie wpływają na inne wystąpienia <xref:System.Speech.Recognition.SpeechRecognizer> klasy.  
  
 Domyślnie wartość <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> właściwości jest `true` dla nowo wystąpienia wystąpienia elementu <xref:System.Speech.Recognition.SpeechRecognizer>. Gdy aparat rozpoznawania jest wyłączony, żadne gramatyki rozpoznawania mowy dla aparatu rozpoznawania nie są dostępne dla operacji rozpoznawania. Ustawienie <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> właściwości aparatu rozpoznawania nie ma wpływu na <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> Właściwość aparatu rozpoznawania.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera kolekcję <see cref="T:System.Speech.Recognition.Grammar" /> obiektów, które są ładowane w tym <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> wystąpieniu.</summary>
        <value>Kolekcja <see cref="T:System.Speech.Recognition.Grammar" /> obiektów, do których aplikacja została załadowana do bieżącego wystąpienia udostępnionego aparatu rozpoznawania.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta właściwość nie zwraca żadnych gramatyki rozpoznawania mowy ładowanych przez inną aplikację.  
  
   
  
## Examples  
 Poniższy przykład wyprowadza informacje do konsoli dla każdej gramatyki rozpoznawania mowy załadowanej do udostępnionego aparatu rozpoznawania mowy.  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Grammar sampleGrammar = new Grammar(new GrammarBuilder("sample phrase"));  
        sampleGrammar.Name = "Sample Grammar";  
        recognizer.LoadGrammar(sampleGrammar);  
  
        OutputGrammarList(recognizer);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void OutputGrammarList(SpeechRecognizer recognizer)  
    {  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      if (grammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in grammars)  
        {  
          Console.WriteLine("  Grammar: {0}",  
            (g.Name != null) ? g.Name : "<no name>");  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
    }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Gramatyka rozpoznawania mowy do załadowania.</param>
        <summary>Ładuje gramatykę rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Współużytkowany aparat rozpoznawania zgłasza wyjątek, jeśli Gramatyka rozpoznawania mowy jest już załadowana, jest ładowane asynchronicznie lub załadowanie do dowolnego aparatu rozpoznawania nie powiodło się. Jeśli aparat rozpoznawania jest uruchomiony, aplikacje muszą korzystać <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> z programu, aby wstrzymać wyszukiwarkę mowy przed załadowaniem, wyładowaniem, włączeniem lub wyłączeniem gramatyki.  
  
 Aby załadować gramatykę rozpoznawania mowy asynchronicznie, użyj <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która ładuje gramatykę rozpoznawania mowy i pokazuje asynchroniczne emulowane dane wejściowe, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy. Jeśli funkcja rozpoznawania mowy systemu Windows nie jest uruchomiona, uruchomienie tej aplikacji również spowoduje uruchomienie funkcji rozpoznawania mowy systemu Windows. Jeśli rozpoznawanie mowy systemu Windows jest w **** stanie uśpienia, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }   
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Gramatyka rozpoznawania mowy do załadowania.</param>
        <summary>Asynchronicznie ładuje gramatykę rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Gdy aparat rozpoznawania ukończy tę operację asynchroniczną, wywołuje <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> zdarzenie. Aparat rozpoznawania zgłasza wyjątek, jeśli Gramatyka rozpoznawania mowy jest już załadowana, jest ładowane asynchronicznie lub załadowanie do dowolnego aparatu rozpoznawania nie powiodło się. Jeśli aparat rozpoznawania jest uruchomiony, aplikacje muszą korzystać <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> z programu, aby wstrzymać wyszukiwarkę mowy przed załadowaniem, wyładowaniem, włączeniem lub wyłączeniem gramatyki.  
  
 Aby synchronicznie załadować gramatykę rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania kończy asynchroniczne ładowanie gramatyki rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> Metoda aparatu rozpoznawania Inicjuje operację asynchroniczną. Aparat rozpoznawania wywołuje `LoadGrammarCompleted` zdarzenie po zakończeniu operacji. Aby uzyskać <xref:System.Speech.Recognition.Grammar> obiekt, który został załadowany przez aparat rozpoznawania, <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> Użyj właściwości skojarzonej <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>klasy. Aby uzyskać bieżące <xref:System.Speech.Recognition.Grammar> obiekty, które zostały załadowane przez aparat rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> właściwości aparatu rozpoznawania.  
  
 Podczas tworzenia delegata dla `LoadGrammarCompleted` zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład tworzy współużytkowany aparat rozpoznawania mowy, a następnie tworzy dwa typy gramatyki do rozpoznawania określonych słów i akceptowania swobodnego dyktowania. Przykład asynchronicznie ładuje wszystkie utworzone gramatyki do aparatu rozpoznawania. Programy obsługi dla aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenia zapisu w konsoli nazwa gramatyki, która została użyta do przeprowadzenia rozpoznawania i tekst wyniku rozpoznawania odpowiednio.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Add a handler for the StateChanged event.  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Create "yesno" grammar.  
        Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah}" });  
        SemanticResultValue yesValue =  
            new SemanticResultValue(yesChoices, (bool)true);  
        Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
        SemanticResultValue noValue =  
            new SemanticResultValue(noChoices, (bool)false);  
        SemanticResultKey yesNoKey =  
            new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
        Grammar yesnoGrammar = new Grammar(yesNoKey);  
        yesnoGrammar.Name = "yesNo";  
  
        // Create "done" grammar.  
        Grammar doneGrammar =  
          new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
        doneGrammar.Name = "Done";  
  
        // Create dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation";  
  
        // Load grammars to the recognizer.  
        recognizer.LoadGrammarAsync(yesnoGrammar);  
        recognizer.LoadGrammarAsync(doneGrammar);  
        recognizer.LoadGrammarAsync(dictation);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Put the shared speech recognizer into "listening" mode.   
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia maksymalną liczbę alternatywnych wyników rozpoznawania, które funkcja udostępnionego aparatu rozpoznawania zwraca dla każdej operacji rozpoznawania.</summary>
        <value>Maksymalna liczba alternatywnych wyników zwracanych przez aparat rozpoznawania mowy dla każdej operacji rozpoznawania.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Właściwość<xref:System.Speech.Recognition.RecognitionResult> klasy zawiera kolekcję obiektówreprezentującychinneinterpretacjekandydatadanychwejściowych.<xref:System.Speech.Recognition.RecognizedPhrase>  
  
 Wartość domyślna dla <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> wynosi 10.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognitionResult.Alternates" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="PauseRecognizerOnRecognition">
      <MemberSignature Language="C#" Value="public bool PauseRecognizerOnRecognition { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool PauseRecognizerOnRecognition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberSignature Language="VB.NET" Value="Public Property PauseRecognizerOnRecognition As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool PauseRecognizerOnRecognition { bool get(); void set(bool value); };" />
      <MemberSignature Language="F#" Value="member this.PauseRecognizerOnRecognition : bool with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia wartość wskazującą, czy współużytkowany aparat rozpoznawania wstrzymuje operacje rozpoznawania, gdy aplikacja obsługuje <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" /> zdarzenie.</summary>
        <value><see langword="true" />Jeśli udostępniony aparat rozpoznawania czeka na przetwarzanie danych wejściowych, <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" /> <see langword="false" />gdy dowolna aplikacja obsługuje zdarzenie; w przeciwnym razie.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ustaw tę właściwość na `true`, jeśli <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> w ramach programu obsługi zdarzeń aplikacja wymaga zmiany stanu usługi rozpoznawania mowy lub zmiany załadowanych lub włączonych gramatyki rozpoznawania mowy przed użyciem usługi rozpoznawania mowy przetwarza więcej danych wejściowych.  
  
> [!NOTE]
>  Ustawienie <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> właściwości powoduje, że każdy program obsługi zdarzeń w każdej aplikacji blokuje usługę rozpoznawania mowy systemu Windows. `true`  
  
 Aby zsynchronizować zmiany w udostępnionym aparacie rozpoznawania przy użyciu stanu aplikacji, użyj <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody.  
  
 Gdy <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> tak `true`jest ,<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> podczas wykonywania programu obsługi usługa rozpoznawania mowy wstrzymuje i buforuje nowe dane wejściowe audio w miarę ich nadejścia. Po zakończeniu obsługi zdarzeń usługa rozpoznawania mowy wznowi rozpoznawanie i zacznie przetwarzać informacje z buforu wejściowego. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>  
  
 Aby włączyć lub wyłączyć usługę rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera bieżącą lokalizację aparatu rozpoznawania w danych wejściowych audio, który jest przetwarzany.</summary>
        <value>Pozycja aparatu rozpoznawania w danych wejściowych audio, który jest przetwarzany.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 `RecognizerAudioPosition` Właściwość odwołuje się do pozycji aparatu rozpoznawania podczas przetwarzania danych wejściowych audio. Z kolei <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> Właściwość odwołuje się do pozycji urządzenia wejściowego w wygenerowanym strumieniu audio. Te pozycje mogą być różne. Na przykład, jeśli aparat rozpoznawania odebrał dane wejściowe, dla których nie Wygenerowano jeszcze wyniku rozpoznawania, wartość <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> właściwości jest mniejsza niż wartość <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera informacje o udostępnionym aparacie rozpoznawania mowy.</summary>
        <value>Informacje o udostępnionym aparacie rozpoznawania mowy.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta właściwość zwraca informacje o aparacie rozpoznawania mowy używanym przez funkcję rozpoznawania mowy systemu Windows.  
  
   
  
## Examples  
 Poniższy przykład wysyła do konsoli informacje o udostępnionym aparacie rozpoznawania.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Console.WriteLine("Recognizer information for the shared recognizer:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania wstrzyma się w celu zsynchronizowania rozpoznawania i innych operacji.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> programu, aby wstrzymać uruchomione <xref:System.Speech.Recognition.SpeechRecognizer> wystąpienie przed zmodyfikowaniem jego <xref:System.Speech.Recognition.Grammar> obiektów. Na przykład podczas <xref:System.Speech.Recognition.SpeechRecognizer> wstrzymania można ładować, zwalniać, włączać i wyłączać <xref:System.Speech.Recognition.Grammar> obiekty. Wywołuje <xref:System.Speech.Recognition.SpeechRecognizer> to zdarzenie, gdy jest gotowe do akceptowania modyfikacji.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono aplikację konsolową, która ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiekty. Aplikacja używa <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody, aby zażądać zatrzymania aparatu rozpoznawania mowy, aby można było odebrać aktualizację. Następnie aplikacja ładuje lub zwalnia <xref:System.Speech.Recognition.Grammar> obiekt.  
  
 W każdej aktualizacji program obsługi <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń zapisuje nazwę i stan aktualnie załadowanych <xref:System.Speech.Recognition.Grammar> obiektów do konsoli programu. Ponieważ gramatyki są ładowane i zwalniane, aplikacja najpierw rozpoznaje nazwy zwierząt farmy, a następnie nazwy zwierząt farmy i nazwy owoców, a następnie tylko nazwy owoców.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Żąda wstrzymania udostępnionego aparatu rozpoznawania i zaktualizowania jego stanu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda umożliwia synchronizowanie zmian współużytkowanego aparatu rozpoznawania. Jeśli na przykład załadujesz lub zwolnisz gramatykę rozpoznawania mowy podczas przetwarzania danych wejściowych przez aparat rozpoznawania, Użyj tej metody <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> i zdarzenia, aby zsynchronizować zachowanie aplikacji ze stanem aparatu rozpoznawania.  
  
 Gdy ta metoda jest wywoływana, aparat rozpoznawania wstrzymuje lub kończy operacje asynchroniczne i generuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzenie. Program <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> obsługi zdarzeń może następnie zmodyfikować stan aparatu rozpoznawania w ramach operacji rozpoznawania.  
  
 Gdy ta metoda jest wywoływana:  
  
-   Jeśli aparat rozpoznawania nie przetwarza danych wejściowych, aparat rozpoznawania natychmiast wygeneruje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzenie.  
  
-   Jeśli aparat rozpoznawania przetwarza dane wejściowe, które obejmują wyciszenie lub hałas w tle, aparat rozpoznawania wstrzymuje operację rozpoznawania i <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> generuje zdarzenie.  
  
-   Jeśli aparat rozpoznawania przetwarza dane wejściowe, które nie składają się z podkładu lub szumu w tle, aparat rozpoznawania wykonuje operację rozpoznawania, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> a następnie generuje zdarzenie.  
  
 Gdy aparat rozpoznawania obsługuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzenie:  
  
-   Aparat rozpoznawania nie przetwarza danych wejściowych, a wartość <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> właściwości pozostaje taka sama.  
  
-   Aparat rozpoznawania w dalszym ciągu zbiera dane wejściowe i może zmienić <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> wartość właściwości.  
  
 Aby zmienić, czy współużytkowany aparat rozpoznawania wstrzymuje operacje rozpoznawania, gdy aplikacja obsługuje <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenie, <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> Użyj właściwości.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono aplikację konsolową, która ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiekty. Aplikacja używa <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody, aby zażądać zatrzymania aparatu rozpoznawania mowy, aby można było odebrać aktualizację. Następnie aplikacja ładuje lub zwalnia <xref:System.Speech.Recognition.Grammar> obiekt.  
  
 W każdej aktualizacji program obsługi <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń zapisuje nazwę i stan aktualnie załadowanych <xref:System.Speech.Recognition.Grammar> obiektów do konsoli programu. Ponieważ gramatyki są ładowane i zwalniane, aplikacja najpierw rozpoznaje nazwy zwierząt farmy, a następnie nazwy zwierząt farmy i nazwy owoców, a następnie tylko nazwy owoców.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
  
      // Check to see if recognizer is loaded, wait if it is not loaded.  
      if (recognizer.State != RecognizerState.Listening)  
      {  
        Thread.Sleep(5000);  
  
        // Put recognizer in listening state.  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    public static void recognizer_RecognizerUpdateReached(object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      // At the update, get the names and enabled status of the currently loaded grammars.  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Żąda wstrzymania udostępnionego aparatu rozpoznawania i zaktualizowania jego stanu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Po wygenerowaniu <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> przez aparat rozpoznawania zdarzeń <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> ma `null`wartość.  
  
 Aby podać token użytkownika, użyj <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody lub. <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> Aby określić przesunięcie pozycji audio, użyj <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informacje zdefiniowane przez użytkownika, które zawierają informacje dla operacji.</param>
        <summary>Żąda, aby współużytkowany aparat rozpoznawania wstrzymał i zaktualizował swój stan i udostępnia token użytkownika dla skojarzonego zdarzenia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Po wygenerowaniu <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> przez aparat rozpoznawania zdarzeń <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> zawiera wartość `userToken` parametru.  
  
 Aby określić przesunięcie pozycji audio, użyj <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informacje zdefiniowane przez użytkownika, które zawierają informacje dla operacji.</param>
        <param name="audioPositionAheadToRaiseUpdate">Przesunięcie od bieżącego <see cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" /> , aby opóźnić żądanie.</param>
        <summary>Żąda, aby współużytkowany aparat rozpoznawania wstrzymał i zaktualizował jego stan oraz podaje przesunięcie i token użytkownika dla skojarzonego zdarzenia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania nie inicjuje żądania aktualizacji aparatu rozpoznawania do momentu, gdy <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> aparat rozpoznawania nie <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> jest równy bieżącemu `audioPositionAheadToRaiseUpdate` i wartości parametru.  
  
 Po wygenerowaniu <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> przez aparat rozpoznawania zdarzeń <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> zawiera wartość `userToken` parametru.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania wykryje dane wejściowe, które mogą identyfikować jako mowę.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Współużytkowany aparat rozpoznawania może zgłosić to zdarzenie w odpowiedzi na dane wejściowe. <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Właściwość skojarzonego<xref:System.Speech.Recognition.SpeechDetectedEventArgs> obiektu wskazuje lokalizację w strumieniu wejściowym, w którym aparat rozpoznawania wykrył mowę. Aby uzyskać więcej informacji, <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> Zobacz i właściwości <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> oraz metody <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> i.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej służącej do wybierania miejscowości początkowych i docelowych dla lotu. Aplikacja rozpoznaje frazy, takie jak "chcę mieć Miami do Chicago".  W przykładzie używa <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> się zdarzenia do <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> raportowania każdej wykrytej mowy.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=   
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje po rozpoznaniu przez aparat rozpoznawania wyrazu lub wyrazów, które mogą być składnikiem wielu kompletnych fraz w gramatyce.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Współużytkowany aparat rozpoznawania może zgłosić to zdarzenie, gdy dane wejściowe są niejednoznaczne. Na przykład w przypadku gramatyki rozpoznawania mowy, która obsługuje rozpoznawanie "nowa gra" lub "nowa gra", "nowa gra" to niejednoznaczne dane wejściowe, a "nowa gra" to niejednoznaczne dane wejściowe.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje zwroty, takie jak "Wyświetlanie listy artystów w kategorii Jazz". W przykładzie używa <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> się zdarzenia, aby wyświetlić niekompletne fragmenty frazy w konsoli w miarę ich rozpoznania.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=   
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechHypothesizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania odbiera dane wejściowe, które nie pasują do żadnego załadowanego gramatyki rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Współużytkowany aparat rozpoznawania wywołuje to zdarzenie, jeśli określa, że dane wejściowe nie pasują do wystarczającej pewności od załadowanej gramatyki rozpoznawania mowy. Właściwość zawiera obiekt odrzucony <xref:System.Speech.Recognition.RecognitionResult>. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs>  
  
 Progi zaufania udostępnionego aparatu rozpoznawania, zarządzane przez <xref:System.Speech.Recognition.SpeechRecognizer>, są skojarzone z profilem użytkownika i przechowywane w rejestrze systemu Windows. Aplikacje nie powinny zapisywać zmian w rejestrze dla właściwości udostępnionego aparatu rozpoznawania.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje frazy, takie jak "Wyświetlanie listy artystów w kategorii Jazz" lub "Wyświetlanie albumów gospel". W przykładzie zastosowano procedurę obsługi dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> zdarzenia, aby wyświetlić powiadomienie w konsoli, gdy dane wejściowe mowy nie mogą być dopasowane do zawartości gramatyki z wystarczającą pewnością do wygenerowania pomyślnego rozpoznania.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=   
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania odbiera dane wejściowe, które pasują do jednej z gramatyk rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania wywołuje zdarzenie `SpeechRecognized` , jeśli ustali, że dane wejściowe są zgodne z jedną z załadowanych i włączonych gramatyki rozpoznawania mowy. Właściwość zawiera zaakceptowany <xref:System.Speech.Recognition.RecognitionResult>obiekt. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs>  
  
 Progi zaufania udostępnionego aparatu rozpoznawania, zarządzane przez <xref:System.Speech.Recognition.SpeechRecognizer>, są skojarzone z profilem użytkownika i przechowywane w rejestrze systemu Windows. Aplikacje nie powinny zapisywać zmian w rejestrze dla właściwości udostępnionego aparatu rozpoznawania.  
  
 Gdy aparat rozpoznawania odbiera dane wejściowe, które pasują do <xref:System.Speech.Recognition.Grammar> gramatyki, obiekt <xref:System.Speech.Recognition.Grammar.SpeechRecognized> może zgłosić zdarzenie. Zdarzenie obiektu jest zgłaszane przed <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzeniem aparatu rozpoznawania mowy. <xref:System.Speech.Recognition.Grammar> <xref:System.Speech.Recognition.Grammar.SpeechRecognized>  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która ładuje gramatykę rozpoznawania mowy i prezentuje dane wejściowe mowy do udostępnionego aparatu rozpoznawania, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy. Jeśli funkcja rozpoznawania mowy systemu Windows nie jest uruchomiona, uruchomienie tej aplikacji również spowoduje uruchomienie funkcji rozpoznawania mowy systemu Windows.  
  
 Wypowiadane dane wejściowe, takie jak "chcę mieć od Chicago do Miami" spowodują <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> wyzwolenie zdarzenia. Mówiąc, że fraza "przylot ja z Houston do Chicago" nie spowoduje <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> wyzwolenia zdarzenia.  
  
 W przykładzie zastosowano procedurę obsługi dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenia, aby wyświetlić pomyślnie rozpoznane frazy i semantykę, która zawiera w konsoli programu.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="T:System.Speech.Recognition.SpeechRecognizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      </Docs>
    </Member>
    <Member MemberName="State">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerState State { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.RecognizerState State" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property State As RecognizerState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerState State { System::Speech::Recognition::RecognizerState get(); };" />
      <MemberSignature Language="F#" Value="member this.State : System.Speech.Recognition.RecognizerState" Usage="System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera stan <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiektu.</summary>
        <value>Stan <see langword="SpeechRecognizer" /> obiektu.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta właściwość tylko do odczytu wskazuje, czy udostępniony aparat rozpoznawania w systemie Windows jest w `Stopped` `Listening` stanie lub. Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.RecognizerState> Wyliczenie.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      </Docs>
    </Member>
    <Member MemberName="StateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Event StateChanged As EventHandler(Of StateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::StateChangedEventArgs ^&gt; ^ StateChanged;" />
      <MemberSignature Language="F#" Value="member this.StateChanged : EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; " Usage="member this.StateChanged : System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy zmienia się stan uruchomienia aparatu rozpoznawania technologii rozpoznawania mowy dla systemu Windows Desktop.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Współużytkowany aparat rozpoznawania zgłasza to zdarzenie, <xref:System.Speech.Recognition.RecognizerState.Listening> gdy stan funkcji rozpoznawania mowy systemu Windows zmieni się na stan lub. <xref:System.Speech.Recognition.RecognizerState.Stopped>  
  
 Aby uzyskać stan udostępnionego aparatu rozpoznawania w momencie zdarzenia, należy użyć <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> właściwości skojarzonej <xref:System.Speech.Recognition.StateChangedEventArgs>klasy. Aby uzyskać bieżący stan udostępnionego aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> właściwości aparatu rozpoznawania.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> zdarzenia należy zidentyfikować metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład tworzy współużytkowany aparat rozpoznawania mowy, a następnie tworzy dwa typy gramatyki do rozpoznawania określonych słów i akceptowania swobodnego dyktowania. Przykład asynchronicznie ładuje wszystkie utworzone gramatyki do aparatu rozpoznawania.  Procedura obsługi dla <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> zdarzenia <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> używa metody w celu umieszczenia rozpoznawania systemu Windows w trybie "nasłuchiwania".  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted += new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Add a handler for the StateChanged event.  
      recognizer.StateChanged += new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Create "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yah}" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "nah" });  
      SemanticResultValue noValue = new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void  recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
     if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void  recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
     Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void  recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
     string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
      }  
  
      // Add exception handling code here.  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="T:System.Speech.Recognition.StateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognizer.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Zwalnia wszystkie gramatyki rozpoznawania mowy z udostępnionego aparatu rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania aktualnie ładuje gramatykę asynchronicznie, ta metoda odczeka do momentu załadowania gramatyki, zanim odładuje wszystkie gramatyki aparatu rozpoznawania.  
  
 Aby zwolnić określoną gramatykę, użyj <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Gramatyka do zwolnienia.</param>
        <summary>Zwalnia określoną gramatykę rozpoznawania mowy z udostępnionego aparatu rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania jest uruchomiony, aplikacje muszą korzystać <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> z programu, aby wstrzymać wyszukiwarkę mowy przed załadowaniem, wyładowaniem, włączeniem lub wyłączeniem gramatyki. Aby zwolnić wszystkie gramatyki, użyj <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      </Docs>
    </Member>
  </Members>
</Type>

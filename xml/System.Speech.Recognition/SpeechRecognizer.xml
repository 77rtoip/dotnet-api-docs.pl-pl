<Type Name="SpeechRecognizer" FullName="System.Speech.Recognition.SpeechRecognizer">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="7f19e98d364cd16bffbf58714877f22e676a4052" />
    <Meta Name="ms.sourcegitcommit" Value="0e1f030650a307c745ee84ed547ef858acaea587" />
    <Meta Name="ms.translationtype" Value="MT" />
    <Meta Name="ms.contentlocale" Value="pl-PL" />
    <Meta Name="ms.lasthandoff" Value="11/29/2018" />
    <Meta Name="ms.locfileid" Value="52596032" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognizer extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognizer" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognizer&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognizer = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Zapewnia dostęp do udostępnionego usługę rozpoznawania mowy dostępne na pulpicie Windows.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aplikacje umożliwia dostęp do rozpoznawania mowy Windows udostępniony aparat rozpoznawania. Użyj <xref:System.Speech.Recognition.SpeechRecognizer> obiekt do dodania do środowiska użytkownika Windows mowy.  
  
 Ta klasa umożliwia sterowanie różnymi aspektami procesu rozpoznawania mowy:  
  
-   Aby zarządzać gramatyki rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A>, i <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A>.  
  
-   Aby uzyskać informacje o bieżącym mowy rozpoznawania operacje, Subskrybuj <xref:System.Speech.Recognition.SpeechRecognizer>firmy <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenia.  
  
-   Aby wyświetlić lub zmodyfikować liczbę wyników alternatywne zwraca aparat rozpoznawania, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> właściwości. Aparat rozpoznawania zwraca wyniki rozpoznawania w <xref:System.Speech.Recognition.RecognitionResult> obiektu.  
  
-   Aby uzyskać dostęp lub monitorowania stanu rozpoznawania udostępnionego, użyj <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, i <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> właściwości i <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged>, i <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> zdarzenia.  
  
-   Aby zsynchronizować zmiany aparat rozpoznawania, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody. Udostępniony aparat rozpoznawania używa więcej niż jeden wątek do wykonywania zadań.  
  
-   Aby emulować dane wejściowe do rozpoznawania udostępnionego, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> i <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metody.  
  
 Konfiguracja rozpoznawania mowy Windows odbywa się przy użyciu **właściwości mowy** okna dialogowego w **Panelu sterowania**. Ten interfejs jest używany, aby wybrać domyślny pulpitu rozpoznawania mowy i języka, urządzenia wejściowego audio i zachowanie uśpienia rozpoznawania mowy. Po zmianie konfiguracji rozpoznawania mowy Windows, gdy aplikacja jest uruchomiona (na przykład, jeśli rozpoznawanie mowy jest wyłączony lub zmianie języka wprowadzania), zmiana ma wpływ na wszystkie <xref:System.Speech.Recognition.SpeechRecognizer> obiektów.  
  
 Aby utworzyć rozpoznawania mowy w procesie, który jest niezależny od Windows rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine> klasy.  
  
> [!NOTE]
>  Zawsze wywołuj <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> przed publikacją swoje ostatnie odwołanie do rozpoznawania mowy. W przeciwnym razie zasobów jest przy użyciu nie zostanie zwolniona, dopóki moduł odśmiecania pamięci wywołuje obiekt rozpoznawania `Finalize` metody.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową która ładuje gramatyki rozpoznawania mowy i demonstruje asynchronicznego emulowanej danych wejściowych, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy.  Jeśli usługa rozpoznawania mowy Windows nie jest uruchomiona, następnie uruchamianie tej aplikacji będzie również uruchomić rozpoznawanie mowy Windows. Jeśli usługa rozpoznawania mowy Windows znajduje się w **uśpienie** stanu następnie <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
  </Docs>
  <Members>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognizer ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognizer();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inicjuje nowe wystąpienie klasy <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> klasy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy <xref:System.Speech.Recognition.SpeechRecognizer> obiekt zachowuje oddzielny zestaw gramatyki rozpoznawania mowy.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową która ładuje gramatyki rozpoznawania mowy i demonstruje asynchronicznego emulowanej danych wejściowych, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy. Jeśli usługa rozpoznawania mowy Windows nie jest uruchomiona, następnie uruchamianie tej aplikacji będzie również uruchomić rozpoznawanie mowy Windows. Jeśli usługa rozpoznawania mowy Windows znajduje się w **uśpienie** stanu następnie <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera format audio one odbierane przez aparat rozpoznawania mowy.</summary>
        <value>Format wejściowego audio dla aparatu rozpoznawania mowy, lub <see langword="null" /> Jeśli dane wejściowe dla aparatu rozpoznawania nie jest skonfigurowany.</value>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera poziom dźwięku odbierane przez aparat rozpoznawania mowy.</summary>
        <value>Poziom audio w danych wejściowych do rozpoznawania mowy, od 0 do 100.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy udostępniony aparat rozpoznawania zgłasza stopień dane wejściowe audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania zgłasza zdarzenie, to wiele razy na sekundę. Częstotliwość, z którym zdarzenie jest wywoływane, zależy od komputera, na którym działa aplikacja.  
  
 Aby uzyskać poziom audio w momencie wystąpienia zdarzenia, użyj <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> właściwości skojarzonego <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Aby uzyskać bieżący poziom audio w danych wejściowych dla aparatu rozpoznawania, użyj aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> właściwości.  
  
 Podczas tworzenia delegata dla `AudioLevelUpdated` zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład dodaje program obsługi `AudioLevelUpdated` zdarzenia <xref:System.Speech.Recognition.SpeechRecognizer> obiektu. Program obsługi danych wyjściowych nowy poziom audio do konsoli.  
  
```csharp  
private SpeechRecognizer recognizer;  
  
// Initialize the SpeechRecognizer object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
    new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera bieżącą lokalizację w strumienia audio generowanych przez urządzenia, który dostarcza dane wejściowe do rozpoznawania mowy.</summary>
        <value>Bieżąca lokalizacja w strumienia wejściowego z audio aparatu rozpoznawania mowy za pośrednictwem którego otrzymał dane wejściowe.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Udostępniony aparat rozpoznawania odbiera dane wejściowe podczas rozpoznawania mowy pulpitu.  
  
 `AudioPosition` Właściwości odwołuje się do pozycji urządzenia wejściowego w jego wygenerowany strumień audio. Z kolei <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> właściwości odwołuje się do pozycji aparat rozpoznawania podczas przetwarzania danych wejściowych audio. Te pozycje mogą być różne.  Na przykład, jeśli aparat rozpoznawania otrzymał dane wejściowe, do których nie ma jeszcze generowany wynik rozpoznawania, a następnie wartość <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> właściwość jest mniejsza niż wartość <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> właściwości.  
  
   
  
## Examples  
 W poniższym przykładzie aparatu rozpoznawania mowy udostępnionego używa gramatyki dyktowanie w celu dopasowania danych wejściowych mowy. Program obsługi <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> zdarzeń zapisuje do konsoli <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, i <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> podczas rozpoznawania mowy wykrywa mowę na dane wejściowe.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add handlers for events.  
      recognizer.LoadGrammarCompleted +=   
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
      recognizer.SpeechRecognized +=   
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
      recognizer.SpeechDetected +=   
        new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load the grammar object to the recognizer.  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Recognizer audio position: " + recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Write the name of the loaded grammar to the console.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania napotka problem w sygnału dźwiękowego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać, jaki problem wystąpił, użyj <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> właściwości skojarzonego <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Podczas tworzenia delegata dla `AudioSignalProblemOccurred` zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie zdefiniowano program obsługi zdarzeń, który gromadzi informacje o `AudioSignalProblemOccurred` zdarzeń.  
  
```  
private SpeechRecognizer recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera stan audio one odbierane przez aparat rozpoznawania mowy.</summary>
        <value>Stan wejścia audio do rozpoznawania mowy.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy zmiany stanu które usłyszysz odbierane przez aparat rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać stan audio w momencie wystąpienia zdarzenia, użyj <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> właściwości skojarzonego <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Aby uzyskać bieżący stan audio w danych wejściowych dla aparatu rozpoznawania, użyj aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> właściwości. Aby uzyskać więcej informacji na temat stanu audio, zobacz <xref:System.Speech.Recognition.AudioState> wyliczenia.  
  
 Podczas tworzenia delegata dla `AudioStateChanged` zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie użyto procedury obsługi dla `AudioStateChanged` zdarzenie, aby zapisać aparat rozpoznawania przez nowe <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> w konsoli każdy czasu zmiany przy użyciu członkiem <xref:System.Speech.Recognition.AudioState> wyliczenia.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the recognizer into Listening mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        Console.WriteLine();  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Usuwa <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiektu.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognizer.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Usuwa <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiektu.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognizer.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" /> Aby zwolnić zasoby zarządzane i niezarządzane; <see langword="false" /> aby zwolnić tylko niezarządzane zasoby.</param>
        <summary>Usuwa <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiektu i zwalnia zasoby używane podczas sesji.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuluje dane wejściowe do rozpoznawania mowy udostępnionego, przy użyciu tekstu zamiast audio rozpoznawania mowy synchroniczne.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody obejścia wejścia audio systemu. Może to być przydatne podczas testowania i debugowania aplikacji lub gramatyki.  
  
> [!NOTE]
>  Jeśli usługa rozpoznawania mowy Windows znajduje się w **uśpienie** stanu, a następnie te metody zwracają `null`.  
  
 Wywołuje udostępniony aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana. Aparat rozpoznawania ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału.  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult> Generowane przez udostępniony aparat rozpoznawania w odpowiedzi na dane wejściowe emulowanej obiektu ma wartość `null` dla jego <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> właściwości.  
  
 Aby emulować asynchroniczne rozpoznawanie, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Dane wejściowe dla operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzania frazę do rozpoznawania mowy udostępnionego, przy użyciu tekstu zamiast audio rozpoznawania mowy synchroniczne.</summary>
        <returns>Wynik rozpoznawania dla operacji rozpoznawania lub <see langword="null" />, jeśli operacja zakończy się niepowodzeniem lub rozpoznawanie mowy Windows znajduje się w **uśpienie** stanu.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 Ignoruj wielkość liter i znaków szerokości podczas stosowania reguły gramatyki sformułować danych wejściowych. Aby uzyskać więcej informacji na temat porównania tego typu, zobacz <xref:System.Globalization.CompareOptions> wartości wyliczenia <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> i <xref:System.Globalization.CompareOptions.IgnoreWidth>. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktować znaków interpunkcyjnych jako dane wejściowe literału.  
  
   
  
## Examples  
 Poniższy przykład ładuje gramatyki próbki do udostępnionego urządzenia rozpoznającego i emuluje dane wejściowe dla aparatu rozpoznawania. Jeśli usługa rozpoznawania mowy Windows nie jest uruchomiona, następnie uruchamianie tej aplikacji będzie również uruchomić rozpoznawanie mowy Windows. Jeśli usługa rozpoznawania mowy Windows znajduje się w **uśpienie** stanu następnie <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> zawsze zwraca wartość null.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        RecognitionResult result;  
  
        // This EmulateRecognize call matches the grammar and returns a  
        // recognition result.  
        result = recognizer.EmulateRecognize("testing testing");  
        OutputResult(result);  
  
        // This EmulateRecognize call does not match the grammar and   
        // returns null.  
        result = recognizer.EmulateRecognize("testing one two three");  
        OutputResult(result);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Output information about a recognition result to the console.  
    private static void OutputResult(RecognitionResult result)  
    {  
      if (result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Tablica jednostki programu word zawierającego dane wejściowe dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla operacji rozpoznawania emulowane.</param>
        <summary>Emuluje dane wejściowe określone słowa do rozpoznawania mowy udostępnionego, przy użyciu tekstu zamiast audio rozpoznawania mowy synchroniczne i określa, jak aparat rozpoznawania obsługuje Unicode porównanie wyrazów i gramatyki rozpoznawania mowy załadowane.</summary>
        <returns>Wynik rozpoznawania dla operacji rozpoznawania lub <see langword="null" />, jeśli operacja zakończy się niepowodzeniem lub rozpoznawanie mowy Windows znajduje się w **uśpienie** stanu.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda tworzy <xref:System.Speech.Recognition.RecognitionResult> obiektu, korzystając z informacji podanych w `wordUnits` parametru.  
  
 Aparat rozpoznawania używa `compareOptions` po stosuje reguły gramatyki sformułować danych wejściowych. Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 ignorowanie wielkości liter, jeśli <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> lub <xref:System.Globalization.CompareOptions.IgnoreCase> wartość jest obecna. Aparatów rozpoznawania zawsze Ignoruj szerokość znaków i nigdy nie Ignoruj typ znaki Kana. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału. Aby uzyskać więcej informacji na temat szerokość znaków i znaków Kana typu, zobacz <xref:System.Globalization.CompareOptions> wyliczenia.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frazy danych wejściowych dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla operacji rozpoznawania emulowane.</param>
        <summary>Emuluje wprowadzania frazę do rozpoznawania mowy udostępnionego, przy użyciu tekstu zamiast audio rozpoznawania mowy synchroniczne i określa, jak aparat rozpoznawania obsługuje porównanie Unicode frazy i gramatyki rozpoznawania mowy załadowane.</summary>
        <returns>Wynik rozpoznawania dla operacji rozpoznawania lub <see langword="null" />, jeśli operacja zakończy się niepowodzeniem lub rozpoznawanie mowy Windows znajduje się w **uśpienie** stanu.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania używa `compareOptions` po stosuje reguły gramatyki sformułować danych wejściowych. Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 ignorowanie wielkości liter, jeśli <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> lub <xref:System.Globalization.CompareOptions.IgnoreCase> wartość jest obecna. Aparatów rozpoznawania zawsze Ignoruj szerokość znaków i nigdy nie Ignoruj typ znaki Kana. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału. Aby uzyskać więcej informacji na temat szerokość znaków i znaków Kana typu, zobacz <xref:System.Globalization.CompareOptions> wyliczenia.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuluje dane wejściowe do rozpoznawania mowy udostępnionego, przy użyciu tekstu zamiast audio rozpoznawania mowy asynchronicznego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody obejścia wejścia audio systemu. Może to być przydatne podczas testowania i debugowania aplikacji lub gramatyki.  
  
 Wywołuje udostępniony aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana. Po ukończeniu operacji asynchronicznej rozpoznawania aparat rozpoznawania zgłasza <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> zdarzeń. Aparat rozpoznawania ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału.  
  
> [!NOTE]
>  Jeśli usługa rozpoznawania mowy Windows znajduje się w **uśpienie** stanu, a aparat rozpoznawania udostępnionego nie przetwarza dane wejściowe nie powoduje <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> i zdarzenia powiązane, ale nadal zgłasza <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> zdarzeń.  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult> Generowane przez udostępniony aparat rozpoznawania w odpowiedzi na dane wejściowe emulowanej obiektu ma wartość `null` dla jego <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> właściwości.  
  
 Aby emulować rozpoznawania synchroniczne, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Dane wejściowe dla operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzania frazę do rozpoznawania mowy udostępnionego, przy użyciu tekstu zamiast audio rozpoznawania mowy asynchronicznego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 Ignoruj wielkość liter i znaków szerokości podczas stosowania reguły gramatyki sformułować danych wejściowych. Aby uzyskać więcej informacji na temat porównania tego typu, zobacz <xref:System.Globalization.CompareOptions> wartości wyliczenia <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> i <xref:System.Globalization.CompareOptions.IgnoreWidth>. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktować znaków interpunkcyjnych jako dane wejściowe literału.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową która ładuje gramatyki rozpoznawania mowy i demonstruje asynchronicznego emulowanej danych wejściowych, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy. Jeśli usługa rozpoznawania mowy Windows nie jest uruchomiona, następnie uruchamianie tej aplikacji będzie również uruchomić rozpoznawanie mowy Windows. Jeśli usługa rozpoznawania mowy Windows znajduje się w **uśpienie** stanu następnie <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Tablica jednostki programu word zawierającego dane wejściowe dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla operacji rozpoznawania emulowane.</param>
        <summary>Emuluje dane wejściowe określone słowa do rozpoznawania mowy udostępnionego, przy użyciu tekstu zamiast audio rozpoznawania mowy asynchronicznego i określa, jak aparat rozpoznawania obsługuje Unicode porównanie wyrazów i gramatyki rozpoznawania mowy załadowane.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda tworzy <xref:System.Speech.Recognition.RecognitionResult> obiektu, korzystając z informacji podanych w `wordUnits` parametru.  
  
 Aparat rozpoznawania używa `compareOptions` po stosuje reguły gramatyki sformułować danych wejściowych. Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 ignorowanie wielkości liter, jeśli <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> lub <xref:System.Globalization.CompareOptions.IgnoreCase> wartość jest obecna. Aparatów rozpoznawania zawsze Ignoruj szerokość znaków i nigdy nie Ignoruj typ znaki Kana. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału. Aby uzyskać więcej informacji na temat szerokość znaków i znaków Kana typu, zobacz <xref:System.Globalization.CompareOptions> wyliczenia.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frazy danych wejściowych dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla operacji rozpoznawania emulowane.</param>
        <summary>Emuluje wprowadzania frazę do rozpoznawania mowy udostępnionego, przy użyciu tekstu zamiast audio rozpoznawania mowy asynchronicznego i określa, jak aparat rozpoznawania obsługuje porównanie Unicode frazy i gramatyki rozpoznawania mowy załadowane.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania używa `compareOptions` po stosuje reguły gramatyki sformułować danych wejściowych. Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 ignorowanie wielkości liter, jeśli <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> lub <xref:System.Globalization.CompareOptions.IgnoreCase> wartość jest obecna. Aparatów rozpoznawania zawsze Ignoruj szerokość znaków i nigdy nie Ignoruj typ znaki Kana. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału. Aby uzyskać więcej informacji na temat szerokość znaków i znaków Kana typu, zobacz <xref:System.Globalization.CompareOptions> wyliczenia.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy udostępniony aparat rozpoznawania Kończenie znajdujących się w operacji asynchronicznej rozpoznawania emulowanej danych wejściowych.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metoda rozpoczyna operację asynchroniczną rozpoznawania. Generuje aparatu rozpoznawania `EmulateRecognizeCompleted` zdarzenie, kiedy go Kończenie znajdujących się w operacji asynchronicznej.  
  
 Może wywoływać operację asynchroniczną rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenia. <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> Zdarzeń jest ostatnim tych zdarzeń, że aparat rozpoznawania zgłasza dla danej operacji.  
  
 Podczas tworzenia delegata dla `EmulateRecognizeCompleted` zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową która ładuje gramatyki rozpoznawania mowy i demonstruje asynchronicznego emulowanej danych wejściowych, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy. Jeśli usługa rozpoznawania mowy Windows nie jest uruchomiona, następnie uruchamianie tej aplikacji będzie również uruchomić rozpoznawanie mowy Windows. Jeśli usługa rozpoznawania mowy Windows znajduje się w **uśpienie** tryb, a następnie <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=   
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="Enabled">
      <MemberSignature Language="C#" Value="public bool Enabled { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool Enabled" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberSignature Language="VB.NET" Value="Public Property Enabled As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool Enabled { bool get(); void set(bool value); };" />
      <MemberSignature Language="F#" Value="member this.Enabled : bool with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia wartość wskazującą, czy to <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiekt jest gotowy do przetwarzania mowy.</summary>
        <value>
          <see langword="true" /> Jeśli ten <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiekt wykonuje rozpoznawanie mowy; w przeciwnym razie <see langword="false" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Zmiany tej właściwości nie wpływają na inne wystąpienia <xref:System.Speech.Recognition.SpeechRecognizer> klasy.  
  
 Domyślnie wartość <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> właściwość `true` dla nowo utworzona wystąpienia <xref:System.Speech.Recognition.SpeechRecognizer>. Po wyłączeniu aparat rozpoznawania żaden aparat rozpoznawania mowy rozpoznawania gramatyki jest dostępna na potrzeby rozpoznawania operacji. Ustawienie aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> właściwość nie ma wpływu na aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera kolekcję <see cref="T:System.Speech.Recognition.Grammar" /> obiekty, które są ładowane w tym <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> wystąpienia.</summary>
        <value>Kolekcja <see cref="T:System.Speech.Recognition.Grammar" /> obiekty, które aplikacja załadowane do bieżącego wystąpienia udostępnionej rozpoznawania.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta właściwość nie zwraca żadnych mowy gramatyki rozpoznawania załadowane przez inną aplikację.  
  
   
  
## Examples  
 Poniższy przykład wyświetla informacje o konsoli dla każdego gramatyki rozpoznawania mowy, ładowane do rozpoznawania mowy udostępnionych.  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Grammar sampleGrammar = new Grammar(new GrammarBuilder("sample phrase"));  
        sampleGrammar.Name = "Sample Grammar";  
        recognizer.LoadGrammar(sampleGrammar);  
  
        OutputGrammarList(recognizer);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void OutputGrammarList(SpeechRecognizer recognizer)  
    {  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      if (grammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in grammars)  
        {  
          Console.WriteLine("  Grammar: {0}",  
            (g.Name != null) ? g.Name : "<no name>");  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
    }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Gramatyka rozpoznawania mowy do załadowania.</param>
        <summary>Ładuje gramatyki rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Udostępniony aparat rozpoznawania zgłasza wyjątek, jeśli jest już załadowany gramatyki rozpoznawania mowy, są ładowane asynchronicznie lub nie udało się załadować do dowolnego aparatu rozpoznawania. Jeśli działa aparat rozpoznawania, aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> wstrzymać aparatu rozpoznawania mowy, przed ładowania, zwalnianie, włączanie lub wyłączanie gramatyki.  
  
 Aby załadować asynchronicznie gramatyki rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową która ładuje gramatyki rozpoznawania mowy i demonstruje asynchronicznego emulowanej danych wejściowych, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy. Jeśli usługa rozpoznawania mowy Windows nie jest uruchomiona, następnie uruchamianie tej aplikacji będzie również uruchomić rozpoznawanie mowy Windows. Jeśli usługa rozpoznawania mowy Windows znajduje się w **uśpienie** stanu następnie <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> zawsze zwraca wartość null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }   
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Gramatyka rozpoznawania mowy do załadowania.</param>
        <summary>Ładuje asynchronicznie gramatyki rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Po ukończeniu tej operacji asynchronicznej aparat rozpoznawania zgłasza <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> zdarzeń. Aparat rozpoznawania zgłasza wyjątek, jeśli jest już załadowany gramatyki rozpoznawania mowy, są ładowane asynchronicznie lub nie udało się załadować do dowolnego aparatu rozpoznawania. Jeśli działa aparat rozpoznawania, aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> wstrzymać aparatu rozpoznawania mowy, przed ładowania, zwalnianie, włączanie lub wyłączanie gramatyki.  
  
 Aby załadować synchronicznie gramatyki rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania zakończy asynchroniczne ładowanie gramatyki rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> metoda inicjuje operację asynchroniczną. Generuje aparatu rozpoznawania `LoadGrammarCompleted` zdarzenie po zakończeniu tej operacji. Aby uzyskać <xref:System.Speech.Recognition.Grammar> obiektu, że aparat rozpoznawania załadowany, należy użyć <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> właściwości skojarzonego <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Aby uzyskać bieżącą <xref:System.Speech.Recognition.Grammar> obiektów aparat rozpoznawania został załadowany, użyj aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> właściwości.  
  
 Podczas tworzenia delegata dla `LoadGrammarCompleted` zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład tworzy rozpoznawania mowy udostępnionego, a następnie tworzy dwa typy gramatyki rozpoznawania konkretnych słów i akceptowania dyktowanie bezpłatne. Przykład ładuje asynchronicznie wszystkich gramatykach utworzonego dla aparatu rozpoznawania. Programy obsługi dla aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> i <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> nazwa gramatycznych, który został użyty do rozpoznawania i tekst wynik rozpoznawania odpowiednio zdarzenia zapisu do konsoli.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Add a handler for the StateChanged event.  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Create "yesno" grammar.  
        Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah}" });  
        SemanticResultValue yesValue =  
            new SemanticResultValue(yesChoices, (bool)true);  
        Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
        SemanticResultValue noValue =  
            new SemanticResultValue(noChoices, (bool)false);  
        SemanticResultKey yesNoKey =  
            new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
        Grammar yesnoGrammar = new Grammar(yesNoKey);  
        yesnoGrammar.Name = "yesNo";  
  
        // Create "done" grammar.  
        Grammar doneGrammar =  
          new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
        doneGrammar.Name = "Done";  
  
        // Create dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation";  
  
        // Load grammars to the recognizer.  
        recognizer.LoadGrammarAsync(yesnoGrammar);  
        recognizer.LoadGrammarAsync(doneGrammar);  
        recognizer.LoadGrammarAsync(dictation);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Put the shared speech recognizer into "listening" mode.   
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia maksymalną liczbę wyników rozpoznawania alternatywne, zwracanych przez aparat rozpoznawania udostępnionego dla każdej operacji rozpoznawania.</summary>
        <value>Maksymalna liczba alternatywne wyniki, które zwraca aparatu rozpoznawania mowy, dla każdej operacji rozpoznawania.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Właściwość <xref:System.Speech.Recognition.RecognitionResult> klasy zawiera kolekcję <xref:System.Speech.Recognition.RecognizedPhrase> obiekty reprezentujące interpretacji danych wejściowych innych Release candidate.  
  
 Wartością domyślną dla <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> wynosi 10.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognitionResult.Alternates" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="PauseRecognizerOnRecognition">
      <MemberSignature Language="C#" Value="public bool PauseRecognizerOnRecognition { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool PauseRecognizerOnRecognition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberSignature Language="VB.NET" Value="Public Property PauseRecognizerOnRecognition As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool PauseRecognizerOnRecognition { bool get(); void set(bool value); };" />
      <MemberSignature Language="F#" Value="member this.PauseRecognizerOnRecognition : bool with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia wartość wskazującą, czy udostępniony aparat rozpoznawania wstrzymuje uznanie operacji, podczas gdy aplikacja jest obsługa <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" /> zdarzeń.</summary>
        <value>
          <see langword="true" /> Jeśli udostępniony aparat rozpoznawania czeka do przetwarzania danych wejściowych, gdy każda aplikacja obsługuje <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" /> zdarzenia; w przeciwnym razie <see langword="false" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ustaw tę właściwość na `true`, jeśli w ramach <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> programu obsługi zdarzeń, aplikacja musi zmienić stan usługi rozpoznawania mowy lub gramatyki rozpoznawania mowy załadowane i włączone, przed uruchomieniem usługi rozpoznawania mowy procesy więcej danych wejściowych.  
  
> [!NOTE]
>  Ustawienie <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> właściwości `true` powoduje, że każdy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> programu obsługi zdarzeń w każdej aplikacji, aby zablokować usługę rozpoznawania mowy Windows.  
  
 Aby zsynchronizować zmiany z udostępnionego rozpoznawania ze stanem aplikacji, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody.  
  
 Gdy <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> jest `true`, podczas wykonywania <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> obsługi usługę rozpoznawania mowy, wstrzymuje i buforuje nowe dane wejściowe audio niezapisywanie ich. Raz <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> kończy działanie procedury obsługi zdarzeń, rozpoznawanie wznawia usługi rozpoznawania mowy i rozpoczyna przetwarzanie informacji z jego buforu wejściowego.  
  
 Aby włączyć lub wyłączyć usługę rozpoznawania mowy, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera bieżącą lokalizację aparatu rozpoznającego w wejścia audio, która przetwarza.</summary>
        <value>Pozycja rozpoznawania wejścia audio, która przetwarza.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 `RecognizerAudioPosition` Właściwości odwołuje się do pozycji aparat rozpoznawania podczas przetwarzania dane wejściowe audio. Z kolei <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> właściwości odwołuje się do pozycji urządzenia wejściowego w jego wygenerowany strumień audio. Te pozycje mogą być różne. Na przykład, jeśli aparat rozpoznawania otrzymał dane wejściowe, do których nie ma jeszcze generowany wynik rozpoznawania, a następnie wartość <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> właściwość jest mniejsza niż wartość <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera informacje o aparatu rozpoznawania mowy udostępnionych.</summary>
        <value>Informacje na temat aparatu rozpoznawania mowy udostępnionych.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta właściwość zwraca informacji na temat aparatu rozpoznawania mowy w użyciu przez Windows rozpoznawanie mowy.  
  
   
  
## Examples  
 Poniższy przykład wysyła informacje o udostępnionych rozpoznawania do konsoli.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Console.WriteLine("Recognizer information for the shared recognizer:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania wstrzymuje się, aby zsynchronizować ocen i innych operacji.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> wstrzymać działającego wystąpienia <xref:System.Speech.Recognition.SpeechRecognizer> przed zmodyfikowaniem jego <xref:System.Speech.Recognition.Grammar> obiektów. Na przykład <xref:System.Speech.Recognition.SpeechRecognizer> jest wstrzymana, możesz można załadować, zwolnij, włączania i wyłączania <xref:System.Speech.Recognition.Grammar> obiektów. <xref:System.Speech.Recognition.SpeechRecognizer> Zgłasza to zdarzenie, gdy wszystko będzie gotowe zaakceptować zmiany.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono aplikację konsolową która ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiektów. Aplikacja używa <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodę, aby zażądać aparatu rozpoznawania mowy, aby wstrzymać, więc może ona odbierać aktualizacji. Aplikacja, a następnie ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiektu.  
  
 Przy każdej aktualizacji program obsługi <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń zapisuje nazwę i stan aktualnie załadowanych <xref:System.Speech.Recognition.Grammar> obiekty do konsoli. Gramatyki są ładowane i zwolniony, najpierw rozpoznaje nazwy zwierząt gospodarskich, a następnie utworzyć nazwy zwierząt gospodarskich nazw owoców, a następnie nazwy tylko owoce.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Żądania, czy udostępniony aparat rozpoznawania wstrzymać i zaktualizuj jego stan.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda umożliwia synchronizowanie zmian do udostępnionego aparatu rozpoznawania. Na przykład jeśli obciążenia, lub zwolnij gramatyki rozpoznawania mowy, gdy aparat rozpoznawania przetwarza dane wejściowe, należy użyć tej metody i <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzenie, aby zsynchronizować swoje zachowanie aplikacji ze stanem aparat rozpoznawania.  
  
 Gdy ta metoda jest wywoływana, aparat rozpoznawania wstrzymuje lub zakończeniu operacji asynchronicznych i generuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń. A <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> program obsługi zdarzeń można zmodyfikować stanu rozpoznawania Between uznanie operacji.  
  
 Gdy ta metoda jest wywoływana:  
  
-   Jeśli aparat rozpoznawania nie przetwarza dane wejściowe, aparat rozpoznawania natychmiast generuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń.  
  
-   Jeśli aparat rozpoznawania przetwarza dane wejściowe, który składa się z wyciszenia lub hałas w tle, aparat rozpoznawania wstrzymuje działanie rozpoznawanie i generuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń.  
  
-   Jeśli aparat rozpoznawania przetwarza dane wejściowe, który składa się z wyciszenia lub hałas w tle, aparat rozpoznawania zakończeniu operacji uznania, a następnie generuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń.  
  
 Podczas obsługi jest aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń:  
  
-   Aparat rozpoznawania nie przetwarza dane wejściowe, a wartość <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> właściwości pozostają takie same.  
  
-   Aparat rozpoznawania w dalszym ciągu zbieranie danych wejściowych, a wartość <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> można zmienić właściwości.  
  
 Aby określić, czy udostępniony aparat rozpoznawania wstrzymuje operacje rozpoznawania, podczas gdy aplikacja jest obsługa <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenia, użyj <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> właściwości.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono aplikację konsolową która ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiektów. Aplikacja używa <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodę, aby zażądać aparatu rozpoznawania mowy, aby wstrzymać, więc może ona odbierać aktualizacji. Aplikacja, a następnie ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiektu.  
  
 Przy każdej aktualizacji program obsługi <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzeń zapisuje nazwę i stan aktualnie załadowanych <xref:System.Speech.Recognition.Grammar> obiekty do konsoli. Gramatyki są ładowane i zwolniony, najpierw rozpoznaje nazwy zwierząt gospodarskich, a następnie utworzyć nazwy zwierząt gospodarskich nazw owoców, a następnie nazwy tylko owoce.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
  
      // Check to see if recognizer is loaded, wait if it is not loaded.  
      if (recognizer.State != RecognizerState.Listening)  
      {  
        Thread.Sleep(5000);  
  
        // Put recognizer in listening state.  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    public static void recognizer_RecognizerUpdateReached(object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      // At the update, get the names and enabled status of the currently loaded grammars.  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Żądania, czy udostępniony aparat rozpoznawania wstrzymać i zaktualizuj jego stan.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Gdy aparat rozpoznawania generuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzenia <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> jest `null`.  
  
 Aby dostarczyć token użytkownika, użyj <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> lub <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody. Aby określić przesunięcie pozycji audio, użyj <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informacje zdefiniowane przez użytkownika, który zawiera informacje dla tej operacji.</param>
        <summary>Żądania udostępniony aparat rozpoznawania wstrzymać i zaktualizuj jego stan i zawiera token użytkownika dla skojarzonego zdarzenia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Gdy aparat rozpoznawania generuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzenia <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> zawiera wartość `userToken` parametru.  
  
 Aby określić przesunięcie pozycji audio, użyj <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informacje zdefiniowane przez użytkownika, który zawiera informacje dla tej operacji.</param>
        <param name="audioPositionAheadToRaiseUpdate">Przesunięcie od bieżącego <see cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" /> opóźnienia żądania.</param>
        <summary>Żądania udostępniony aparat rozpoznawania wstrzymać i zaktualizuj jego stan i zawiera przesunięcie i tokenu użytkownika dla skojarzonego zdarzenia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania nie zostanie zainicjowane do momentu aparat rozpoznawania żądanie aktualizacji aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> jest równe bieżącego <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> plus wartość `audioPositionAheadToRaiseUpdate` parametru.  
  
 Gdy aparat rozpoznawania generuje <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> zdarzenia <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> zawiera wartość `userToken` parametru.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania wykrywa dane wejściowe, którą można zidentyfikować jako mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Udostępniony aparat rozpoznawania może zgłosić to zdarzenie w odpowiedzi na dane wejściowe. <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Właściwości skojarzonego <xref:System.Speech.Recognition.SpeechDetectedEventArgs> obiektu wskazuje lokalizację w strumieniu wejściowym, w przypadku wykrycia przez aparat rozpoznawania mowy. Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> i <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> właściwości i <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> i <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metody.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsoli dotyczące wybierania początkowe i docelowe miast w locie. Aplikacja rozpoznaje fraz, takie jak "Chcę się z Miami do Chicago".  W przykładzie użyto <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> zdarzeń do raportu <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> każdego mowy czasu wykrycia.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=   
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania rozpoznał wyrazów, które mogą być składnik wielu wyrażeń ukończone w gramatyce.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Udostępniony aparat rozpoznawania może zgłosić to zdarzenie, gdy dane wejściowe są niejednoznaczne. Na przykład gramatyki rozpoznawania mowy, który obsługuje rozpoznawanie albo "nowych gier," lub "nową grę", "nowych gier," jest jednoznaczna dane wejściowe, a "nową grę" jest niejednoznaczne dane wejściowe.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje fraz, takie jak "Display listę artystów w kategorii jazz". W przykładzie użyto <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> zdarzenie, aby wyświetlić fragmenty niekompletne frazę w konsoli, jak zostaną rozpoznane.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=   
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechHypothesizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania odbiera dane wejściowe, które nie pasuje do żadnego gramatyki rozpoznawania mowy, które ma on ładowany.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Udostępniony aparat rozpoznawania zgłasza to zdarzenie, gdy ustali, że dane wejściowe nie jest zgodna z wystarczający poziom zaufania dowolnej gramatyki rozpoznawania mowy załadowane. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Właściwość <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> zawiera odrzuconych <xref:System.Speech.Recognition.RecognitionResult> obiektu.  
  
 Progi zaufania dla udostępnionego urządzenia rozpoznającego zarządzane przez <xref:System.Speech.Recognition.SpeechRecognizer>, są skojarzone z profilem użytkownika i przechowywane w rejestrze systemu Windows. Aplikacje nie powinien zapisać zmiany w rejestrze dla właściwości udostępniony aparat rozpoznawania.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje fraz, takie jak "Wyświetlić listę artystów kategorii jazz" lub "Wyświetlanie albumów gospel". W przykładzie użyto procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> zdarzenie, aby wyświetlić powiadomienie w konsoli po mowy danych wejściowych nie można dopasować do zawartości gramatyki wystarczający poziom zaufania, aby wygenerować pomyślne rozpoznawanie.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=   
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy aparat rozpoznawania odbiera dane wejściowe, które pasuje do jednej z jego gramatyki rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Generuje aparatu rozpoznawania `SpeechRecognized` zdarzeń, jeśli okaże się pewnie wystarczających danych wejściowych odpowiada jednej gramatyki rozpoznawania mowy załadowane i włączone. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Właściwość <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> zawiera zaakceptowane <xref:System.Speech.Recognition.RecognitionResult> obiektu.  
  
 Progi zaufania dla udostępnionego urządzenia rozpoznającego zarządzane przez <xref:System.Speech.Recognition.SpeechRecognizer>, są skojarzone z profilem użytkownika i przechowywane w rejestrze systemu Windows. Aplikacje nie powinien zapisać zmiany w rejestrze dla właściwości udostępniony aparat rozpoznawania.  
  
 Gdy aparat rozpoznawania odbiera dane wejściowe, który odpowiada gramatykę, <xref:System.Speech.Recognition.Grammar> obiektów może zgłosić <xref:System.Speech.Recognition.Grammar.SpeechRecognized> zdarzeń. <xref:System.Speech.Recognition.Grammar> Obiektu <xref:System.Speech.Recognition.Grammar.SpeechRecognized> zdarzenie jest wywoływane przed aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzeń.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową która ładuje gramatyki rozpoznawania mowy i pokazuje dane wejściowe mowy udostępniony aparat rozpoznawania, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy. Jeśli usługa rozpoznawania mowy Windows nie jest uruchomiona, następnie uruchamianie tej aplikacji będzie również uruchomić rozpoznawanie mowy Windows.  
  
 Wypowiadane dane wejściowe, takie jak "Chcę się z Chicago do Warszawa" spowoduje wyzwolenie <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzeń. Nie wywoła wypowiedzi frazę "Podnoszenia me z Houston do Chicago" <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzeń.  
  
 W przykładzie użyto procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> zdarzenie, aby wyświetlić pomyślnie rozpoznane fraz i semantyka zawierają one w konsoli.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="T:System.Speech.Recognition.SpeechRecognizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      </Docs>
    </Member>
    <Member MemberName="State">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerState State { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.RecognizerState State" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property State As RecognizerState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerState State { System::Speech::Recognition::RecognizerState get(); };" />
      <MemberSignature Language="F#" Value="member this.State : System.Speech.Recognition.RecognizerState" Usage="System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera stan <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> obiektu.</summary>
        <value>Stan <see langword="SpeechRecognizer" /> obiektu.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta właściwość tylko do odczytu wskazuje, czy aparat rozpoznawania udostępnionych w Windows znajduje się w `Stopped` lub `Listening` stanu. Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.RecognizerState> wyliczenia.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      </Docs>
    </Member>
    <Member MemberName="StateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Event StateChanged As EventHandler(Of StateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::StateChangedEventArgs ^&gt; ^ StateChanged;" />
      <MemberSignature Language="F#" Value="member this.StateChanged : EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; " Usage="member this.StateChanged : System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Występuje, gdy zmieni się stan działania aparatu rozpoznawania technologii mowy pulpitu Windows.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Udostępniony aparat rozpoznawania zgłasza to zdarzenie po zmianie stanu rozpoznawania mowy Windows do <xref:System.Speech.Recognition.RecognizerState.Listening> lub <xref:System.Speech.Recognition.RecognizerState.Stopped> stanu.  
  
 Aby uzyskać stan udostępnionego urządzenia rozpoznającego w momencie wystąpienia zdarzenia, użyj <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> właściwości skojarzonego <xref:System.Speech.Recognition.StateChangedEventArgs>. Aby uzyskać bieżący stan rozpoznawania udostępnionego, użyj aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> właściwości.  
  
 Podczas tworzenia delegata dla <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> zdarzeń, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład tworzy rozpoznawania mowy udostępnionego, a następnie tworzy dwa typy gramatyki rozpoznawania konkretnych słów i akceptowania dyktowanie bezpłatne. Przykład ładuje asynchronicznie wszystkich gramatykach utworzonego dla aparatu rozpoznawania.  Program obsługi <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> zdarzeń używa <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metodę, aby umieścić rozpoznawania Windows w trybie "nasłuchiwania".  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted += new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Add a handler for the StateChanged event.  
      recognizer.StateChanged += new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Create "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yah}" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "nah" });  
      SemanticResultValue noValue = new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void  recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
     if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void  recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
     Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void  recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
     string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
      }  
  
      // Add exception handling code here.  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="T:System.Speech.Recognition.StateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognizer.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Zwalnia wszystkich gramatykach rozpoznawania mowy z udostępnionego rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania aktualnie jest asynchroniczne ładowanie gramatyki, ta metoda czeka, aż gramatyki zostanie załadowana przed zwalnia wszystkich gramatykach aparat rozpoznawania.  
  
 Aby zwolnić określonego gramatyki, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Gramatyka zwolnić.</param>
        <summary>Zwalnia gramatyki rozpoznawania mowy określony z udostępnionego rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli działa aparat rozpoznawania, aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> wstrzymać aparatu rozpoznawania mowy, przed ładowania, zwalnianie, włączanie lub wyłączanie gramatyki. Aby zwolnić wszystkich gramatykach, należy użyć <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      </Docs>
    </Member>
  </Members>
</Type>
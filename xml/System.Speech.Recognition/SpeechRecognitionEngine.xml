<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata><Meta Name="ms.openlocfilehash" Value="abb3a2ff2128107c5ae14925df9908b48656fcb4" /><Meta Name="ms.sourcegitcommit" Value="9e3550fb2088d4faf2043f0acb29da4555519937" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="pl-PL" /><Meta Name="ms.lasthandoff" Value="12/12/2018" /><Meta Name="ms.locfileid" Value="53297466" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Udostępnia metody dostępu i zarządzania aparatu rozpoznawania mowy w procesie.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Dla każdej z aparatów rozpoznawania mowy zainstalowane, można utworzyć wystąpienie tej klasy. Aby uzyskać informacje o tym, które są zainstalowane aparatów rozpoznawania gestów, używa się statycznej <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metody.  
  
 Ta klasa dotyczy uruchamiania mowy rozpoznawania aparatów w procesie i zapewnia kontrolę nad różnych aspektów programu rozpoznawania mowy w następujący sposób:  
  
-   Aby utworzyć aparatu rozpoznawania mowy w procesie, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> konstruktorów.  
  
-   Aby zarządzać gramatyki rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> metod i <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> właściwości.  
  
-   Aby skonfigurować dane wejściowe dla aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> metody.  
  
-   Aby wykonać rozpoznawanie mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody.  
  
-   Aby zmodyfikować sposób rozpoznawania obsługi wyciszenia lub nieoczekiwane dane wejściowe, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości.  
  
-   Aby zmienić liczbę zastępców zwraca aparat rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> właściwości. Aparat rozpoznawania zwraca wyniki rozpoznawania w <xref:System.Speech.Recognition.RecognitionResult> obiektu.  
  
-   Aby zsynchronizować zmiany aparat rozpoznawania, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody. Aparat rozpoznawania używa więcej niż jeden wątek do wykonywania zadań.  
  
-   Aby emulować dane wejściowe dla aparatu rozpoznawania, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metody.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Obiekt jest jedyny do używania procesu, który wystąpienia obiektu. Z kolei <xref:System.Speech.Recognition.SpeechRecognizer> udostępni pojedynczego rozpoznawania każdą aplikację, która chce używać go.  
  
> [!NOTE]
>  Zawsze wywołuj <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> przed publikacją swoje ostatnie odwołanie do rozpoznawania mowy. W przeciwnym razie zasobów jest przy użyciu nie zostanie zwolniona, dopóki moduł odśmiecania pamięci wywołuje obiekt rozpoznawania `Finalize` metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowe. Ponieważ w tym przykładzie użyto `Multiple` tryb <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody wykonuje rozpoznawanie do czasu zamknięcia okna konsoli lub zatrzymać debugowanie.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inicjuje nowe wystąpienie klasy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> klasy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Można skonstruować <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienie z dowolną z następujących czynności:  
  
-   Aparat rozpoznawania mowy domyślne systemu  
  
-   Określone rozpoznawania mowy, określonej przez nazwę  
  
-   Aparat rozpoznawania mowy domyślne dla ustawień regionalnych, który określisz  
  
-   Aparat rozpoznawania określonej, która spełnia kryteria, które są określone w <xref:System.Speech.Recognition.RecognizerInfo> obiektu.  
  
 Zanim aparatu rozpoznawania mowy, można rozpocząć rozpoznawania, należy załadować gramatyki rozpoznawania mowy co najmniej jedną i konfigurowanie danych wejściowych przez aparat rozpoznawania.  
  
 Aby załadować gramatyki, należy wywołać <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metody.  
  
 Aby skonfigurować dane wejściowe audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inicjuje nowe wystąpienie klasy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> przy użyciu aparatu rozpoznawania mowy domyślne systemu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Zanim aparatu rozpoznawania mowy, można rozpocząć rozpoznawania mowy, należy załadować co najmniej jeden gramatyki rozpoznawanie i konfigurowanie danych wejściowych przez aparat rozpoznawania.  
  
 Aby załadować gramatyki, należy wywołać <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metody.  
  
 Aby skonfigurować dane wejściowe audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Ustawienia regionalne musi obsługiwać aparatu rozpoznawania mowy.</param>
        <summary>Inicjuje nowe wystąpienie klasy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> klasy przy użyciu aparatu rozpoznawania mowy domyślny dla określonych ustawień regionalnych.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Program Microsoft Windows i interfejsu API System.Speech Zaakceptuj wszystkie prawidłowe kody krajów języka. Aby wykonać rozpoznawanie mowy, przy użyciu języka określonego w `CultureInfo` argumentu, aparatu rozpoznawania mowy, który obsługuje kod kraju języka musi być zainstalowany. Aparatów rozpoznawania mowy, które są dostarczane z programem Microsoft Windows 7 współpracować z poniższych kodów kraju języka.  
  
-   en-GB. Angielski (Zjednoczone Królestwo)  
  
-   en-US. Angielski (Stany Zjednoczone)  
  
-   de-DE. Niemiecki (Niemcy)  
  
-   es-ES. Hiszpański (Hiszpania)  
  
-   fr-FR. Francuski (Francja)  
  
-   ja-JP. Japoński (Japonia)  
  
-   zh-CN. Chiński (Chiny)  
  
-   zh-TW. Chiński (Tajwan)  
  
 Kody dwuliterowych języka, takich jak "en", "fr" lub "es" są również dozwolone.  
  
 Zanim aparatu rozpoznawania mowy, można rozpocząć rozpoznawania, należy załadować gramatyki rozpoznawania mowy co najmniej jedną i konfigurowanie danych wejściowych przez aparat rozpoznawania.  
  
 Aby załadować gramatyki, należy wywołać <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metody.  
  
 Aby skonfigurować dane wejściowe audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsoli, który demonstruje rozpoznawania mowy podstawowe i inicjuje aparatu rozpoznawania mowy, dla ustawień regionalnych en US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Żadne aparaty rozpoznawania mowy zainstalowane obsługi określonych ustawień regionalnych lub <paramref name="culture" /> jest niezmiennej kultury.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="Culture" /> jest <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Informacje dotyczące aparatu rozpoznawania mowy określonych.</param>
        <summary>Inicjuje nowe wystąpienie klasy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> korzystając z informacji podanych w <see cref="T:System.Speech.Recognition.RecognizerInfo" /> obiektu w celu określenia rozpoznawania do użycia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Dla każdej z aparatów rozpoznawania mowy zainstalowane, można utworzyć wystąpienie tej klasy. Aby uzyskać informacje o tym, które są zainstalowane aparatów rozpoznawania gestów, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metody.  
  
 Zanim aparatu rozpoznawania mowy, można rozpocząć rozpoznawania, należy załadować gramatyki rozpoznawania mowy co najmniej jedną i konfigurowanie danych wejściowych przez aparat rozpoznawania.  
  
 Aby załadować gramatyki, należy wywołać <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metody.  
  
 Aby skonfigurować dane wejściowe audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsoli, który demonstruje rozpoznawania mowy podstawowe i inicjuje aparatu rozpoznawania mowy, która obsługuje język angielski.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Nazwa tokenu aparatu rozpoznawania mowy do użycia.</param>
        <summary>Inicjuje nowe wystąpienie klasy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> klasy z parametrem ciągu, który określa nazwę rozpoznawania do użycia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Nazwa tokenu aparatu rozpoznawania jest wartością <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> właściwość <xref:System.Speech.Recognition.RecognizerInfo> obiektu zwróconego przez <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> właściwość aparat rozpoznawania. Aby uzyskać zbiór wszystkich zainstalowanych aparatów rozpoznawania, używa się statycznej <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metody.  
  
 Zanim aparatu rozpoznawania mowy, można rozpocząć rozpoznawania, należy załadować gramatyki rozpoznawania mowy co najmniej jedną i konfigurowanie danych wejściowych przez aparat rozpoznawania.  
  
 Aby załadować gramatyki, należy wywołać <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metody.  
  
 Aby skonfigurować dane wejściowe audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono część aplikację konsolową która demonstruje rozpoznawania mowy podstawowe i tworzy wystąpienie 8.0 aparatu rozpoznawania mowy, for Windows (Angielski - Stany Zjednoczone).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Nie rozpoznawania mowy, z tą nazwą tokenu jest zainstalowany, lub <paramref name="recognizerId" /> jest pustym ciągiem ("").</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="recognizerId" /> jest <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera format audio one odbierane przez <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Format audio w danych wejściowych na <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wystąpienia lub <see langword="null" /> Jeśli danych wejściowych nie jest skonfigurowany lub równa null danych wejściowych.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby skonfigurować dane wejściowe audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 W poniższym przykładzie użyto <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> do wyświetlania danych audio format.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera poziom dźwięku odbierane przez <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Poziom audio w danych wejściowych do rozpoznawania mowy, od 0 do 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wartość 0 oznacza wyciszenia, a 100 oznacza maksymalną woluminu danych wejściowych.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> raporty stopień dane wejściowe audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Zgłasza zdarzenie, to wiele razy na sekundę. Częstotliwość, z którym zdarzenie jest wywoływane, zależy od komputera, na którym działa aplikacja.  
  
 Aby uzyskać poziom audio w momencie wystąpienia zdarzenia, użyj <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> właściwości skojarzonego <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Aby uzyskać bieżący poziom audio w danych wejściowych dla aparatu rozpoznawania, użyj aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> właściwości.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład dodaje program obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> zdarzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine> obiektu. Program obsługi danych wyjściowych nowy poziom audio do konsoli.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera bieżącą lokalizację w generowanych przez urządzenia, który dostarcza dane wejściowe strumienia audio <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Bieżąca lokalizacja w strumienia audio generowanych przez urządzenia wejściowego.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Właściwości odwołuje się do pozycji urządzenia wejściowego w jego wygenerowany strumień audio. Z kolei <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> właściwości odwołuje się do pozycji aparat rozpoznawania dane wejściowe audio. Te pozycje mogą być różne. Na przykład, jeśli aparat rozpoznawania otrzymał dane wejściowe, do których nie ma jeszcze generowany wynik rozpoznawania, a następnie wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> właściwość jest mniejsza niż wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> właściwości.  
  
   
  
## Examples  
 W poniższym przykładzie aparatu rozpoznawania mowy w trakcie używa gramatyki dyktowanie w celu dopasowania danych wejściowych mowy. Program obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> zdarzeń zapisuje do konsoli <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> podczas rozpoznawania mowy wykrywa mowę na dane wejściowe.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wykryje problem w sygnału dźwiękowego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać, jaki problem wystąpił, użyj <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> właściwości skojarzonego <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie zdefiniowano program obsługi zdarzeń, który gromadzi informacje o <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> zdarzeń.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera stan audio one odbierane przez <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Stan wejścia audio do rozpoznawania mowy.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> Właściwość reprezentuje stan audio z elementem członkowskim <xref:System.Speech.Recognition.AudioState> wyliczenia.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywoływane, gdy zmiany stanu które usłyszysz odbierane przez <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać stan audio w momencie wystąpienia zdarzenia, użyj <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> właściwości skojarzonego <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Aby uzyskać bieżący stan audio w danych wejściowych dla aparatu rozpoznawania, użyj aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> właściwości. Aby uzyskać więcej informacji na temat stanu audio, zobacz <xref:System.Speech.Recognition.AudioState> wyliczenia.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie użyto procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> zdarzenie, aby zapisać aparat rozpoznawania przez nowe <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> w konsoli każdy czasu zmiany, przy użyciu członkiem <xref:System.Speech.Recognition.AudioState> wyliczenia.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia przedział czasu, przez który <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> akceptuje wejściowe zawierającego tylko hałas w tle, zanim zostanie zakończony i rozpoznawania.</summary>
        <value>Czas trwania interwału czasu.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy aparat rozpoznawania mowy ma algorytm rozróżnienie między wyciszenia i mowy. Aparat rozpoznawania klasyfikuje, ponieważ hałas w tle żadnych innych wyciszenia wejściowej niezgodny początkową regułę dowolnego aparatu rozpoznawania załadowane i włączone gramatyki rozpoznawania mowy. Jeśli aparat rozpoznawania odbiera tylko hałas w tle i wyciszenia w ciągu interwału limitu czasu babble, aparat rozpoznawania Kończenie znajdujących tej operacji rozpoznawania.  
  
-   Dla operacji asynchronicznej rozpoznawania, wywołuje aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzeń, gdzie <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> właściwość `true`i <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> właściwość `null`.  
  
-   Operacje synchroniczne rozpoznawanie i emulacji, aparat rozpoznawania zwraca `null`, zamiast prawidłowego <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Okres limitu czasu babble jest równa 0, aparat rozpoznawania nie wykonuje sprawdzenie limitu czasu babble. Interwał limitu czasu może być dowolną wartością nieujemną. Wartość domyślna to 0 sekund.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowa, która ustawia <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine> przed zainicjowaniem rozpoznawanie mowy. Programy obsługi dla aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzeń wyjściowych informacji o zdarzeniach w konsoli, aby zademonstrować sposób, w jaki <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine> wpływających na funkcjonowanie rozpoznawania.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Ta właściwość jest równa mniejszy niż 0 sekund.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Usuwa <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiektu.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Usuwa <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiektu.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing"><see langword="true" /> Aby zwolnić zasoby zarządzane i niezarządzane; <see langword="false" /> aby zwolnić tylko niezarządzane zasoby.</param>
        <summary>Usuwa <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiektu i zwalnia zasoby używane podczas sesji.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuluje dane wejściowe do rozpoznawania mowy, przy użyciu tekstu, zamiast audio rozpoznawania mowy synchroniczne.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody obejścia wejścia audio systemu i Przekaż do urządzenia rozpoznającego jako <xref:System.String> obiektów lub jako tablicę <xref:System.Speech.Recognition.RecognizedWordUnit> obiektów. Może to być przydatne podczas testowania i debugowania aplikacji lub gramatyki. Aby określić, czy słowo jest gramatyki i jakie semantyki są zwracane, gdy rozpoznano słowa można na przykład korzystania z emulacji. Użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> metodę, aby wyłączyć wejścia audio do rozpoznawania mowy, podczas wykonywania operacji emulacji.  
  
 Generuje aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana. Aparat rozpoznawania ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału.  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult> Generowane przez aparat rozpoznawania mowy w odpowiedzi na dane wejściowe emulowanej obiektu ma wartość `null` dla jego <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> właściwości.  
  
 Aby emulować asynchroniczne rozpoznawanie, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Dane wejściowe dla operacji rozpoznawania.</param>
        <summary>Emuluje dane wejściowe frazy aparatu rozpoznawania mowy, przy użyciu tekstu, zamiast audio rozpoznawania mowy synchroniczne.</summary>
        <returns>Wynik operacji rozpoznawania lub <see langword="null" /> Jeśli operacja zakończy się niepowodzeniem lub nie włączono aparatu rozpoznawania.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Generuje aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana.  
  
 Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 Ignoruj wielkość liter i znaków szerokości podczas stosowania reguły gramatyki sformułować danych wejściowych. Aby uzyskać więcej informacji na temat porównania tego typu, zobacz <xref:System.Globalization.CompareOptions> wartości wyliczenia <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> i <xref:System.Globalization.CompareOptions.IgnoreWidth>. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktować znaków interpunkcyjnych jako dane wejściowe literału.  
  
   
  
## Examples  
 W poniższym przykładzie kodu jest częścią aplikację konsolową, która demonstruje emulowanej danych wejściowych, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy. Przykład generuje następujące dane wyjściowe.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma żadnych gramatyki rozpoznawania mowy, załadowane.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> ciąg pusty ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Tablica jednostki programu word zawierającego dane wejściowe dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla operacji rozpoznawania emulowane.</param>
        <summary>Emuluje dane wejściowe określone słowa do rozpoznawania mowy, przy użyciu tekstu, zamiast audio rozpoznawania mowy synchroniczne i określa, jak aparat rozpoznawania obsługuje Unicode porównanie wyrazów i gramatyki rozpoznawania mowy załadowane.</summary>
        <returns>Wynik operacji rozpoznawania lub <see langword="null" /> Jeśli operacja zakończy się niepowodzeniem lub nie włączono aparatu rozpoznawania.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Generuje aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana.  
  
 Aparat rozpoznawania używa `compareOptions` po stosuje reguły gramatyki sformułować danych wejściowych. Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 ignorowanie wielkości liter, jeśli <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> lub <xref:System.Globalization.CompareOptions.IgnoreCase> wartość jest obecna. Aparat rozpoznawania zawsze ignoruje szerokość znaków i nigdy nie ignoruje typu znaki Kana. Aparat rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału. Aby uzyskać więcej informacji na temat szerokość znaków i znaków Kana typu, zobacz <xref:System.Globalization.CompareOptions> wyliczenia.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma żadnych gramatyki rozpoznawania mowy, załadowane.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> zawiera co najmniej jeden <see langword="null" /> elementów.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> zawiera <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, lub <see cref="F:System.Globalization.CompareOptions.StringSort" /> flagi.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frazy danych wejściowych dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla operacji rozpoznawania emulowane.</param>
        <summary>Emuluje dane wejściowe frazy aparatu rozpoznawania mowy, przy użyciu tekstu, zamiast audio rozpoznawania mowy synchroniczne i określa, jak aparat rozpoznawania obsługuje porównanie Unicode frazy i gramatyki rozpoznawania mowy załadowane.</summary>
        <returns>Wynik operacji rozpoznawania lub <see langword="null" /> Jeśli operacja zakończy się niepowodzeniem lub nie włączono aparatu rozpoznawania.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Generuje aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana.  
  
 Aparat rozpoznawania używa `compareOptions` po stosuje reguły gramatyki sformułować danych wejściowych. Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 ignorowanie wielkości liter, jeśli <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> lub <xref:System.Globalization.CompareOptions.IgnoreCase> wartość jest obecna. Aparat rozpoznawania zawsze ignoruje szerokość znaków i nigdy nie ignoruje typu znaki Kana. Aparat rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału. Aby uzyskać więcej informacji na temat szerokość znaków i znaków Kana typu, zobacz <xref:System.Globalization.CompareOptions> wyliczenia.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma żadnych gramatyki rozpoznawania mowy, załadowane.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> ciąg pusty ("").</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> zawiera <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, lub <see cref="F:System.Globalization.CompareOptions.StringSort" /> flagi.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuluje dane wejściowe do rozpoznawania mowy, przy użyciu tekstu, zamiast audio rozpoznawania mowy asynchronicznego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody obejścia wejścia audio systemu i Przekaż do urządzenia rozpoznającego jako <xref:System.String> obiektów lub jako tablicę <xref:System.Speech.Recognition.RecognizedWordUnit> obiektów. Może to być przydatne podczas testowania i debugowania aplikacji lub gramatyki. Aby określić, czy słowo jest gramatyki i jakie semantyki są zwracane, gdy rozpoznano słowa można na przykład korzystania z emulacji. Użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> metodę, aby wyłączyć wejścia audio do rozpoznawania mowy, podczas wykonywania operacji emulacji.  
  
 Generuje aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana. Po ukończeniu operacji asynchronicznej rozpoznawania aparat rozpoznawania zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzeń. Aparat rozpoznawania ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako dane wejściowe literału.  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult> Generowane przez aparat rozpoznawania mowy w odpowiedzi na dane wejściowe emulowanej obiektu ma wartość `null` dla jego <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> właściwości.  
  
 Aby emulować rozpoznawania synchroniczne, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Dane wejściowe dla operacji rozpoznawania.</param>
        <summary>Emuluje dane wejściowe frazy aparatu rozpoznawania mowy, przy użyciu tekstu, zamiast audio rozpoznawania mowy asynchronicznego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Generuje aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana. Po ukończeniu operacji asynchronicznej rozpoznawania aparat rozpoznawania zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzeń.  
  
 Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 Ignoruj wielkość liter i znaków szerokości podczas stosowania reguły gramatyki sformułować danych wejściowych. Aby uzyskać więcej informacji na temat porównania tego typu, zobacz <xref:System.Globalization.CompareOptions> wartości wyliczenia <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> i <xref:System.Globalization.CompareOptions.IgnoreWidth>. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktować znaków interpunkcyjnych jako dane wejściowe literału.  
  
   
  
## Examples  
 W poniższym przykładzie kodu jest częścią aplikację konsolową, która demonstruje asynchronicznego emulowanej danych wejściowych, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy. Przykład generuje następujące dane wyjściowe.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania ma gramatyki rozpoznawania mowy, nie załadowano lub aparat rozpoznawania ma operacji asynchronicznych rozpoznawania, która nie została jeszcze zakończona.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> ciąg pusty ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Tablica jednostki programu word zawierającego dane wejściowe dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla operacji rozpoznawania emulowane.</param>
        <summary>Dane wejściowe określone słowa do rozpoznawania mowy, użycie tablicy emuluje <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> obiekty zamiast audio rozpoznawania mowy asynchronicznego i określa, jak aparat rozpoznawania obsługuje Unicode porównanie wyrazów i załadować mowy rozpoznawanie gramatyki.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Generuje aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana. Po ukończeniu operacji asynchronicznej rozpoznawania aparat rozpoznawania zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzeń.  
  
 Aparat rozpoznawania używa `compareOptions` po stosuje reguły gramatyki sformułować danych wejściowych. Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 ignorowanie wielkości liter, jeśli <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> lub <xref:System.Globalization.CompareOptions.IgnoreCase> wartość jest obecna. Aparatów rozpoznawania zawsze Ignoruj szerokość znaków i nigdy nie Ignoruj typ znaki Kana. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktować znaków interpunkcyjnych jako dane wejściowe literału. Aby uzyskać więcej informacji na temat szerokość znaków i znaków Kana typu, zobacz <xref:System.Globalization.CompareOptions> wyliczenia.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania ma gramatyki rozpoznawania mowy, nie załadowano lub aparat rozpoznawania ma operacji asynchronicznych rozpoznawania, która nie została jeszcze zakończona.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> zawiera co najmniej jeden <see langword="null" /> elementów.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> zawiera <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, lub <see cref="F:System.Globalization.CompareOptions.StringSort" /> flagi.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frazy danych wejściowych dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla operacji rozpoznawania emulowane.</param>
        <summary>Emuluje dane wejściowe frazy aparatu rozpoznawania mowy, przy użyciu tekstu, zamiast audio rozpoznawania mowy asynchronicznego i określa, jak aparat rozpoznawania obsługuje porównanie Unicode frazy i gramatyki rozpoznawania mowy załadowane.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Generuje aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia tak, jakby operacji rozpoznawania nie jest emulowana. Po ukończeniu operacji asynchronicznej rozpoznawania aparat rozpoznawania zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzeń.  
  
 Aparat rozpoznawania używa `compareOptions` po stosuje reguły gramatyki sformułować danych wejściowych. Aparaty rozpoznawania, które są dostarczane z Vista i Windows 7 ignorowanie wielkości liter, jeśli <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> lub <xref:System.Globalization.CompareOptions.IgnoreCase> wartość jest obecna. Aparatów rozpoznawania zawsze Ignoruj szerokość znaków i nigdy nie Ignoruj typ znaki Kana. Aparatów rozpoznawania również ignoruje nowe wiersze i dodatkowy biały znak i traktować znaków interpunkcyjnych jako dane wejściowe literału. Aby uzyskać więcej informacji na temat szerokość znaków i znaków Kana typu, zobacz <xref:System.Globalization.CompareOptions> wyliczenia.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania ma gramatyki rozpoznawania mowy, nie załadowano lub aparat rozpoznawania ma operacji asynchronicznych rozpoznawania, która nie została jeszcze zakończona.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> ciąg pusty ("").</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> zawiera <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, lub <see cref="F:System.Globalization.CompareOptions.StringSort" /> flagi.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Kończenie znajdujących się w operacji asynchronicznej rozpoznawania emulowanej danych wejściowych.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metoda rozpoczyna operację asynchroniczną rozpoznawania. <xref:System.Speech.Recognition.SpeechRecognitionEngine> Zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzenie, kiedy go Kończenie znajdujących się w operacji asynchronicznej.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Może wywoływać operację <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia. <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Zdarzeń jest ostatnim tych zdarzeń, że aparat rozpoznawania zgłasza dla danej operacji.  
  
 Rozpoznawanie emulowanej zakończyło się pomyślnie, można przejść do wyników rozpoznawanie przy użyciu jednej z następujących czynności:  
  
-   <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> Właściwość <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> obiekt obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzeń.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Właściwość <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> obiekt obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeń.  
  
 Jeśli emulowanej rozpoznawania zakończyła się niepowodzeniem, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenie jest zgłaszane w nie i <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> będzie miał wartość null.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> pochodzi od klasy <xref:System.ComponentModel.AsyncCompletedEventArgs>.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> pochodzi od klasy <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową która ładuje gramatyki rozpoznawania mowy i demonstruje asynchronicznego emulowanej danych wejściowych, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia interwał wyciszenia, który <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> będzie akceptować na końcu danych wejściowych jednoznaczną przed ukończeniem operacji rozpoznawania.</summary>
        <value>Czas trwania interwału wyciszenia.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Podczas rozpoznawania danych wejściowych jest jednoznaczna, rozpoznawania mowy używa ten interwał limitu czasu. Na przykład gramatyki rozpoznawania mowy, który obsługuje rozpoznawanie albo "nowych gier," lub "nową grę", "nowych gier," jest jednoznaczna dane wejściowe, a "nową grę" jest niejednoznaczne dane wejściowe.  
  
 Ta właściwość określa, jak długo aparatu rozpoznawania mowy będzie czekać na dodatkowe dane wejściowe przed ukończeniem operacji rozpoznawania. Interwał limitu czasu może być z zakresu od 0 do 10 sekund (włącznie). Wartość domyślna wynosi 150 milisekund.  
  
 Aby ustawić interwał limitu czasu dla niejednoznaczne dane wejściowe, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Ta właściwość ma wartość mniejszą niż 0 sekund lub większa niż 10 sekund.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia interwał wyciszenia, który <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> będzie akceptować na końcu niejednoznacznego wejścia przed ukończeniem operacji rozpoznawania.</summary>
        <value>Czas trwania interwału wyciszenia.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania mowy używa tego interwału limitu czasu podczas rozpoznawania danych wejściowych jest niejednoznaczny. Na przykład gramatyki rozpoznawania mowy, który obsługuje rozpoznawanie albo "nowych gier," lub "nową grę", "nowych gier," jest jednoznaczna dane wejściowe, a "nową grę" jest niejednoznaczne dane wejściowe.  
  
 Ta właściwość określa, jak długo aparatu rozpoznawania mowy będzie czekać na dodatkowe dane wejściowe przed ukończeniem operacji rozpoznawania. Interwał limitu czasu może być z zakresu od 0 do 10 sekund (włącznie). Wartość domyślna to 500 milisekund.  
  
 Aby ustawić interwał limitu czasu dla danych wejściowych jednoznaczna, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Ta właściwość ma wartość mniejszą niż 0 sekund lub większa niż 10 sekund.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera kolekcję <see cref="T:System.Speech.Recognition.Grammar" /> obiekty, które są ładowane w tym <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wystąpienia.</summary>
        <value>Kolekcja <see cref="T:System.Speech.Recognition.Grammar" /> obiektów.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Poniższy przykład wyświetla informacje o konsoli dla poszczególnych gramatyki rozpoznawania mowy, która jest aktualnie załadowana przez aparat rozpoznawania mowy.  
  
> [!IMPORTANT]
>  Skopiuj kolekcji gramatyki, aby uniknąć błędów, jeśli kolekcja jest modyfikowana podczas, gdy ta metoda wylicza elementów kolekcji.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia przedział czasu, przez który <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> akceptuje wejściowe zawierającego tylko wyciszenia, zanim zostanie zakończony i rozpoznawania.</summary>
        <value>Czas trwania interwału wyciszenia.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy aparat rozpoznawania mowy ma algorytm rozróżnienie między wyciszenia i mowy. W przypadku danych wejściowych rozpoznawania wyciszenia podczas początkowego wyciszenia limitu czasu, aparat rozpoznawania Kończenie znajdujących tej operacji rozpoznawania.  
  
-   Operacje asynchroniczne rozpoznawanie i emulacji, wywołuje aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzeń, gdzie <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> właściwość jest `true`i <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> właściwość `null`.  
  
-   Operacje synchroniczne rozpoznawanie i emulacji, aparat rozpoznawania zwraca `null`, zamiast prawidłowego <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Interwał limitu czasu początkowej wyciszenia jest równa 0, aparat rozpoznawania nie powoduje wykonania początkowej wyciszenia limitu czasu wyboru. Interwał limitu czasu może być dowolną wartością nieujemną. Wartość domyślna to 0 sekund.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowe. Przykład ustawia <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine> przed zainicjowaniem rozpoznawanie mowy. Programy obsługi dla aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzeń wyjściowych informacji o zdarzeniach w konsoli, aby zademonstrować sposób, w jaki <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine> właściwości wpływających na funkcjonowanie rozpoznawania.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Ta właściwość jest równa mniejszy niż 0 sekund.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Zwraca informacje dotyczące wszystkich aparatów rozpoznawania mowy zainstalowanych w bieżącym systemie.</summary>
        <returns>Kolekcja tylko do odczytu <see cref="T:System.Speech.Recognition.RecognizerInfo" /> obiekty, które opisują odmówiono zainstalowana.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać informacje o bieżącym rozpoznawania, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> właściwości.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowe. W przykładzie użyto zbiorze zwróconym przez <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metody do znalezienia aparatu rozpoznawania mowy, która obsługuje język angielski.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Obiekt gramatyki do załadowania.</param>
        <summary>Ładuje synchronicznie <see cref="T:System.Speech.Recognition.Grammar" /> obiektu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania zgłasza wyjątek, jeśli <xref:System.Speech.Recognition.Grammar> obiekt jest już załadowany, są ładowane asynchronicznie lub nie udało się załadować do dowolnego aparatu rozpoznawania. Nie można załadować takie same <xref:System.Speech.Recognition.Grammar> obiektu do wielu wystąpień <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Zamiast tego utwórz nową <xref:System.Speech.Recognition.Grammar> obiekt dla każdego <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia.  
  
 Jeśli działa aparat rozpoznawania, aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> wstrzymać aparatu rozpoznawania mowy, przed ładowania, zwalnianie, włączanie lub wyłączanie gramatyki.  
  
 Podczas ładowania gramatyki go jest domyślnie włączona. Aby wyłączyć załadować gramatyki, użyj <xref:System.Speech.Recognition.Grammar.Enabled%2A> właściwości.  
  
 Aby załadować <xref:System.Speech.Recognition.Grammar> obiektu asynchronicznie, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowe. W przykładzie jest tworzony <xref:System.Speech.Recognition.DictationGrammar> i ładuje je do rozpoznawania mowy.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> nie jest w nieprawidłowym stanie.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Gramatyka rozpoznawania mowy do załadowania.</param>
        <summary>Ładuje asynchronicznie gramatyki rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Po zakończeniu ładowania przez aparat rozpoznawania <xref:System.Speech.Recognition.Grammar> obiektu zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> zdarzeń. Aparat rozpoznawania zgłasza wyjątek, jeśli <xref:System.Speech.Recognition.Grammar> obiekt jest już załadowany, są ładowane asynchronicznie lub nie udało się załadować do dowolnego aparatu rozpoznawania. Nie można załadować takie same <xref:System.Speech.Recognition.Grammar> obiektu do wielu wystąpień <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Zamiast tego utwórz nową <xref:System.Speech.Recognition.Grammar> obiekt dla każdego <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia.  
  
 Jeśli działa aparat rozpoznawania, aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> wstrzymać aparatu rozpoznawania mowy, przed ładowania, zwalnianie, włączanie lub wyłączanie gramatyki.  
  
 Podczas ładowania gramatyki go jest domyślnie włączona. Aby wyłączyć załadować gramatyki, użyj <xref:System.Speech.Recognition.Grammar.Enabled%2A> właściwości.  
  
 Aby załadować synchronicznie gramatyki rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metody.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> nie jest w nieprawidłowym stanie.</exception>
        <exception cref="T:System.OperationCanceledException">Operacja asynchroniczna została anulowana.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> zakończeniu asynchroniczne ładowanie <see cref="T:System.Speech.Recognition.Grammar" /> obiektu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metoda inicjuje operację asynchroniczną. <xref:System.Speech.Recognition.SpeechRecognitionEngine> Zgłasza to zdarzenie po zakończeniu tej operacji. Aby uzyskać <xref:System.Speech.Recognition.Grammar> obiektu, że aparat rozpoznawania załadowany, należy użyć <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> właściwości skojarzonego <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Aby uzyskać bieżącą <xref:System.Speech.Recognition.Grammar> obiektów aparat rozpoznawania został załadowany, użyj aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> właściwości.  
  
 Jeśli działa aparat rozpoznawania, aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> wstrzymać aparatu rozpoznawania mowy, przed ładowania, zwalnianie, włączanie lub wyłączanie gramatyki.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład tworzy aparatu rozpoznawania mowy w procesie, a następnie tworzy dwa typy gramatyki rozpoznawania konkretnych słów i akceptowania dyktowanie bezpłatne. Przykład tworzy <xref:System.Speech.Recognition.Grammar> obiektu z każdego gramatyki rozpoznawania mowy zakończonych asynchronicznie ładuje <xref:System.Speech.Recognition.Grammar> obiekty do <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia. Programy obsługi dla aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia zapisu do konsoli nazwę <xref:System.Speech.Recognition.Grammar> obiektu, który został użyty do rozpoznawania i tekst wynik rozpoznawania odpowiednio.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia maksymalną liczbę wyników rozpoznawania alternatywnego, który <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> zwraca dla każdej operacji rozpoznawania.</summary>
        <value>Liczba alternatywne wyników do zwrócenia.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Właściwość <xref:System.Speech.Recognition.RecognitionResult> klasy zawiera kolekcję <xref:System.Speech.Recognition.RecognizedPhrase> obiektami, które reprezentują możliwe interpretacji danych wejściowych.  
  
 Wartością domyślną dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> wynosi 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> jest ustawiona na wartość mniejszą niż 0.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nazwa ustawienia do zwrócenia.</param>
        <summary>Zwraca wartości ustawienia dla aparatu rozpoznawania.</summary>
        <returns>Wartość ustawienia.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ustawienia rozpoznawania może zawierać ciąg, 64-bitowa liczba całkowita lub dane adresu pamięci. W poniższej tabeli opisano ustawienia, które są zdefiniowane dla interfejsu API rozpoznawania mowy firmy Microsoft (nieokreślone)-rozpoznawania zgodne. Poniższe ustawienia muszą mieć ten sam zakres każdego rozpoznawania, która obsługuje dane ustawienie. Zgodne nieokreślone rozpoznawania nie jest wymagany do obsługi tych ustawień i może obsługiwać inne ustawienia.  
  
|Nazwa|Opis|  
|----------|-----------------|  
|`ResourceUsage`|Określa użycie procesora CPU przez aparat rozpoznawania. Zakres jest z zakresu od 0 do 100. Wartością domyślną jest 50.|  
|`ResponseSpeed`|Określa długość wyciszenia na końcu danych wejściowych jednoznaczną przed aparatu rozpoznawania mowy zakończeniem operacji rozpoznawania. Zakres jest z zakresu od 0 do 10 000 milisekund (ms). To ustawienie odpowiada aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> właściwości.  Domyślne = 150ms.|  
|`ComplexResponseSpeed`|Określa długość wyciszenia na końcu niejednoznacznego wejścia przed aparatu rozpoznawania mowy zakończeniem operacji rozpoznawania. Zakres jest z zakresu od 0 do 10,000ms. To ustawienie odpowiada aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości. Domyślnie 500 MS.|  
|`AdaptationOn`|Wskazuje, czy dostosowanie modelu akustycznego jest włączone (wartość = `1`). lub Wył. (wartość = `0`). Wartość domyślna to `1` (dalej).|  
|`PersistedBackgroundAdaptation`|Wskazuje, czy dostosowania tła jest włączone (wartość = `1`). lub Wył. (wartość = `0`), będzie się powtarzał ustawienie w rejestrze. Wartość domyślna to `1` (dalej).|  
  
 Aby zaktualizować ustawienia dla aparatu rozpoznawania, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową, która wyświetla wartości szereg ustawień zdefiniowanych przez aparat rozpoznawania, który obsługuje ustawień regionalnych en US. Przykład generuje następujące dane wyjściowe.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> ciąg pusty ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Aparat rozpoznawania nie ma ustawienie o takiej nazwie.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Rozpoczyna operację rozpoznawania mowy synchroniczne.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody wykonywać operacji rozpoznawania pojedynczy, synchroniczne. Aparat rozpoznawania wykonuje tej operacji względem jego gramatyki rozpoznawania mowy załadowane i włączone.  
  
 Podczas wywołania tej metody aparat rozpoznawania może zgłosić następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wywoływane, gdy aparat rozpoznawania wykrywa dane wejściowe, którą można zidentyfikować jako mowy.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Wywoływane, gdy dane wejściowe tworzy niejednoznaczne dopasowanie za pomocą jednego aktywnego gramatyki.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wywoływane, gdy aparat rozpoznawania Kończenie znajdujących się w operacji rozpoznawania.  
  
 Aparat rozpoznawania zgłaszaj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzenie, kiedy przy użyciu jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metody.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Metody zwracają <xref:System.Speech.Recognition.RecognitionResult> obiektu lub `null` Jeśli operacja zakończy się niepowodzeniem lub nie włączono aparatu rozpoznawania.  
  
 Operacja synchroniczna rozpoznawania może się nie powieść z następujących powodów:  
  
-   Mowy nie zostanie wykryty w odstępach czasu wygaśnięcia dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości lub `initialSilenceTimeout` parametru <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metody.  
  
-   Aparat rozpoznawania wykrywa mowy, ale umożliwia znalezienie żadnych dopasowań w żadnym z załadowane i włączone <xref:System.Speech.Recognition.Grammar> obiektów.  
  
 Aby zmodyfikować sposób obsługiwania przez aparat rozpoznawania czas mowy wyciszenia w odniesieniu do rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Musi mieć co najmniej jeden <xref:System.Speech.Recognition.Grammar> obiektu załadowane przed wykonaniem rozpoznawania. Aby załadować gramatyki rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metody.  
  
 Aby wykonać rozpoznawanie asynchronicznego, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Wykonuje operację rozpoznawania mowy synchroniczne.</summary>
        <returns>Wynik rozpoznawania dla danych wejściowych, lub <see langword="null" /> Jeśli operacja zakończy się niepowodzeniem lub nie włączono aparatu rozpoznawania.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda wykonuje operację pojedynczego rozpoznawania. Aparat rozpoznawania wykonuje tej operacji względem jego gramatyki rozpoznawania mowy załadowane i włączone.  
  
 Podczas wywołania tej metody aparat rozpoznawania może zgłosić następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wywoływane, gdy aparat rozpoznawania wykrywa dane wejściowe, którą można zidentyfikować jako mowy.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Wywoływane, gdy dane wejściowe tworzy niejednoznaczne dopasowanie za pomocą jednego aktywnego gramatyki.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wywoływane, gdy aparat rozpoznawania Kończenie znajdujących się w operacji rozpoznawania.  
  
 Aparat rozpoznawania zgłaszaj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzeń przy użyciu tej metody.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> Metoda zwraca <xref:System.Speech.Recognition.RecognitionResult> obiektu lub `null` Jeśli operacja zakończy się niepowodzeniem.  
  
 Operacja synchroniczna rozpoznawania może się nie powieść z następujących powodów:  
  
-   Mowy nie zostanie wykryty w odstępach czasu wygaśnięcia dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości.  
  
-   Aparat rozpoznawania wykrywa mowy, ale umożliwia znalezienie żadnych dopasowań w żadnym z załadowane i włączone <xref:System.Speech.Recognition.Grammar> obiektów.  
  
 Aby wykonać rozpoznawanie asynchronicznego, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowe. W przykładzie jest tworzony <xref:System.Speech.Recognition.DictationGrammar>, ładuje je do rozpoznawania mowy w procesie i wykonuje jedną operację rozpoznawania.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">Przedział czasu, który akceptuje rozpoznawania mowy wejściowe, zawierający tylko wyciszenia, zanim zostanie zakończony i rozpoznawania.</param>
        <summary>Wykonuje operację rozpoznawania mowy synchroniczne wyciszenia początkowej określony limit czasu.</summary>
        <returns>Wynik rozpoznawania dla danych wejściowych, lub <see langword="null" /> Jeśli operacja zakończy się niepowodzeniem lub nie włączono aparatu rozpoznawania.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania mowy wykrywa mowy w przedziale czasu określonym przez `initialSilenceTimeout` argument <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> wykonuje operację rozpoznawania pojedynczego, a następnie kończy.  `initialSilenceTimeout` Parametr zastępuje aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości.  
  
 Podczas wywołania tej metody aparat rozpoznawania może zgłosić następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wywoływane, gdy aparat rozpoznawania wykrywa dane wejściowe, którą można zidentyfikować jako mowy.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Wywoływane, gdy dane wejściowe tworzy niejednoznaczne dopasowanie za pomocą jednego aktywnego gramatyki.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wywoływane, gdy aparat rozpoznawania Kończenie znajdujących się w operacji rozpoznawania.  
  
 Aparat rozpoznawania zgłaszaj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzeń przy użyciu tej metody.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> Metoda zwraca <xref:System.Speech.Recognition.RecognitionResult> obiektu lub `null` Jeśli operacja zakończy się niepowodzeniem.  
  
 Operacja synchroniczna rozpoznawania może się nie powieść z następujących powodów:  
  
-   Mowy nie zostanie wykryty w odstępach czasu wygaśnięcia dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> lub `initialSilenceTimeout` parametru.  
  
-   Aparat rozpoznawania wykrywa mowy, ale umożliwia znalezienie żadnych dopasowań w żadnym z załadowane i włączone <xref:System.Speech.Recognition.Grammar> obiektów.  
  
 Aby wykonać rozpoznawanie asynchronicznego, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowe. W przykładzie jest tworzony <xref:System.Speech.Recognition.DictationGrammar>, ładuje je do rozpoznawania mowy w procesie i wykonuje jedną operację rozpoznawania.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Rozpoczyna operację rozpoznawania mowy asynchronicznego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody wykonywania pojedyncze lub wielokrotne, operacje asynchroniczne rozpoznawanie. Aparat rozpoznawania wykonuje każdej operacji względem jego gramatyki rozpoznawania mowy załadowane i włączone.  
  
 Podczas wywołania tej metody aparat rozpoznawania może zgłosić następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wywoływane, gdy aparat rozpoznawania wykrywa dane wejściowe, którą można zidentyfikować jako mowy.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Wywoływane, gdy dane wejściowe tworzy niejednoznaczne dopasowanie za pomocą jednego aktywnego gramatyki.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wywoływane, gdy aparat rozpoznawania Kończenie znajdujących się w operacji rozpoznawania.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wywołane, gdy <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> zakończeniu operacji.  
  
 Aby pobrać wynik operacji asynchronicznej rozpoznawania, Dołącz program obsługi zdarzeń dla aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeń. Aparat rozpoznawania zgłasza to zdarzenie po każdym pomyślnym ukończeniu operacji rozpoznawania synchroniczna lub asynchroniczna. Jeśli rozpoznawanie zakończyła się niepowodzeniem, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> właściwość <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> obiektu, który jest dostępny w obsłudze dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> będą zdarzenia `null`.  
  
 Operacji asynchronicznych rozpoznawania może się nie powieść z następujących powodów:  
  
-   Mowy nie zostanie wykryty w odstępach czasu wygaśnięcia dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości.  
  
-   Aparat rozpoznawania wykrywa mowy, ale umożliwia znalezienie żadnych dopasowań w żadnym z załadowane i włączone <xref:System.Speech.Recognition.Grammar> obiektów.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine> Musi mieć co najmniej jeden <xref:System.Speech.Recognition.Grammar> obiektu załadowane przed wykonaniem rozpoznawania. Aby załadować gramatyki rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metody.  
  
-   Aby zmodyfikować sposób obsługiwania przez aparat rozpoznawania czas mowy wyciszenia w odniesieniu do rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości.  
  
-   Aby wykonać rozpoznawanie synchroniczne, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Wykonuje operację rozpoznawania mowy w jednym, asynchronicznego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda wykonuje operację rozpoznawania pojedynczego, asynchronicznego. Aparat rozpoznawania wykonuje operację względem jego gramatyki rozpoznawania mowy załadowane i włączone.  
  
 Podczas wywołania tej metody aparat rozpoznawania może zgłosić następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wywoływane, gdy aparat rozpoznawania wykrywa dane wejściowe, którą można zidentyfikować jako mowy.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Wywoływane, gdy dane wejściowe tworzy niejednoznaczne dopasowanie za pomocą jednego aktywnego gramatyki.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wywoływane, gdy aparat rozpoznawania Kończenie znajdujących się w operacji rozpoznawania.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wywołane, gdy <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> zakończeniu operacji.  
  
 Aby pobrać wynik operacji asynchronicznej rozpoznawania, Dołącz program obsługi zdarzeń dla aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeń. Aparat rozpoznawania zgłasza to zdarzenie po każdym pomyślnym ukończeniu operacji rozpoznawania synchroniczna lub asynchroniczna. Jeśli rozpoznawanie zakończyła się niepowodzeniem, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> właściwość <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> obiektu, który jest dostępny w obsłudze dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> będą zdarzenia `null`.  
  
 Aby wykonać rozpoznawanie synchroniczne, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy asynchronicznego podstawowe. W przykładzie jest tworzony <xref:System.Speech.Recognition.DictationGrammar>, ładuje je do rozpoznawania mowy w procesie i wykonuje jedną operację asynchroniczną rozpoznawania. Programy obsługi zdarzeń są dołączone do zademonstrowania zdarzenia, które wywołuje aparat rozpoznawania, podczas operacji.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Wskazuje, czy należy wykonać jedną lub wiele operacji rozpoznawania.</param>
        <summary>Wykonuje co najmniej jednej operacji rozpoznawania mowy asynchronicznego.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli `mode` jest <xref:System.Speech.Recognition.RecognizeMode.Multiple>, aparat rozpoznawania kontynuuje wykonywanie operacji asynchronicznych rozpoznawania do momentu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> metoda jest wywoływana.  
  
 Podczas wywołania tej metody aparat rozpoznawania może zgłosić następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wywoływane, gdy aparat rozpoznawania wykrywa dane wejściowe, którą można zidentyfikować jako mowy.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Wywoływane, gdy dane wejściowe tworzy niejednoznaczne dopasowanie za pomocą jednego aktywnego gramatyki.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wywoływane, gdy aparat rozpoznawania Kończenie znajdujących się w operacji rozpoznawania.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wywołane, gdy <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> zakończeniu operacji.  
  
 Aby pobrać wynik operacji asynchronicznej rozpoznawania, Dołącz program obsługi zdarzeń dla aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeń. Aparat rozpoznawania zgłasza to zdarzenie po każdym pomyślnym ukończeniu operacji rozpoznawania synchroniczna lub asynchroniczna. Jeśli rozpoznawanie zakończyła się niepowodzeniem, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> właściwość <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> obiektu, który jest dostępny w obsłudze dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> będą zdarzenia `null`.  
  
 Operacji asynchronicznych rozpoznawania może się nie powieść z następujących powodów:  
  
-   Mowy nie zostanie wykryty w odstępach czasu wygaśnięcia dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> właściwości.  
  
-   Aparat rozpoznawania wykrywa mowy, ale umożliwia znalezienie żadnych dopasowań w żadnym z załadowane i włączone <xref:System.Speech.Recognition.Grammar> obiektów.  
  
 Aby wykonać rozpoznawanie synchroniczne, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy asynchronicznego podstawowe. W przykładzie jest tworzony <xref:System.Speech.Recognition.DictationGrammar>, ładuje je do rozpoznawania mowy w procesie i wykonuje wiele operacji asynchronicznych rozpoznawania. Operacje asynchroniczne są anulowane po 30 sekundach. Programy obsługi zdarzeń są dołączone do zademonstrowania zdarzenia, które wywołuje aparat rozpoznawania, podczas operacji.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Asynchroniczne rozpoznawanie kończy się bez oczekiwania na ukończenie bieżącej operacji rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda natychmiast Kończenie znajdujących się w asynchronicznej rozpoznawania. Jeśli bieżąca operacja asynchroniczne rozpoznawanie otrzymuje dane wejściowe, dane wejściowe zostały obcięte i zakończeniu operacji przy użyciu istniejących danych wejściowych. Generuje aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzenie, gdy operacja asynchroniczna została anulowana i ustawia <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> właściwość <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> do `true`. Ta metoda powoduje anulowanie operacji asynchronicznych inicjowane przez <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metody.  
  
 Aby zatrzymać asynchroniczne rozpoznawanie bez obcinania danych wejściowych, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> metody.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono część aplikację konsolową, która demonstruje użycie <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> metody. Przykład tworzy i ładuje gramatyki rozpoznawania mowy, inicjuje kontynuowanie operacji asynchronicznych rozpoznawanie i następnie wstrzymuje 2 sekundy, zanim go anuluje operację. Aparat rozpoznawania odbiera dane wejściowe z pliku, c:\temp\audioinput\sample.wav. Programy obsługi zdarzeń są dołączone do zademonstrowania zdarzenia, które wywołuje aparat rozpoznawania, podczas operacji.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Asynchroniczne rozpoznawanie zatrzymuje się po zakończeniu bieżącej operacji rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda Kończenie znajdujących się w asynchroniczne rozpoznawanie bez obcinania danych wejściowych. Jeśli bieżąca operacja asynchroniczne rozpoznawanie otrzymuje dane wejściowe, aparat rozpoznawania nadal akceptować dane wejściowe, aż do zakończenia bieżącej operacji rozpoznawania. Generuje aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzenie, gdy operacja asynchroniczna została zatrzymana, a następnie ustawia <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> właściwość <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> do `true`. Ta metoda zatrzymuje asynchronicznych operacji zainicjowanych przez <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metody.  
  
 Aby natychmiast anulować asynchroniczne rozpoznawanie przy użyciu tylko istniejących danych wejściowych, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> metody.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono część aplikację konsolową, która demonstruje użycie <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> metody. Przykład tworzy i ładuje gramatyki rozpoznawania mowy, inicjuje kontynuowanie operacji asynchronicznych rozpoznawanie i następnie wstrzymuje 2 sekundy, zanim go zatrzymuje operację. Aparat rozpoznawania odbiera dane wejściowe z pliku, c:\temp\audioinput\sample.wav. Programy obsługi zdarzeń są dołączone do zademonstrowania zdarzenia, które wywołuje aparat rozpoznawania, podczas operacji.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Kończenie znajdujących się w operacji asynchronicznej rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Obiektu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metoda inicjuje operację asynchroniczną rozpoznawania. Gdy aparat rozpoznawania Kończenie znajdujących się w operacji asynchronicznej, zgłasza to zdarzenie.  
  
 Za pomocą programu obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzeń, możesz uzyskać dostęp <xref:System.Speech.Recognition.RecognitionResult> w <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> obiektu. Jeśli rozpoznawanie zakończyła się niepowodzeniem, <xref:System.Speech.Recognition.RecognitionResult> będzie `null`. Aby ustalić, czy rozpoznawanie nie powiedzie się przyczyną przekroczenia limitu czasu lub przerwania wejścia audio, można uzyskać dostęp do właściwości dla <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, lub <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Zobacz <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> klasy, aby uzyskać więcej informacji.  
  
 Aby uzyskać szczegółowe informacje na temat najlepszych kandydatów rozpoznawania odrzucone, należy dołączyć program obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> zdarzeń.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje fraz, takie jak "Wyświetlić listę artystów kategorii jazz" lub "Wyświetlanie albumów gospel". W przykładzie użyto procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzenie, aby wyświetlić informacje o wynikach rozpoznawania w konsoli.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera bieżącą lokalizację <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> w danych wejściowych audio, która przetwarza.</summary>
        <value>Pozycja rozpoznawania wejścia audio, która przetwarza.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pozycja audio jest charakterystyczne dla każdego aparatu rozpoznawania mowy. Wartość zero w strumienia wejściowego zostanie nawiązane, gdy jest włączone.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Odwołania do właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine> położenie obiektu w jego wejścia audio. Z kolei <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> właściwości odwołuje się do pozycji urządzenia wejściowego w jego wygenerowany strumień audio. Te pozycje mogą być różne. Na przykład, jeśli aparat rozpoznawania otrzymał dane wejściowe, do których nie ma jeszcze generowany wynik rozpoznawania, a następnie wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> właściwość jest mniejsza niż wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera informacje o bieżącym wystąpieniu programu <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Informacje o bieżącym aparatu rozpoznawania mowy.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać informacje na temat wszystkich aparatów rozpoznawania mowy zainstalowanych dla bieżącego systemu, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład pobiera częściowa lista danych dla bieżącego w procesie rozpoznawania mowy. Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywoływane, gdy uruchomione <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wstrzymuje działanie, aby zaakceptować zmiany.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> wstrzymać działającego wystąpienia <xref:System.Speech.Recognition.SpeechRecognitionEngine> przed zmodyfikowaniem ustawienia lub jego <xref:System.Speech.Recognition.Grammar> obiektów. <xref:System.Speech.Recognition.SpeechRecognitionEngine> Zgłasza to zdarzenie, gdy wszystko będzie gotowe zaakceptować zmiany.  
  
 Na przykład <xref:System.Speech.Recognition.SpeechRecognitionEngine> jest wstrzymana, możesz można załadować, zwolnij, włączania i wyłączania <xref:System.Speech.Recognition.Grammar> obiektów, a następnie zmodyfikuj wartości <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> właściwości. Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono aplikację konsolową która ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiektów. Aplikacja używa <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodę, aby zażądać aparatu rozpoznawania mowy, aby wstrzymać, więc może ona odbierać aktualizacji. Aplikacja, a następnie ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiektu.  
  
 Przy każdej aktualizacji program obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń zapisuje nazwę i stan aktualnie załadowanych <xref:System.Speech.Recognition.Grammar> obiekty do konsoli. Gramatyki są ładowane i zwolniony, najpierw rozpoznaje nazwy zwierząt gospodarskich, a następnie utworzyć nazwy zwierząt gospodarskich nazw owoców, a następnie nazwy tylko owoce.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Żądania, że aparat rozpoznawania wstrzymuje się, aby zaktualizować jego stan.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda umożliwia synchronizowanie zmian do aparatu rozpoznawania. Na przykład jeśli obciążenia, lub zwolnij gramatyki rozpoznawania mowy, gdy aparat rozpoznawania przetwarza dane wejściowe, należy użyć tej metody i <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenie, aby zsynchronizować swoje zachowanie aplikacji ze stanem aparat rozpoznawania.  
  
 Gdy ta metoda jest wywoływana, aparat rozpoznawania wstrzymuje lub zakończeniu operacji asynchronicznych i generuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń. A <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> program obsługi zdarzeń można zmodyfikować stanu rozpoznawania Between uznanie operacji. Podczas obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenia, aparat rozpoznawania wstrzymuje, dopóki nie zwraca program obsługi zdarzeń.  
  
> [!NOTE]
>  Jeśli dane wejściowe dla aparatu rozpoznawania zostało zmienione przed wywołuje aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenia, żądania są odrzucane.  
  
 Gdy ta metoda jest wywoływana:  
  
-   Jeśli aparat rozpoznawania nie przetwarza dane wejściowe, aparat rozpoznawania natychmiast generuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń.  
  
-   Jeśli aparat rozpoznawania przetwarza dane wejściowe, który składa się z wyciszenia lub hałas w tle, aparat rozpoznawania wstrzymuje działanie rozpoznawanie i generuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń.  
  
-   Jeśli aparat rozpoznawania przetwarza dane wejściowe, który składa się z wyciszenia lub hałas w tle, aparat rozpoznawania zakończeniu operacji uznania, a następnie generuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń.  
  
 Podczas obsługi jest aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń:  
  
-   Aparat rozpoznawania nie przetwarza dane wejściowe, a wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> właściwości pozostają takie same.  
  
-   Aparat rozpoznawania w dalszym ciągu zbieranie danych wejściowych, a wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> można zmienić właściwości.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Żądania, że aparat rozpoznawania wstrzymuje się, aby zaktualizować jego stan.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Gdy aparat rozpoznawania generuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenia <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> jest `null`.  
  
 Aby dostarczyć token użytkownika, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody. Aby określić przesunięcie pozycji audio, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono aplikację konsolową która ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiektów. Aplikacja używa <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodę, aby zażądać aparatu rozpoznawania mowy, aby wstrzymać, więc może ona odbierać aktualizacji. Aplikacja, a następnie ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiektu.  
  
 Przy każdej aktualizacji program obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń zapisuje nazwę i stan aktualnie załadowanych <xref:System.Speech.Recognition.Grammar> obiekty do konsoli. Gramatyki są ładowane i zwolniony, najpierw rozpoznaje nazwy zwierząt gospodarskich, a następnie utworzyć nazwy zwierząt gospodarskich nazw owoców, a następnie nazwy tylko owoce.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informacje zdefiniowane przez użytkownika, który zawiera informacje dla tej operacji.</param>
        <summary>Żądania, który aparat rozpoznawania wstrzymuje się, aby zaktualizować jego stan oraz tokenu użytkownika dla skojarzonego zdarzenia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Gdy aparat rozpoznawania generuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenia <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> zawiera wartość `userToken` parametru.  
  
 Aby określić przesunięcie pozycji audio, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informacje zdefiniowane przez użytkownika, który zawiera informacje dla tej operacji.</param>
        <param name="audioPositionAheadToRaiseUpdate">Przesunięcie od bieżącego <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> opóźnienia żądania.</param>
        <summary>Żądania, który aparat rozpoznawania wstrzymuje się, aby zaktualizować jego stan oraz przesunięcia i tokenu użytkownika dla skojarzonego zdarzenia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania nie zostanie zainicjowane do momentu aparat rozpoznawania żądanie aktualizacji aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> jest równe bieżącego <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> oraz `audioPositionAheadToRaiseUpdate`.  
  
 Gdy aparat rozpoznawania generuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenia <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> zawiera wartość `userToken` parametru.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Audio strumienia wejściowego.</param>
        <param name="audioFormat">Format wejścia audio.</param>
        <summary>Konfiguruje <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiektów dla danych wejściowych ze strumienia audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania osiągnie koniec strumienia wejściowego, podczas operacji rozpoznawania, operacja rozpoznawania Kończenie znajdujących się przy użyciu dostępnych danych wejściowych. Wszystkie operacje kolejnego odczytu może generować wyjątek, chyba że aktualizujesz dane wejściowe dla aparatu rozpoznawania.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowe. W przykładzie użyto danych wejściowych z pliku audio example.wav, zawierający frazy "Testowanie testowania jednego dwóch trzy" i "mister cooper", oddzielone przerwie. Przykład generuje następujące dane wyjściowe.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Konfiguruje <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiektów dla danych wejściowych z domyślnego urządzenia audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje rozpoznawania mowy podstawowe. Przykład korzysta z danych wyjściowych z domyślnego urządzenia audio, wykonuje wiele operacji asynchronicznych rozpoznawanie i umożliwia zamknięcie po użytkownik utters frazy, "Zamknij".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Wyłącza dane wejściowe do rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Konfigurowanie <xref:System.Speech.Recognition.SpeechRecognitionEngine> obiektu nie można wprowadzać przy użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metody, lub robiąc to aparat rozpoznawania tymczasowo wyłączony.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Ścieżka pliku do użycia jako dane wejściowe.</param>
        <summary>Konfiguruje <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiektów dla danych wejściowych z pliku dźwiękowego format dźwięku (wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania osiągnie koniec pliku wejściowego podczas operacji rozpoznawania, operacja rozpoznawania Kończenie znajdujących się przy użyciu dostępnych danych wejściowych. Wszystkie operacje kolejnego odczytu może generować wyjątek, chyba że aktualizujesz dane wejściowe dla aparatu rozpoznawania.  
  
   
  
## Examples  
 Poniższy przykład wykonuje rozpoznawanie audio w formacie .wav pliku i zapisuje rozpoznany tekst do konsoli.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Strumień, zawierająca dane audio.</param>
        <summary>Konfiguruje <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiektów dla danych wejściowych ze strumienia, zawierającą dane formatu audio (wav) w przebiegu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania osiągnie koniec strumienia wejściowego, podczas operacji rozpoznawania, operacja rozpoznawania Kończenie znajdujących się przy użyciu dostępnych danych wejściowych. Wszystkie operacje kolejnego odczytu może generować wyjątek, chyba że aktualizujesz dane wejściowe dla aparatu rozpoznawania.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wykrywa dane wejściowe, którą można zidentyfikować jako mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy aparat rozpoznawania mowy ma algorytm rozróżnienie między wyciszenia i mowy. Gdy <xref:System.Speech.Recognition.SpeechRecognitionEngine> wykonuje operację rozpoznawania mowy zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> zdarzenie, kiedy jego algorytmu identyfikuje dane wejściowe na zamiana mowy. <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Właściwości skojarzonego <xref:System.Speech.Recognition.SpeechDetectedEventArgs> obiektu wskazuje lokalizację w strumieniu wejściowym, w przypadku wykrycia przez aparat rozpoznawania mowy. <xref:System.Speech.Recognition.SpeechRecognitionEngine> Zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> zdarzenie przed zgłasza wszystkich <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> zdarzenia.  
  
 Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metody.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsoli dotyczące wybierania początkowe i docelowe miast w locie. Aplikacja rozpoznaje fraz, takie jak "Chcę się z Miami do Chicago".  W przykładzie użyto <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> zdarzeń do raportu <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> każdego mowy czasu wykrycia.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> rozpoznał wyrazów, które mogą być składnik wielu wyrażeń ukończone w gramatyce.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Generuje wiele <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> zdarzenia, ponieważ próbuje zidentyfikować frazy danych wejściowych. Dostęp można uzyskać tekst częściowo rozpoznawanym frazy w <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> właściwość <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> obiekt obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> zdarzeń. Zazwyczaj obsługi tych zdarzeń jest użyteczna tylko w przypadku debugowania.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> pochodzi od klasy <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości i <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metody.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje fraz, takie jak "Display listę artystów w kategorii jazz". W przykładzie użyto <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> zdarzenie, aby wyświetlić fragmenty niekompletne frazę w konsoli, jak zostaną rozpoznane.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> odbiera dane wejściowe, które nie pasuje do żadnego załadowane i włączone <see cref="T:System.Speech.Recognition.Grammar" /> obiektów.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania zgłasza to zdarzenie, gdy ustali, że dane wejściowe nie jest zgodna z wystarczający poziom zaufania załadowane i włączone <xref:System.Speech.Recognition.Grammar> obiektów. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Właściwość <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> zawiera odrzuconych <xref:System.Speech.Recognition.RecognitionResult> obiektu. Możesz użyć programu obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> zdarzenie, aby pobrać rozpoznawania <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> które zostały odrzucone i ich <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> wyniki.  
  
 Jeśli aplikacja wykorzystuje <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia, możesz zmodyfikować poziom ufności, w których mowy danych wejściowych jest zaakceptowane lub odrzucone z jednym z <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metody. Można modyfikować, jak rozpoznawanie mowy reaguje na nie mowy danych wejściowych przy użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje fraz, takie jak "Wyświetlić listę artystów kategorii jazz" lub "Wyświetlanie albumów gospel". W przykładzie użyto procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> zdarzenie, aby wyświetlić powiadomienie w konsoli, gdy mowy danych wejściowych nie można dopasować do zawartości gramatyki o wystarczającej ilości <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> do produkcji pomyślne rozpoznawanie. Program obsługi wyświetla również wynik rozpoznawania <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> które zostały odrzucone z powodu niskiej ufności wyniki.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wywołane, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> odbiera dane wejściowe, które pasuje do żadnej z załadowane i włączone <see cref="T:System.Speech.Recognition.Grammar" /> obiektów.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Może zainicjować operację rozpoznawanie przy użyciu jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody. Generuje aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeń, jeśli wykryje, że dane wejściowe pasuje do jednej z jej załadować <xref:System.Speech.Recognition.Grammar> obiektów przy użyciu odpowiedniego poziomu zaufania do stworzenia rozpoznawania. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Właściwość <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> zawiera zaakceptowane <xref:System.Speech.Recognition.RecognitionResult> obiektu. Programy obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia można uzyskać rozpoznawanym frazy, jak również lista rozpoznawania <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> z niższym oceny zaufania.  
  
 Jeśli aplikacja wykorzystuje <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia, możesz zmodyfikować poziom ufności, w których mowy danych wejściowych jest zaakceptowane lub odrzucone z jednym z <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metody.  Można modyfikować, jak rozpoznawanie mowy reaguje na nie mowy danych wejściowych przy użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości.  
  
 Gdy aparat rozpoznawania odbiera dane wejściowe, który odpowiada gramatykę, <xref:System.Speech.Recognition.Grammar> obiektów może zgłosić jego <xref:System.Speech.Recognition.Grammar.SpeechRecognized> zdarzeń. <xref:System.Speech.Recognition.Grammar> Obiektu <xref:System.Speech.Recognition.Grammar.SpeechRecognized> zdarzenie jest wywoływane przed aparatu rozpoznawania mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeń. Wszystkie zadania, które są specyficzne dla danego gramatyki powinny być zawsze realizowane przez program obsługi <xref:System.Speech.Recognition.Grammar.SpeechRecognized> zdarzeń.  
  
 Po utworzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> delegata, możesz zidentyfikować metody, która będzie obsługiwać zdarzenia. Aby skojarzyć zdarzenia z programu obsługi zdarzeń, należy dodać wystąpienie delegata zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [zdarzenia i delegatów](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową, która tworzy gramatykę rozpoznawania mowy, konstrukcje <xref:System.Speech.Recognition.Grammar> obiektu i ładuje je do <xref:System.Speech.Recognition.SpeechRecognitionEngine> przeprowadzić rozpoznawania. W przykładzie pokazano dane wejściowe mowy <xref:System.Speech.Recognition.SpeechRecognitionEngine>, wyniki rozpoznawania skojarzone i powiązanych zdarzeń wywołanych przez aparat rozpoznawania mowy.  
  
 Wypowiadane dane wejściowe, takie jak "Chcę się z Chicago do Warszawa" spowoduje wyzwolenie <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeń. Nie wywoła wypowiedzi frazę "Podnoszenia me z Houston do Chicago" <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeń.  
  
 W przykładzie użyto procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenie, aby wyświetlić pomyślnie rozpoznane fraz i semantyka zawierają one w konsoli.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Zwalnia wszystkie <see cref="T:System.Speech.Recognition.Grammar" /> obiektów z aparatu rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania aktualnie jest ładowana <xref:System.Speech.Recognition.Grammar> asynchronicznie, ta metoda czeka, aż do <xref:System.Speech.Recognition.Grammar> jest ładowany, zanim wszystkie zwalnia <xref:System.Speech.Recognition.Grammar> obiekty <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia.  
  
 Aby zwolnić określonego gramatyki, należy użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje synchroniczne ładowanie i zwalnianie z gramatyki rozpoznawania mowy.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Obiekt gramatyki do zwolnienia.</param>
        <summary>Zwalnia określony <see cref="T:System.Speech.Recognition.Grammar" /> obiektu z <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wystąpienia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli działa aparat rozpoznawania, aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> wstrzymać <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia przed ładowania, zwalnianie, włączanie lub wyłączanie <xref:System.Speech.Recognition.Grammar> obiektu. Aby zwolnić wszystkie <xref:System.Speech.Recognition.Grammar> obiekty, używają <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikację konsolową, która pokazuje synchroniczne ładowanie i zwalnianie z gramatyki rozpoznawania mowy.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">Gramatykę nie został załadowany w tym aparat rozpoznawania lub ten aparat rozpoznawania aktualnie Trwa ładowanie gramatyki asynchronicznie.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Aktualizuje wartości ustawienia aparat rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ustawienia rozpoznawania może zawierać ciąg, 64-bitowa liczba całkowita lub dane adresu pamięci. W poniższej tabeli opisano ustawienia, które są zdefiniowane dla interfejsu API rozpoznawania mowy firmy Microsoft (nieokreślone)-rozpoznawania zgodne. Poniższe ustawienia muszą mieć ten sam zakres każdego rozpoznawania, która obsługuje dane ustawienie. Zgodne nieokreślone rozpoznawania nie jest wymagany do obsługi tych ustawień i może obsługiwać inne ustawienia.  
  
|Nazwa|Opis|  
|----------|-----------------|  
|`ResourceUsage`|Określa użycie procesora CPU przez aparat rozpoznawania. Zakres jest z zakresu od 0 do 100. Wartością domyślną jest 50.|  
|`ResponseSpeed`|Określa długość wyciszenia na końcu danych wejściowych jednoznaczną przed aparatu rozpoznawania mowy zakończeniem operacji rozpoznawania. Zakres jest z zakresu od 0 do 10 000 milisekund (ms). To ustawienie odpowiada aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> właściwości. Domyślne = 150ms.|  
|`ComplexResponseSpeed`|Wskazuje długość wyciszenia w milisekundach (ms) na końcu niejednoznaczne dane wejściowe, przed aparatu rozpoznawania mowy zakończeniem operacji rozpoznawania. Zakres jest z zakresu od 0 do 10,000ms. To ustawienie odpowiada aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości. Domyślnie 500 MS.|  
|`AdaptationOn`|Wskazuje, czy dostosowanie modelu akustycznego jest włączone (wartość = `1`). lub Wył. (wartość = `0`). Wartość domyślna to `1` (dalej).|  
|`PersistedBackgroundAdaptation`|Wskazuje, czy dostosowania tła jest włączone (wartość = `1`). lub Wył. (wartość = `0`), będzie się powtarzał ustawienie w rejestrze. Wartość domyślna to `1` (dalej).|  
  
 Aby zwrócić jedno z ustawień, aparat rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> metody.  
  
 Z wyjątkiem produktów `PersistedBackgroundAdaptation`, wartości właściwości można ustawić przy użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metody obowiązywać tylko przez bieżące wystąpienie <xref:System.Speech.Recognition.SpeechRecognitionEngine>, po którym ich przywrócenie ustawień domyślnych.  
  
 Można modyfikować, jak rozpoznawanie mowy reaguje na nie mowy danych wejściowych przy użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Nazwa ustawienia do aktualizacji.</param>
        <param name="updatedValue">Nowa wartość dla ustawienia.</param>
        <summary>Aktualizuje określone ustawienie dla <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> przy użyciu określonej liczby całkowitej.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Z wyjątkiem produktów `PersistedBackgroundAdaptation`, wartości właściwości można ustawić przy użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metoda obowiązywać tylko przez bieżące wystąpienie <xref:System.Speech.Recognition.SpeechRecognitionEngine>, po którym ich przywrócenie ustawień domyślnych. Zobacz <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> opisy obsługiwanych ustawień.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikację konsolową, która wyświetla wartości szereg ustawień zdefiniowanych przez aparat rozpoznawania, który obsługuje ustawień regionalnych en US. Przykład zaktualizowanie ustawień poziomu zaufania, a następnie kwerendy rozpoznawania, aby sprawdzić zaktualizowane wartości. Przykład generuje następujące dane wyjściowe.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> ciąg pusty ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Aparat rozpoznawania nie ma ustawienie o takiej nazwie.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nazwa ustawienia do aktualizacji.</param>
        <param name="updatedValue">Nowa wartość dla ustawienia.</param>
        <summary>Aktualizuje ustawienia aparatu rozpoznawania mowy określony za pomocą określona wartość ciągu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Z wyjątkiem produktów `PersistedBackgroundAdaptation`, wartości właściwości można ustawić przy użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metoda obowiązywać tylko przez bieżące wystąpienie <xref:System.Speech.Recognition.SpeechRecognitionEngine>, po którym ich przywrócenie ustawień domyślnych. Zobacz <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> opisy obsługiwanych ustawień.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> ciąg pusty ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Aparat rozpoznawania nie ma ustawienie o takiej nazwie.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>
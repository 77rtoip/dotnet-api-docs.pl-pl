<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata><Meta Name="ms.openlocfilehash" Value="eed89ae88fdd6e3eb5d993c63104666d18daaa47" /><Meta Name="ms.sourcegitcommit" Value="055a4a82a0b08bfbdc21bd1347fb71f7fe2c099e" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="pl-PL" /><Meta Name="ms.lasthandoff" Value="08/15/2019" /><Meta Name="ms.locfileid" Value="69231056" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Zapewnia dostęp do aparatu rozpoznawania mowy w procesie i zarządzania nim.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Dla dowolnego z zainstalowanych aparatów rozpoznawania mowy można utworzyć wystąpienie tej klasy. Aby uzyskać informacje o zainstalowanych aparatach rozpoznawania, użyj metody statycznej <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> .  
  
 Ta klasa jest używana do uruchamiania aparatów rozpoznawania mowy w procesie i zapewnia kontrolę nad różnymi aspektami rozpoznawania mowy w następujący sposób:  
  
-   Aby utworzyć aparat rozpoznawania mowy w procesie, użyj jednego z <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> konstruktorów.  
  
-   Aby zarządzać gramatykami <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> metod <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> , <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>,, i właściwości.  
  
-   Aby skonfigurować dane wejściowe do <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>aparatu rozpoznawania, użyj metody, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> .  
  
-   Aby przeprowadzić rozpoznawanie mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metody lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>  
  
-   Aby zmodyfikować sposób, w jaki rozpoznawanie obsługuje wyciszenie lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>nieoczekiwane dane <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> wejściowe, użyj właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, i.  
  
-   Aby zmienić liczbę elementów zastępczych zwracanych przez aparat rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> właściwości. Aparat rozpoznawania zwraca wyniki rozpoznawania w <xref:System.Speech.Recognition.RecognitionResult> obiekcie.  
  
-   Aby zsynchronizować zmiany w aparacie rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody. Aparat rozpoznawania używa więcej niż jednego wątku do wykonywania zadań.  
  
-   Aby emulować dane wejściowe do aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> metod i. <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Obiekt służy wyłącznie do korzystania z procesu, który tworzy wystąpienie obiektu. Z drugiej strony, <xref:System.Speech.Recognition.SpeechRecognizer> współużytkuje jeden aparat rozpoznawania z dowolną aplikacją, która chce jej używać.  
  
> [!NOTE]
>  Zawsze wywołuj <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> przed wydaniem ostatniego odwołania do aparatu rozpoznawania mowy. W przeciwnym razie używane zasoby nie zostaną zwolnione do momentu wywołania `Finalize` metody aparatu rozpoznawania elementów bezużytecznych.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy. Ponieważ w `Multiple` tym przykładzie używa trybu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody, wykonuje rozpoznawanie, dopóki nie zamkniesz okna konsoli lub nie zatrzymasz debugowania.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inicjuje nowe wystąpienie klasy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> klasy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Można skonstruować <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienie z dowolnego z następujących elementów:  
  
-   Domyślny aparat rozpoznawania mowy dla systemu  
  
-   Określony aparat rozpoznawania mowy określony przez użytkownika według nazwy  
  
-   Domyślny aparat rozpoznawania mowy dla określonych ustawień regionalnych  
  
-   Konkretny aparat rozpoznawania spełniający kryteria określone w <xref:System.Speech.Recognition.RecognizerInfo> obiekcie.  
  
 Zanim aparat rozpoznawania mowy zacznie rozpoznawać, należy załadować co najmniej jedną gramatykę rozpoznawania mowy i skonfigurować dane wejściowe dla aparatu rozpoznawania.  
  
 Aby załadować gramatykę, wywołaj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metodę <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> lub.  
  
 Aby skonfigurować wejście audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inicjuje nowe wystąpienie <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> klasy przy użyciu domyślnego aparatu rozpoznawania mowy dla systemu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Przed rozpoczęciem rozpoznawania mowy przez aparat rozpoznawania mowy należy załadować co najmniej jedną gramatykę rozpoznawania i skonfigurować dane wejściowe dla aparatu rozpoznawania.  
  
 Aby załadować gramatykę, wywołaj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metodę <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> lub.  
  
 Aby skonfigurować wejście audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Ustawienia regionalne wymagane przez aparat rozpoznawania mowy.</param>
        <summary>Inicjuje nowe wystąpienie <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> klasy przy użyciu domyślnego aparatu rozpoznawania mowy dla określonych ustawień regionalnych.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows i system. Speech API akceptują wszystkie prawidłowe kody krajów językowych. Aby przeprowadzić rozpoznawanie mowy przy użyciu języka określonego w `CultureInfo` argumencie, należy zainstalować aparat rozpoznawania mowy obsługujący ten kod w języku kraju. Aparaty rozpoznawania mowy dostarczane z systemem Microsoft Windows 7 współpracują z poniższymi kodami kraju języka.  
  
-   pl GB. Angielski (Zjednoczone Królestwo)  
  
-   en-US. Angielski (Stany Zjednoczone)  
  
-   de-DE. Niemiecki (Niemcy)  
  
-   es-ES. Hiszpański (Hiszpania)  
  
-   fr-FR. Francuski (Francja)  
  
-   ja-JP. Japoński (Japonia)  
  
-   zh-CN. Chiński (Chiny)  
  
-   zh-TW. Chiński (Tajwan)  
  
 Dozwolone są również dwuliterowe kody języka, takie jak "en", "fr" lub "es".  
  
 Zanim aparat rozpoznawania mowy zacznie rozpoznawać, należy załadować co najmniej jedną gramatykę rozpoznawania mowy i skonfigurować dane wejściowe dla aparatu rozpoznawania.  
  
 Aby załadować gramatykę, wywołaj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metodę <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> lub.  
  
 Aby skonfigurować wejście audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy, i inicjuje aparat rozpoznawania mowy dla ustawień regionalnych en-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Żadna z zainstalowanych funkcji rozpoznawania mowy nie obsługuje określonych ustawień regionalnych lub <paramref name="culture" /> jest kulturą niezmienną.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="Culture" />jest <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Informacje dotyczące określonego aparatu rozpoznawania mowy.</param>
        <summary>Inicjuje nowe wystąpienie <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> programu przy użyciu informacji <see cref="T:System.Speech.Recognition.RecognizerInfo" /> w obiekcie, aby określić aparat rozpoznawania, który ma być używany.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Dla dowolnego z zainstalowanych aparatów rozpoznawania mowy można utworzyć wystąpienie tej klasy. Aby uzyskać informacje o zainstalowanych aparatach rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metody.  
  
 Zanim aparat rozpoznawania mowy zacznie rozpoznawać, należy załadować co najmniej jedną gramatykę rozpoznawania mowy i skonfigurować dane wejściowe dla aparatu rozpoznawania.  
  
 Aby załadować gramatykę, wywołaj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metodę <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> lub.  
  
 Aby skonfigurować wejście audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy, i inicjuje aparat rozpoznawania mowy obsługujący język angielski.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Nazwa tokenu aparatu rozpoznawania mowy do użycia.</param>
        <summary>Inicjuje nowe wystąpienie <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> klasy z parametrem ciągu, który określa nazwę aparatu rozpoznawania do użycia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Nazwa tokenu aparatu rozpoznawania jest wartością <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> właściwości <xref:System.Speech.Recognition.RecognizerInfo> obiektu zwróconego przez <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> Właściwość aparatu rozpoznawania. Aby uzyskać kolekcję wszystkich zainstalowanych aparatów rozpoznawania, użyj metody statycznej <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> .  
  
 Zanim aparat rozpoznawania mowy zacznie rozpoznawać, należy załadować co najmniej jedną gramatykę rozpoznawania mowy i skonfigurować dane wejściowe dla aparatu rozpoznawania.  
  
 Aby załadować gramatykę, wywołaj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metodę <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> lub.  
  
 Aby skonfigurować wejście audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy, i tworzy wystąpienie aparatu rozpoznawania mowy 8,0 dla systemu Windows (angielski-US).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Nie zainstalowano aparatu rozpoznawania mowy z tą nazwą tokenu <paramref name="recognizerId" /> lub jest to ciąg pusty ("").</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="recognizerId" />jest <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera format dźwięku odbieranego przez <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Format dźwięku na wejściu do <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wystąpienia lub <see langword="null" /> Jeśli dane wejściowe nie są skonfigurowane lub ustawione na dane wejściowe o wartości null.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby skonfigurować wejście audio, użyj jednej z następujących metod:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 W poniższym <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> przykładzie pokazano, jak uzyskać i wyświetlić dane formatu audio.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera poziom audio odbieranego przez <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Poziom audio danych wejściowych aparatu rozpoznawania mowy, od 0 do 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wartość 0 oznacza ciszę, a 100 reprezentuje maksymalny wolumin wejściowy.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> raportuje poziom wejścia audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Wywołuje to zdarzenie wiele razy na sekundę. Częstotliwość, z jaką zdarzenie jest zgłaszane, zależy od komputera, na którym jest uruchomiona aplikacja.  
  
 Aby uzyskać poziom audio w czasie zdarzenia, należy użyć <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> właściwości skojarzonej. <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs> Aby uzyskać bieżący poziom audio danych wejściowych do aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> właściwości aparatu rozpoznawania.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład dodaje procedurę obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> zdarzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine> do obiektu. Program obsługi wyprowadza nowy poziom dźwięku do konsoli programu.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera bieżącą lokalizację w strumieniu audio generowanym przez urządzenie, które udostępnia dane wejściowe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Bieżąca lokalizacja w strumieniu audio generowanym przez urządzenie wejściowe.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Właściwość odwołuje się do pozycji urządzenia wejściowego w wygenerowanym strumieniu audio. Z kolei <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Właściwość odwołuje się do pozycji aparatu rozpoznawania w ramach wejścia audio. Te pozycje mogą być różne. Na przykład, jeśli aparat rozpoznawania odebrał dane wejściowe, dla których nie Wygenerowano jeszcze wyniku rozpoznawania, wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> właściwości jest mniejsza niż wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> właściwości.  
  
   
  
## Examples  
 W poniższym przykładzie aparat rozpoznawania mowy w procesie używa gramatyki dyktowania, aby dopasować dane wejściowe mowy. Program obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> zapisuje zdarzenia w konsoli programu <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> , gdy aparat rozpoznawania mowy wykrywa mowę na wejściu.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany po <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wykryciu problemu w sygnale audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać ten problem, użyj <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> właściwości skojarzonej <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>klasy.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie zdefiniowano procedurę obsługi zdarzeń, która zbiera informacje o <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> zdarzeniu.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera stan dźwięku odbieranego przez <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Stan wejścia audio do aparatu rozpoznawania mowy.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Właściwość reprezentuje stan dźwięku za pomocą elementu członkowskiego <xref:System.Speech.Recognition.AudioState> wyliczenia. <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany po zmianie stanu w dźwięku otrzymywanym przez <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać stan dźwięku w momencie zdarzenia, należy użyć <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> właściwości skojarzonej. <xref:System.Speech.Recognition.AudioStateChangedEventArgs> Aby uzyskać bieżący stan audio danych wejściowych do aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> właściwości aparatu rozpoznawania. Aby uzyskać więcej informacji na temat stanu audio, <xref:System.Speech.Recognition.AudioState> zobacz Wyliczenie.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie użyto procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> zdarzenia, aby napisać aparat rozpoznawania nowy <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> do konsoli <xref:System.Speech.Recognition.AudioState> przy każdej zmianie przy użyciu elementu członkowskiego wyliczenia.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia przedział czasu, w którym <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> akceptowane są dane wejściowe zawierające tylko hałas w tle, przed zakończeniem rozpoznawania.</summary>
        <value>Czas trwania przedziału czasu.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy aparat rozpoznawania mowy ma algorytm rozróżniania między ciszą a mową. Aparat rozpoznawania klasyfikuje jako szumy w tle wszystkie wyciszenie wejściowe, które nie jest zgodne z początkową regułą dowolnego z załadowanych i włączonych gramatyki rozpoznawania mowy. Jeśli aparat rozpoznawania odbiera tylko hałas w tle i wyciszenie w czasie interwału limitu czasu Babble, aparat rozpoznawania kończy tę operację rozpoznawania.  
  
-   W przypadku asynchronicznych operacji rozpoznawania aparat rozpoznawania wywołuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzenie, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> gdzie <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> właściwość jest `true`i właściwość jest `null`.  
  
-   W przypadku synchronicznych operacji rozpoznawania i emulacji aparat `null`rozpoznawania zwraca, a nie <xref:System.Speech.Recognition.RecognitionResult>prawidłowy.  
  
 Jeśli limit czasu Babble jest ustawiony na 0, aparat rozpoznawania nie wykona sprawdzenia limitu czasu Babble. Interwał limitu czasu może być dowolną wartością nieujemną. Wartość domyślna to 0 s.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy, która ustawia <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine> i przed inicjacją rozpoznawania mowy. Procedury obsługi dla informacji o zdarzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> aparatu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> rozpoznawania mowy i zdarzeń wyjściowych zdarzenia do konsoli programu, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> aby zademonstrować, <xref:System.Speech.Recognition.SpeechRecognitionEngine> jak właściwości mają wpływ na operacje rozpoznawania.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Ta właściwość jest ustawiona na wartość mniejszą niż 0 sekund.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Usuwa obiekt.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Usuwa obiekt.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing"><see langword="true" />Aby zwolnić zasoby zarządzane i niezarządzane; <see langword="false" /> do zwolnienia tylko zasobów niezarządzanych.</param>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Usuwa obiekt i zwalnia zasoby używane podczas sesji.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuluje dane wejściowe do aparatu rozpoznawania mowy przy użyciu tekstu zamiast dźwięku do synchronicznego rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody pomijają wprowadzanie danych audio systemu i zapewniają tekst do aparatu rozpoznawania <xref:System.String> jako obiekty lub <xref:System.Speech.Recognition.RecognizedWordUnit> tablicę obiektów. Może to być przydatne w przypadku testowania lub debugowania aplikacji lub gramatyki. Na przykład można użyć emulacji, aby określić, czy słowo znajduje się w gramatyce i jakie semantyki są zwracane po rozpoznaniu wyrazu. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Użyj metody, aby wyłączyć dane wejściowe audio w aparacie rozpoznawania mowy podczas operacji emulacji.  
  
 Aparat <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>rozpoznawania mowy wywołuje,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, tak jakby operacja rozpoznawania nie została emulowana. Aparat rozpoznawania ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako literały.  
  
> [!NOTE]
>  Obiekt wygenerowany przez aparat rozpoznawania mowy w odpowiedzi na emulowane dane wejściowe ma `null` wartość dla <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> właściwości. <xref:System.Speech.Recognition.RecognitionResult>  
  
 Aby emulować rozpoznawanie asynchroniczne, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Dane wejściowe dla operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie frazy do aparatu rozpoznawania mowy przy użyciu tekstu zamiast dźwięku do synchronicznego rozpoznawania mowy.</summary>
        <returns>Wynik operacji rozpoznawania lub <see langword="null" /> Jeśli operacja nie powiedzie się lub aparat rozpoznawania nie jest włączony.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>rozpoznawania mowy wywołuje,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, tak jakby operacja rozpoznawania nie została emulowana.  
  
 Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują wielkość liter i znaki w przypadku stosowania reguł gramatycznych do frazy wejściowej. Aby uzyskać więcej informacji na temat tego typu porównania, zobacz <xref:System.Globalization.CompareOptions> wartości <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> wyliczenia i <xref:System.Globalization.CompareOptions.IgnoreWidth>. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy oraz traktują znaki interpunkcyjne jako literały.  
  
   
  
## Examples  
 Poniższy przykład kodu jest częścią aplikacji konsolowej, która demonstruje emulowane dane wejściowe, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy. Przykład generuje następujące dane wyjściowe.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma załadowanych gramatyki rozpoznawania mowy.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" />jest ciągiem pustym ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Tablica jednostek programu Word, która zawiera dane wejściowe dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla emulowanej operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie określonych słów do aparatu rozpoznawania mowy, przy użyciu tekstu zamiast dźwięku do synchronicznego rozpoznawania mowy i określa, jak aparat rozpoznawania obsługuje porównanie Unicode między wyrazami a załadowanymi gramatykami rozpoznawania mowy.</summary>
        <returns>Wynik operacji rozpoznawania lub <see langword="null" /> Jeśli operacja nie powiedzie się lub aparat rozpoznawania nie jest włączony.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>rozpoznawania mowy wywołuje,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, tak jakby operacja rozpoznawania nie została emulowana.  
  
 Aparat rozpoznawania używa `compareOptions` , gdy stosuje reguły gramatyki do frazy wejściowej. Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują <xref:System.Globalization.CompareOptions.IgnoreCase> przypadek, <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Jeśli wartość lub jest obecna. Aparat rozpoznawania zawsze ignoruje szerokość znaków i nigdy nie ignoruje typu kana. Aparat rozpoznawania ignoruje także nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako literały. Aby uzyskać więcej informacji na temat szerokości znaków i typu kana, <xref:System.Globalization.CompareOptions> zobacz Wyliczenie.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma załadowanych gramatyki rozpoznawania mowy.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" />zawiera co najmniej jeden <see langword="null" /> element.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" />zawiera flagę <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />,,lub <see cref="F:System.Globalization.CompareOptions.StringSort" />. <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" /></exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Fraza wejściowa dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla emulowanej operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie frazy do aparatu rozpoznawania mowy, przy użyciu tekstu zamiast dźwięku do synchronicznego rozpoznawania mowy i określa, jak aparat rozpoznawania obsługuje porównanie Unicode między frazą a załadowane gramatyki rozpoznawania mowy.</summary>
        <returns>Wynik operacji rozpoznawania lub <see langword="null" /> Jeśli operacja nie powiedzie się lub aparat rozpoznawania nie jest włączony.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>rozpoznawania mowy wywołuje,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, tak jakby operacja rozpoznawania nie została emulowana.  
  
 Aparat rozpoznawania używa `compareOptions` , gdy stosuje reguły gramatyki do frazy wejściowej. Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują <xref:System.Globalization.CompareOptions.IgnoreCase> przypadek, <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Jeśli wartość lub jest obecna. Aparat rozpoznawania zawsze ignoruje szerokość znaków i nigdy nie ignoruje typu kana. Aparat rozpoznawania ignoruje także nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako literały. Aby uzyskać więcej informacji na temat szerokości znaków i typu kana, <xref:System.Globalization.CompareOptions> zobacz Wyliczenie.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma załadowanych gramatyki rozpoznawania mowy.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" />jest ciągiem pustym ("").</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" />zawiera flagę <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />,,lub <see cref="F:System.Globalization.CompareOptions.StringSort" />. <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" /></exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuluje dane wejściowe do aparatu rozpoznawania mowy przy użyciu tekstu zamiast dźwięku na potrzeby asynchronicznego rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody pomijają wprowadzanie danych audio systemu i zapewniają tekst do aparatu rozpoznawania <xref:System.String> jako obiekty lub <xref:System.Speech.Recognition.RecognizedWordUnit> tablicę obiektów. Może to być przydatne w przypadku testowania lub debugowania aplikacji lub gramatyki. Na przykład można użyć emulacji, aby określić, czy słowo znajduje się w gramatyce i jakie semantyki są zwracane po rozpoznaniu wyrazu. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Użyj metody, aby wyłączyć dane wejściowe audio w aparacie rozpoznawania mowy podczas operacji emulacji.  
  
 Aparat <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>rozpoznawania mowy wywołuje,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, tak jakby operacja rozpoznawania nie została emulowana. Gdy aparat rozpoznawania ukończy asynchroniczne operacje rozpoznawania, zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzenie. Aparat rozpoznawania ignoruje nowe wiersze i dodatkowy biały znak i traktuje znaki interpunkcyjne jako literały.  
  
> [!NOTE]
>  Obiekt wygenerowany przez aparat rozpoznawania mowy w odpowiedzi na emulowane dane wejściowe ma `null` wartość dla <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> właściwości. <xref:System.Speech.Recognition.RecognitionResult>  
  
 Aby emulować rozpoznawanie synchroniczne, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> metody.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Dane wejściowe dla operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie frazy do aparatu rozpoznawania mowy przy użyciu tekstu zamiast dźwięku na potrzeby asynchronicznego rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>rozpoznawania mowy wywołuje,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, tak jakby operacja rozpoznawania nie została emulowana. Gdy aparat rozpoznawania ukończy asynchroniczne operacje rozpoznawania, zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzenie.  
  
 Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują wielkość liter i znaki w przypadku stosowania reguł gramatycznych do frazy wejściowej. Aby uzyskać więcej informacji na temat tego typu porównania, zobacz <xref:System.Globalization.CompareOptions> wartości <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> wyliczenia i <xref:System.Globalization.CompareOptions.IgnoreWidth>. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy oraz traktują znaki interpunkcyjne jako literały.  
  
   
  
## Examples  
 Poniższy przykład kodu jest częścią aplikacji konsolowej, która pokazuje asynchroniczne emulowane dane wejściowe, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy. Przykład generuje następujące dane wyjściowe.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma załadowanych gramatyki rozpoznawania mowy lub aparat rozpoznawania ma asynchroniczną operację rozpoznawania, która nie została jeszcze ukończona.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" />jest ciągiem pustym ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Tablica jednostek programu Word, która zawiera dane wejściowe dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla emulowanej operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie określonych słów do aparatu rozpoznawania mowy przy użyciu tablicy <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> obiektów zamiast dźwięku na potrzeby asynchronicznego rozpoznawania mowy i określa, jak aparat rozpoznawania obsługuje porównanie Unicode między wyrazami a załadowanej mowy Rozpoznawanie gramatyki.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>rozpoznawania mowy wywołuje,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, tak jakby operacja rozpoznawania nie została emulowana. Gdy aparat rozpoznawania ukończy asynchroniczne operacje rozpoznawania, zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzenie.  
  
 Aparat rozpoznawania używa `compareOptions` , gdy stosuje reguły gramatyki do frazy wejściowej. Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują <xref:System.Globalization.CompareOptions.IgnoreCase> przypadek, <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Jeśli wartość lub jest obecna. Aparaty rozpoznawania zawsze ignorują szerokość znaków i nigdy nie ignorują typu kana. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy oraz traktują znaki interpunkcyjne jako literały. Aby uzyskać więcej informacji na temat szerokości znaków i typu kana, <xref:System.Globalization.CompareOptions> zobacz Wyliczenie.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma załadowanych gramatyki rozpoznawania mowy lub aparat rozpoznawania ma asynchroniczną operację rozpoznawania, która nie została jeszcze ukończona.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" />zawiera co najmniej jeden <see langword="null" /> element.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" />zawiera flagę <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />,,lub <see cref="F:System.Globalization.CompareOptions.StringSort" />. <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" /></exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Fraza wejściowa dla operacji rozpoznawania.</param>
        <param name="compareOptions">Bitowa kombinacja wartości wyliczenia, które opisują typ porównania do użycia dla emulowanej operacji rozpoznawania.</param>
        <summary>Emuluje wprowadzanie frazy do aparatu rozpoznawania mowy, przy użyciu tekstu zamiast dźwięku na potrzeby asynchronicznego rozpoznawania mowy i określa, jak aparat rozpoznawania obsługuje porównanie Unicode między frazą a załadowane gramatyki rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>rozpoznawania mowy wywołuje,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, tak jakby operacja rozpoznawania nie została emulowana. Gdy aparat rozpoznawania ukończy asynchroniczne operacje rozpoznawania, zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> zdarzenie.  
  
 Aparat rozpoznawania używa `compareOptions` , gdy stosuje reguły gramatyki do frazy wejściowej. Aparaty rozpoznawania, które są dostarczane z systemami Vista i Windows 7 ignorują <xref:System.Globalization.CompareOptions.IgnoreCase> przypadek, <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Jeśli wartość lub jest obecna. Aparaty rozpoznawania zawsze ignorują szerokość znaków i nigdy nie ignorują typu kana. Aparaty rozpoznawania ignorują również nowe wiersze i dodatkowe odstępy oraz traktują znaki interpunkcyjne jako literały. Aby uzyskać więcej informacji na temat szerokości znaków i typu kana, <xref:System.Globalization.CompareOptions> zobacz Wyliczenie.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Aparat rozpoznawania nie ma załadowanych gramatyki rozpoznawania mowy lub aparat rozpoznawania ma asynchroniczną operację rozpoznawania, która nie została jeszcze ukończona.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" />jest ciągiem pustym ("").</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" />zawiera flagę <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />,,lub <see cref="F:System.Globalization.CompareOptions.StringSort" />. <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" /></exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany, <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> gdy kończy asynchroniczne operacje rozpoznawania emulowanej danych wejściowych.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każda <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Metoda rozpoczyna asynchroniczne operacje rozpoznawania. Wywołuje zdarzenie w <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> momencie zakończenia operacji asynchronicznej. <xref:System.Speech.Recognition.SpeechRecognitionEngine>  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Operacja <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>może <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>podnieść zdarzenia, ,<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>i .<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Zdarzenie jest ostatnim zdarzeniem, które aparat rozpoznawania zgłasza dla danej operacji.  
  
 Jeśli emulacja emulowana zakończyła się pomyślnie, możesz uzyskać dostęp do wyniku rozpoznawania przy użyciu jednego z następujących elementów:  
  
-   Właściwość obiektu w programie obsługi zdarzenia. <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>Właściwość w <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> obiekcie programu obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia.  
  
 Jeśli emulacja emulowana zakończyła się <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> niepowodzeniem, zdarzenie nie zostanie <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> zgłoszone i będzie miało wartość null.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>pochodzi od <xref:System.ComponentModel.AsyncCompletedEventArgs>.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs>pochodzi od <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która ładuje gramatykę rozpoznawania mowy i pokazuje asynchroniczne emulowane dane wejściowe, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call matches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia interwał wyciszenia, który <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> zostanie zaakceptowany na końcu niejednoznacznych danych wejściowych przed zakończeniem operacji rozpoznawania.</summary>
        <value>Czas trwania interwału wyciszenia.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania mowy używa tego interwału limitu czasu, gdy dane wejściowe rozpoznawania są niejednoznaczne. Na przykład w przypadku gramatyki rozpoznawania mowy, która obsługuje rozpoznawanie "nowa gra" lub "nowa gra", "nowa gra" to niejednoznaczne dane wejściowe, a "nowa gra" to niejednoznaczne dane wejściowe.  
  
 Ta właściwość określa, jak długo aparat rozpoznawania mowy czeka na dodatkowe dane wejściowe przed zakończeniem operacji rozpoznawania. Interwał limitu czasu może mieć wartość od 0 do 10 sekund włącznie. Wartość domyślna to 150 milisekund.  
  
 Aby ustawić interwał limitu czasu dla niejednoznacznych danych wejściowych, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Użyj właściwości.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Ta właściwość jest ustawiona na wartość mniejszą niż 0 sekund lub większa niż 10 sekund.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia interwał wyciszenia, który <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> zostanie zaakceptowany na końcu niejednoznacznych danych wejściowych przed zakończeniem operacji rozpoznawania.</summary>
        <value>Czas trwania interwału wyciszenia.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania mowy używa tego interwału limitu czasu, gdy dane wejściowe rozpoznawania są niejednoznaczne. Na przykład w przypadku gramatyki rozpoznawania mowy, która obsługuje rozpoznawanie "nowa gra" lub "nowa gra", "nowa gra" to niejednoznaczne dane wejściowe, a "nowa gra" to niejednoznaczne dane wejściowe.  
  
 Ta właściwość określa, jak długo aparat rozpoznawania mowy czeka na dodatkowe dane wejściowe przed zakończeniem operacji rozpoznawania. Interwał limitu czasu może mieć wartość od 0 do 10 sekund włącznie. Wartość domyślna to 500 milisekund.  
  
 Aby ustawić interwał limitu czasu dla niejednoznacznych danych wejściowych, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Ta właściwość jest ustawiona na wartość mniejszą niż 0 sekund lub większa niż 10 sekund.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera kolekcję <see cref="T:System.Speech.Recognition.Grammar" /> obiektów, które są ładowane w tym <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wystąpieniu.</summary>
        <value>Kolekcja <see cref="T:System.Speech.Recognition.Grammar" /> obiektów.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Poniższy przykład wyprowadza informacje do konsoli dla każdej gramatyki rozpoznawania mowy, która jest aktualnie załadowana przez aparat rozpoznawania mowy.  
  
> [!IMPORTANT]
>  Skopiuj kolekcję gramatyki, aby uniknąć błędów, jeśli kolekcja zostanie zmodyfikowana, a ta metoda wylicza elementy kolekcji.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia przedział czasu, w którym <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> akceptowane są dane wejściowe zawierające tylko cisz przed finalizowaniem rozpoznawania.</summary>
        <value>Czas trwania interwału wyciszenia.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy aparat rozpoznawania mowy ma algorytm rozróżniania między ciszą a mową. Jeśli dane wejściowe aparatu rozpoznawania są wyciszenie w początkowym okresie limitu czasu, aparat rozpoznawania kończy tę operację rozpoznawania.  
  
-   W przypadku asynchronicznych operacji rozpoznawania i emulacji aparat rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> wywołuje zdarzenie, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> gdzie <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> właściwość jest `true`i właściwość jest `null`.  
  
-   W przypadku synchronicznych operacji rozpoznawania i emulacji aparat `null`rozpoznawania zwraca, a nie <xref:System.Speech.Recognition.RecognitionResult>prawidłowy.  
  
 Jeśli początkowy interwał limitu czasu wyciszenia jest ustawiony na 0, aparat rozpoznawania nie wykonuje początkowego sprawdzenia limitu czasu wyciszenia. Interwał limitu czasu może być dowolną wartością nieujemną. Wartość domyślna to 0 s.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy. Przykład ustawia <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> i przedinicjacjąrozpoznawaniamowy.<xref:System.Speech.Recognition.SpeechRecognitionEngine> Programy obsługi dla informacji o zdarzeniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> aparatu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> rozpoznawania mowy i zdarzeń wyjściowych zdarzenia do konsoli programu, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> aby zademonstrować, <xref:System.Speech.Recognition.SpeechRecognitionEngine> jak właściwości właściwości wpływają na operacje rozpoznawania.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Ta właściwość jest ustawiona na wartość mniejszą niż 0 sekund.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Zwraca informacje dla wszystkich zainstalowanych aparatów rozpoznawania mowy w bieżącym systemie.</summary>
        <returns>Kolekcja <see cref="T:System.Speech.Recognition.RecognizerInfo" /> obiektów w trybie tylko do odczytu, które opisują zainstalowane aparaty rozpoznawania.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać informacje o bieżącym aparacie rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> właściwości.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy. W przykładzie zastosowano kolekcję zwracaną przez <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metodę, aby znaleźć aparat rozpoznawania mowy obsługujący język angielski.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Obiekt gramatyki do załadowania.</param>
        <summary>Synchronicznie ładuje <see cref="T:System.Speech.Recognition.Grammar" /> obiekt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania zgłasza wyjątek, jeśli <xref:System.Speech.Recognition.Grammar> obiekt jest już załadowany, jest ładowany asynchronicznie lub nie został załadowany do żadnego aparatu rozpoznawania. Nie można załadować tego samego <xref:System.Speech.Recognition.Grammar> obiektu do wielu <xref:System.Speech.Recognition.SpeechRecognitionEngine>wystąpień. Zamiast tego należy utworzyć nowy <xref:System.Speech.Recognition.Grammar> obiekt dla każdego <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia.  
  
 Jeśli aparat rozpoznawania jest uruchomiony, aplikacje muszą korzystać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> z programu, aby wstrzymać wyszukiwarkę mowy przed załadowaniem, wyładowaniem, włączeniem lub wyłączeniem gramatyki.  
  
 Po załadowaniu gramatyki jest ona domyślnie włączona. Aby wyłączyć załadowana Gramatyka, należy <xref:System.Speech.Recognition.Grammar.Enabled%2A> użyć właściwości.  
  
 Aby załadować <xref:System.Speech.Recognition.Grammar> obiekt asynchronicznie, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Użyj metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy. Przykład tworzy <xref:System.Speech.Recognition.DictationGrammar> i ładuje do aparatu rozpoznawania mowy.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" />jest <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" />jest w nieprawidłowym stanie.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Gramatyka rozpoznawania mowy do załadowania.</param>
        <summary>Asynchronicznie ładuje gramatykę rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Gdy aparat rozpoznawania zakończy ładowanie <xref:System.Speech.Recognition.Grammar> obiektu, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> wywołuje zdarzenie. Aparat rozpoznawania zgłasza wyjątek, jeśli <xref:System.Speech.Recognition.Grammar> obiekt jest już załadowany, jest ładowany asynchronicznie lub nie został załadowany do żadnego aparatu rozpoznawania. Nie można załadować tego samego <xref:System.Speech.Recognition.Grammar> obiektu do wielu <xref:System.Speech.Recognition.SpeechRecognitionEngine>wystąpień. Zamiast tego należy utworzyć nowy <xref:System.Speech.Recognition.Grammar> obiekt dla każdego <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia.  
  
 Jeśli aparat rozpoznawania jest uruchomiony, aplikacje muszą korzystać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> z programu, aby wstrzymać wyszukiwarkę mowy przed załadowaniem, wyładowaniem, włączeniem lub wyłączeniem gramatyki.  
  
 Po załadowaniu gramatyki jest ona domyślnie włączona. Aby wyłączyć załadowana Gramatyka, należy <xref:System.Speech.Recognition.Grammar.Enabled%2A> użyć właściwości.  
  
 Aby synchronicznie załadować gramatykę rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metody.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" />jest <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" />jest w nieprawidłowym stanie.</exception>
        <exception cref="T:System.OperationCanceledException">Operacja asynchroniczna została anulowana.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany po <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> zakończeniu asynchronicznego ładowania <see cref="T:System.Speech.Recognition.Grammar" /> obiektu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Metoda aparatu rozpoznawania Inicjuje operację asynchroniczną. <xref:System.Speech.Recognition.SpeechRecognitionEngine> Wywołuje to zdarzenie po zakończeniu operacji. Aby uzyskać <xref:System.Speech.Recognition.Grammar> obiekt, który został załadowany przez aparat rozpoznawania, <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> Użyj właściwości skojarzonej <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>klasy. Aby uzyskać bieżące <xref:System.Speech.Recognition.Grammar> obiekty, które zostały załadowane przez aparat rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> właściwości aparatu rozpoznawania.  
  
 Jeśli aparat rozpoznawania jest uruchomiony, aplikacje muszą korzystać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> z programu, aby wstrzymać wyszukiwarkę mowy przed załadowaniem, wyładowaniem, włączeniem lub wyłączeniem gramatyki.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład tworzy aparat rozpoznawania mowy w procesie, a następnie tworzy dwa typy gramatyki do rozpoznawania określonych słów i akceptowania swobodnego dyktowania. Przykład tworzy <xref:System.Speech.Recognition.Grammar> obiekt z każdego wykonanej gramatyki rozpoznawania mowy, a następnie asynchronicznie <xref:System.Speech.Recognition.Grammar> ładuje obiekty do <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia. Programy obsługi dla aparatu rozpoznawania <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> i <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia zapisu w <xref:System.Speech.Recognition.Grammar> konsoli, nazwa obiektu, który został użyty do przeprowadzenia rozpoznawania i tekst wyniku rozpoznawania odpowiednio.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera lub ustawia maksymalną liczbę alternatywnych wyników <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> rozpoznawania zwracanych przez funkcję Return dla każdej operacji rozpoznawania.</summary>
        <value>Liczba alternatywnych wyników do zwrócenia.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Właściwość<xref:System.Speech.Recognition.RecognitionResult> klasy zawiera kolekcję obiektów,którereprezentująmożliweinterpretacjedanychwejściowych.<xref:System.Speech.Recognition.RecognizedPhrase>  
  
 Wartość domyślna dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> wynosi 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />jest ustawiona na wartość mniejszą niż 0.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nazwa ustawienia do zwrócenia.</param>
        <summary>Zwraca wartości ustawień dla aparatu rozpoznawania.</summary>
        <returns>Wartość ustawienia.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ustawienia aparatu rozpoznawania mogą zawierać ciąg, 64-bitową liczbę całkowitą lub dane adresu pamięci. W poniższej tabeli opisano ustawienia, które są zdefiniowane dla aparatu rozpoznawania zgodnego z programem Microsoft Speech API (SAPI). Poniższe ustawienia muszą mieć ten sam zakres dla każdego aparatu rozpoznawania, który obsługuje ustawienie. Aparat rozpoznawania zgodny z interfejsem SAPI nie jest wymagany do obsługi tych ustawień i może obsługiwać inne ustawienia.  
  
|Nazwa|Opis|  
|----------|-----------------|  
|`ResourceUsage`|Określa użycie procesora przez aparat rozpoznawania. Zakresem jest z zakresu od 0 do 100. Wartość domyślna to 50.|  
|`ResponseSpeed`|Wskazuje długość wyciszenia na końcu jednoznacznego wejścia, zanim aparat rozpoznawania mowy ukończy operację rozpoznawania. Zakresem jest z zakresu od 0 do 10 000 milisekund (MS). To ustawienie odpowiada <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> właściwości aparatu rozpoznawania.  Wartość domyślna = 150ms.|  
|`ComplexResponseSpeed`|Wskazuje długość wyciszenia na końcu niejednoznacznych danych wejściowych przed zakończeniem operacji rozpoznawania przez aparat rozpoznawania mowy. Zakresem jest z zakresu od 0 do 10, 000ms. To ustawienie odpowiada <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości aparatu rozpoznawania. Wartość domyślna = 500 ms.|  
|`AdaptationOn`|Wskazuje, czy adaptacja modelu akustycznego jest włączona (wartość `1`=) czy wyłączona (wartość `0`=). Wartość domyślna to `1` (włączone).|  
|`PersistedBackgroundAdaptation`|Wskazuje, czy adaptacja w tle jest włączona `1`(wartość =) czy wyłączona `0`(wartość =) i utrzymuje ustawienie w rejestrze. Wartość domyślna to `1` (włączone).|  
  
 Aby zaktualizować ustawienia dla aparatu rozpoznawania, użyj jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metod.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która wyprowadza wartości dla wielu ustawień zdefiniowanych dla aparatu rozpoznawania, który obsługuje ustawienia regionalne en-US. Przykład generuje następujące dane wyjściowe.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" />jest ciągiem pustym ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Aparat rozpoznawania nie ma ustawienia o tej nazwie.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Uruchamia synchroniczną operację rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Te metody wykonują pojedynczą, synchroniczną operację rozpoznawania. Aparat rozpoznawania wykonuje tę operację w odniesieniu do załadowanych i włączonych gramatyki rozpoznawania mowy.  
  
 Podczas wywołania tej metody aparat rozpoznawania może wywoływać następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Uruchamiany, gdy aparat rozpoznawania wykryje dane wejściowe, które mogą identyfikować jako mowę.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Uruchamiany, gdy dane wejściowe tworzą niejednoznaczne dopasowanie z jedną z aktywnych gramatyk.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Uruchamiany, gdy aparat rozpoznawania zakończy operację rozpoznawania.  
  
 Aparat rozpoznawania nie zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzenia podczas korzystania z jednej <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> z metod.  
  
 Metody zwracają obiekt lub`null` Jeśli operacja nie powiedzie się lub aparat rozpoznawania nie jest włączony. <xref:System.Speech.Recognition.RecognitionResult> <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>  
  
 Synchroniczna operacja rozpoznawania może zakończyć się niepowodzeniem z następujących powodów:  
  
-   Nie wykryto mowy przed upływem limitu <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> czasu dla `initialSilenceTimeout` właściwości lub lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> parametru <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metody.  
  
-   Aparat rozpoznawania wykrywa mowę, ale nie znalazł żadnych dopasowań w żadnym z załadowanych <xref:System.Speech.Recognition.Grammar> i włączonych obiektów.  
  
 Aby zmodyfikować sposób obsługi przez aparat rozpoznawania czasu mowy lub ciszi w odniesieniu do rozpoznawania, <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>Użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> i.  
  
 Przed przeprowadzeniem rozpoznawania <xref:System.Speech.Recognition.Grammar> musibyćzaładowanyconajmniejjedenobiekt.<xref:System.Speech.Recognition.SpeechRecognitionEngine> Aby załadować gramatykę rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metody lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>  
  
 Aby przeprowadzić rozpoznawanie asynchroniczne, należy użyć jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metod.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Wykonuje synchroniczną operację rozpoznawania mowy.</summary>
        <returns>Wynik rozpoznawania dla danych wejściowych lub <see langword="null" /> Jeśli operacja nie powiedzie się lub aparat rozpoznawania nie jest włączony.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda wykonuje pojedyncze operacje rozpoznawania. Aparat rozpoznawania wykonuje tę operację w odniesieniu do załadowanych i włączonych gramatyki rozpoznawania mowy.  
  
 Podczas wywołania tej metody aparat rozpoznawania może wywoływać następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Uruchamiany, gdy aparat rozpoznawania wykryje dane wejściowe, które mogą identyfikować jako mowę.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Uruchamiany, gdy dane wejściowe tworzą niejednoznaczne dopasowanie z jedną z aktywnych gramatyk.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Uruchamiany, gdy aparat rozpoznawania zakończy operację rozpoznawania.  
  
 Aparat rozpoznawania nie zgłasza zdarzenia w <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> przypadku korzystania z tej metody.  
  
 Metoda zwraca obiekt lub`null` Jeśli operacja nie powiedzie się. <xref:System.Speech.Recognition.RecognitionResult> <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>  
  
 Synchroniczna operacja rozpoznawania może zakończyć się niepowodzeniem z następujących powodów:  
  
-   Nie wykryto mowy przed upływem limitu czasu dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> właściwości lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>  
  
-   Aparat rozpoznawania wykrywa mowę, ale nie znalazł żadnych dopasowań w żadnym z załadowanych <xref:System.Speech.Recognition.Grammar> i włączonych obiektów.  
  
 Aby przeprowadzić rozpoznawanie asynchroniczne, należy użyć jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metod.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy. Przykład tworzy <xref:System.Speech.Recognition.DictationGrammar>, ładuje go do aparatu rozpoznawania mowy w procesie i wykonuje jedną operację rozpoznawania.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">Przedział czasu, w którym aparat rozpoznawania mowy akceptuje dane wejściowe zawierające tylko cisze przed finalizowaniem rozpoznawania.</param>
        <summary>Wykonuje synchroniczną operację rozpoznawania mowy z określonym początkowym limitem czasu wyciszenia.</summary>
        <returns>Wynik rozpoznawania dla danych wejściowych lub <see langword="null" /> Jeśli operacja nie powiedzie się lub aparat rozpoznawania nie jest włączony.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania mowy wykrywa mowę w przedziale czasu określonym przez `initialSilenceTimeout` argument, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> wykonuje pojedyncze operacje rozpoznawania, a następnie kończy pracę.  Parametr zastępuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Właściwość aparatu rozpoznawania. `initialSilenceTimeout`  
  
 Podczas wywołania tej metody aparat rozpoznawania może wywoływać następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Uruchamiany, gdy aparat rozpoznawania wykryje dane wejściowe, które mogą identyfikować jako mowę.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Uruchamiany, gdy dane wejściowe tworzą niejednoznaczne dopasowanie z jedną z aktywnych gramatyk.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Uruchamiany, gdy aparat rozpoznawania zakończy operację rozpoznawania.  
  
 Aparat rozpoznawania nie zgłasza zdarzenia w <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> przypadku korzystania z tej metody.  
  
 Metoda zwraca obiekt lub`null` Jeśli operacja nie powiedzie się. <xref:System.Speech.Recognition.RecognitionResult> <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>  
  
 Synchroniczna operacja rozpoznawania może zakończyć się niepowodzeniem z następujących powodów:  
  
-   Nie wykryto mowy przed upływem interwału limitu czasu dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> `initialSilenceTimeout` parametru lub.  
  
-   Aparat rozpoznawania wykrywa mowę, ale nie znalazł żadnych dopasowań w żadnym z załadowanych <xref:System.Speech.Recognition.Grammar> i włączonych obiektów.  
  
 Aby przeprowadzić rozpoznawanie asynchroniczne, należy użyć jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metod.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy. Przykład tworzy <xref:System.Speech.Recognition.DictationGrammar>, ładuje go do aparatu rozpoznawania mowy w procesie i wykonuje jedną operację rozpoznawania.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Uruchamia asynchroniczną operację rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Metody te wykonują jedną lub wiele asynchronicznych operacji rozpoznawania. Aparat rozpoznawania wykonuje każdą operację w odniesieniu do załadowanych i włączonych gramatyki rozpoznawania mowy.  
  
 Podczas wywołania tej metody aparat rozpoznawania może wywoływać następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Uruchamiany, gdy aparat rozpoznawania wykryje dane wejściowe, które mogą identyfikować jako mowę.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Uruchamiany, gdy dane wejściowe tworzą niejednoznaczne dopasowanie z jedną z aktywnych gramatyk.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Uruchamiany, gdy aparat rozpoznawania zakończy operację rozpoznawania.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Uruchamiany po <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> zakończeniu operacji.  
  
 Aby pobrać wynik asynchronicznej operacji rozpoznawania, Dołącz procedurę obsługi zdarzeń do <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia aparatu rozpoznawania. Aparat rozpoznawania wywołuje to zdarzenie za każdym razem, gdy pomyślnie zakończy operację rozpoznawania synchronicznego lub asynchronicznego. Jeśli rozpoznawanie nie powiodło się, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> właściwość obiektu, do którego można uzyskać dostęp <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> w programie obsługi zdarzenia, będzie miała `null`wartość.  
  
 Asynchroniczna operacja rozpoznawania może zakończyć się niepowodzeniem z następujących powodów:  
  
-   Nie wykryto mowy przed upływem limitu czasu dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> właściwości lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>  
  
-   Aparat rozpoznawania wykrywa mowę, ale nie znalazł żadnych dopasowań w żadnym z załadowanych <xref:System.Speech.Recognition.Grammar> i włączonych obiektów.  
  
-   Przed przeprowadzeniem rozpoznawania <xref:System.Speech.Recognition.Grammar> musibyćzaładowanyconajmniejjedenobiekt.<xref:System.Speech.Recognition.SpeechRecognitionEngine> Aby załadować gramatykę rozpoznawania mowy, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metody lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>  
  
-   Aby zmodyfikować sposób obsługi przez aparat rozpoznawania czasu mowy lub ciszi w odniesieniu do rozpoznawania, <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>Użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> i.  
  
-   Aby przeprowadzić rozpoznawanie synchroniczne, należy użyć jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metod.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Wykonuje pojedynczą, asynchroniczną operację rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda wykonuje pojedynczą, asynchroniczną operację rozpoznawania. Aparat rozpoznawania wykonuje operację na podstawie załadowanych i włączonych gramatyki rozpoznawania mowy.  
  
 Podczas wywołania tej metody aparat rozpoznawania może wywoływać następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Uruchamiany, gdy aparat rozpoznawania wykryje dane wejściowe, które mogą identyfikować jako mowę.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Uruchamiany, gdy dane wejściowe tworzą niejednoznaczne dopasowanie z jedną z aktywnych gramatyk.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Uruchamiany, gdy aparat rozpoznawania zakończy operację rozpoznawania.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Uruchamiany po <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> zakończeniu operacji.  
  
 Aby pobrać wynik asynchronicznej operacji rozpoznawania, Dołącz procedurę obsługi zdarzeń do <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia aparatu rozpoznawania. Aparat rozpoznawania wywołuje to zdarzenie za każdym razem, gdy pomyślnie zakończy operację rozpoznawania synchronicznego lub asynchronicznego. Jeśli rozpoznawanie nie powiodło się, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> właściwość obiektu, do którego można uzyskać dostęp <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> w programie obsługi zdarzenia, będzie miała `null`wartość.  
  
 Aby przeprowadzić rozpoznawanie synchroniczne, należy użyć jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metod.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe asynchroniczne rozpoznawanie mowy. Przykład tworzy <xref:System.Speech.Recognition.DictationGrammar>, ładuje go do aparatu rozpoznawania mowy w procesie i wykonuje jedną operację rozpoznawania asynchronicznego. Procedury obsługi zdarzeń są dołączone do zademonstrowania zdarzeń zgłaszanych przez aparat rozpoznawania podczas operacji.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Wskazuje, czy wykonać jedną lub wiele operacji rozpoznawania.</param>
        <summary>Wykonuje co najmniej jedną asynchroniczne operacje rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli `mode` <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> jest <xref:System.Speech.Recognition.RecognizeMode.Multiple>, aparat rozpoznawania kontynuuje asynchroniczne operacje rozpoznawania do momentu wywołania metody lub.  
  
 Podczas wywołania tej metody aparat rozpoznawania może wywoływać następujące zdarzenia:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Uruchamiany, gdy aparat rozpoznawania wykryje dane wejściowe, które mogą identyfikować jako mowę.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Uruchamiany, gdy dane wejściowe tworzą niejednoznaczne dopasowanie z jedną z aktywnych gramatyk.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Uruchamiany, gdy aparat rozpoznawania zakończy operację rozpoznawania.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Uruchamiany po <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> zakończeniu operacji.  
  
 Aby pobrać wynik asynchronicznej operacji rozpoznawania, Dołącz procedurę obsługi zdarzeń do <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia aparatu rozpoznawania. Aparat rozpoznawania wywołuje to zdarzenie za każdym razem, gdy pomyślnie zakończy operację rozpoznawania synchronicznego lub asynchronicznego. Jeśli rozpoznawanie nie powiodło się, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> właściwość obiektu, do którego można uzyskać dostęp <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> w programie obsługi zdarzenia, będzie miała `null`wartość.  
  
 Asynchroniczna operacja rozpoznawania może zakończyć się niepowodzeniem z następujących powodów:  
  
-   Nie wykryto mowy przed upływem limitu czasu dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> właściwości lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>  
  
-   Aparat rozpoznawania wykrywa mowę, ale nie znalazł żadnych dopasowań w żadnym z załadowanych <xref:System.Speech.Recognition.Grammar> i włączonych obiektów.  
  
 Aby przeprowadzić rozpoznawanie synchroniczne, należy użyć jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metod.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe asynchroniczne rozpoznawanie mowy. Przykład tworzy <xref:System.Speech.Recognition.DictationGrammar>, ładuje go do aparatu rozpoznawania mowy w procesie i wykonuje wiele asynchronicznych operacji rozpoznawania. Operacje asynchroniczne są anulowane po 30 sekundach. Procedury obsługi zdarzeń są dołączone do zademonstrowania zdarzeń zgłaszanych przez aparat rozpoznawania podczas operacji.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Kończy rozpoznawanie asynchroniczne bez oczekiwania na zakończenie bieżącej operacji rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda natychmiast kończy rozpoznawanie asynchroniczne. Jeśli bieżąca operacja rozpoznawania asynchronicznego otrzymuje dane wejściowe, dane wejściowe są obcinane i operacja zostanie ukończona z istniejącymi danymi wejściowymi. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Aparat rozpoznawania zgłasza zdarzenie lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> po <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> anulowaniu operacji asynchronicznej i <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> ustawia właściwość na `true`. Ta metoda anuluje asynchroniczne operacje zainicjowane przez <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody i. <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>  
  
 Aby zatrzymać asynchroniczne rozpoznawanie bez obcinania danych wejściowych, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje użycie <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> metody. Przykładem jest tworzenie i ładowanie gramatyki rozpoznawania mowy, inicjowanie asynchronicznej operacji rozpoznawania, a następnie wstrzymanie 2 sekund przed anulowaniem operacji. Aparat rozpoznawania odbiera dane wejściowe z pliku, c:\temp\audioinput\sample.wav. Procedury obsługi zdarzeń są dołączone do zademonstrowania zdarzeń zgłaszanych przez aparat rozpoznawania podczas operacji.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Wyłącza asynchroniczne rozpoznawanie po zakończeniu bieżącej operacji rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda kończy rozpoznawanie asynchroniczne bez obcinania danych wejściowych. Jeśli bieżąca operacja rozpoznawania asynchronicznego odbiera dane wejściowe, aparat rozpoznawania kontynuuje akceptowanie danych wejściowych do momentu zakończenia bieżącej operacji rozpoznawania. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Aparat rozpoznawania wywołuje zdarzenie lub <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> , gdy operacja asynchroniczna jest <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> zatrzymana, i <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> ustawia właściwość na `true`. Ta metoda powoduje zatrzymanie asynchronicznych operacji zainicjowanych przez <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metody i. <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>  
  
 Aby natychmiast anulować rozpoznawanie asynchroniczne tylko z istniejącymi danymi wejściowymi, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> Użyj metody.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje użycie <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> metody. Przykład tworzy i ładuje gramatykę rozpoznawania mowy, inicjuje asynchroniczną operację rozpoznawania, a następnie wstrzymuje 2 sekundy przed zatrzymaniem operacji. Aparat rozpoznawania odbiera dane wejściowe z pliku, c:\temp\audioinput\sample.wav. Procedury obsługi zdarzeń są dołączone do zademonstrowania zdarzeń zgłaszanych przez aparat rozpoznawania podczas operacji.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany, <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> gdy kończy asynchroniczne operacje rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Metoda<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>obiektuinicjuje asynchroniczne operacje rozpoznawania. <xref:System.Speech.Recognition.SpeechRecognitionEngine> Gdy aparat rozpoznawania kończy operację asynchroniczną, zgłasza to zdarzenie.  
  
 Korzystając z procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzenia, można <xref:System.Speech.Recognition.RecognitionResult> uzyskać dostęp <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> do obiektu. Jeśli rozpoznawanie nie powiodło się <xref:System.Speech.Recognition.RecognitionResult> , `null`będzie. Aby określić, czy przekroczenie limitu czasu lub przerwania w danych wejściowych audio spowodowało niepowodzenie rozpoznawania, można uzyskać <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>dostęp <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>do właściwości <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>dla, lub.  
  
 Aby uzyskać <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> więcej informacji, zobacz klasę.  
  
 Aby uzyskać szczegółowe informacje na temat najlepszych odrzuconych kandydatów rozpoznawania, Dołącz procedurę <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> obsługi dla zdarzenia.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje frazy, takie jak "Wyświetlanie listy artystów w kategorii Jazz" lub "Wyświetlanie albumów gospel". W przykładzie zastosowano procedurę obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> zdarzenia, aby wyświetlić informacje o wynikach rozpoznawania w konsoli programu.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera bieżącą lokalizację <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> w danych wejściowych audio, która jest przetwarzana.</summary>
        <value>Pozycja aparatu rozpoznawania w danych wejściowych audio, który jest przetwarzany.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pozycja audio jest specyficzna dla każdego aparatu rozpoznawania mowy. Wartość zerowa strumienia wejściowego jest określana, gdy jest włączona.  
  
 Właściwość odwołuje się <xref:System.Speech.Recognition.SpeechRecognitionEngine> do położenia obiektu w obrębie wejścia audio. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Z kolei <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Właściwość odwołuje się do pozycji urządzenia wejściowego w wygenerowanym strumieniu audio. Te pozycje mogą być różne. Na przykład, jeśli aparat rozpoznawania odebrał dane wejściowe, dla których nie Wygenerowano jeszcze wyniku rozpoznawania, wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> właściwości jest mniejsza niż wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> właściwości.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Pobiera informacje o bieżącym wystąpieniu <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Informacje o bieżącym aparacie rozpoznawania mowy.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aby uzyskać informacje o wszystkich zainstalowanych aparatach rozpoznawania mowy dla bieżącego systemu, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metody.  
  
   
  
## Examples  
 Poniższy przykład pobiera częściową listę danych dla bieżącego aparatu rozpoznawania mowy w procesie. Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany, gdy działa <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wstrzymanie, aby akceptować modyfikacje.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aplikacje muszą używać <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> programu, aby wstrzymać uruchomione <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienie przed zmodyfikowaniem jego ustawień <xref:System.Speech.Recognition.Grammar> lub jego obiektów. Wywołuje <xref:System.Speech.Recognition.SpeechRecognitionEngine> to zdarzenie, gdy jest gotowe do akceptowania modyfikacji.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Na przykład podczas wstrzymania można ładować, zwalniać, włączać i wyłączać <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.Grammar> obiekty oraz modyfikować wartości właściwości, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> . Aby uzyskać więcej informacji, zobacz <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodę.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono aplikację konsolową, która ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiekty. Aplikacja używa <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody, aby zażądać zatrzymania aparatu rozpoznawania mowy, aby można było odebrać aktualizację. Następnie aplikacja ładuje lub zwalnia <xref:System.Speech.Recognition.Grammar> obiekt.  
  
 W każdej aktualizacji program obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń zapisuje nazwę i stan aktualnie załadowanych <xref:System.Speech.Recognition.Grammar> obiektów do konsoli programu. Ponieważ gramatyki są ładowane i zwalniane, aplikacja najpierw rozpoznaje nazwy zwierząt farmy, a następnie nazwy zwierząt farmy i nazwy owoców, a następnie tylko nazwy owoców.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Żąda, aby aparat rozpoznawania wstrzymał aktualizację stanu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ta metoda umożliwia synchronizowanie zmian w aparacie rozpoznawania. Jeśli na przykład załadujesz lub zwolnisz gramatykę rozpoznawania mowy podczas przetwarzania danych wejściowych przez aparat rozpoznawania, Użyj tej metody <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> i zdarzenia, aby zsynchronizować zachowanie aplikacji ze stanem aparatu rozpoznawania.  
  
 Gdy ta metoda jest wywoływana, aparat rozpoznawania wstrzymuje lub kończy operacje asynchroniczne i generuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenie. Program <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> obsługi zdarzeń może następnie zmodyfikować stan aparatu rozpoznawania w ramach operacji rozpoznawania. Podczas obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń aparat rozpoznawania wstrzymuje się do momentu, gdy program obsługi zdarzeń zwróci wartość.  
  
> [!NOTE]
>  Jeśli dane wejściowe do aparatu rozpoznawania ulegną zmianie, zanim aparat rozpoznawania zgłasza <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenie, żądanie zostanie odrzucone.  
  
 Gdy ta metoda jest wywoływana:  
  
-   Jeśli aparat rozpoznawania nie przetwarza danych wejściowych, aparat rozpoznawania natychmiast wygeneruje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenie.  
  
-   Jeśli aparat rozpoznawania przetwarza dane wejściowe, które obejmują wyciszenie lub hałas w tle, aparat rozpoznawania wstrzymuje operację rozpoznawania i <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> generuje zdarzenie.  
  
-   Jeśli aparat rozpoznawania przetwarza dane wejściowe, które nie składają się z podkładu lub szumu w tle, aparat rozpoznawania wykonuje operację rozpoznawania, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> a następnie generuje zdarzenie.  
  
 Gdy aparat rozpoznawania obsługuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzenie:  
  
-   Aparat rozpoznawania nie przetwarza danych wejściowych, a wartość <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> właściwości pozostaje taka sama.  
  
-   Aparat rozpoznawania w dalszym ciągu zbiera dane wejściowe i może zmienić <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> wartość właściwości.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Żąda, aby aparat rozpoznawania wstrzymał aktualizację stanu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Po wygenerowaniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> przez aparat rozpoznawania zdarzeń <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> ma `null`wartość.  
  
 Aby podać token użytkownika, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Aby określić przesunięcie pozycji audio, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono aplikację konsolową, która ładuje i zwalnia <xref:System.Speech.Recognition.Grammar> obiekty. Aplikacja używa <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody, aby zażądać zatrzymania aparatu rozpoznawania mowy, aby można było odebrać aktualizację. Następnie aplikacja ładuje lub zwalnia <xref:System.Speech.Recognition.Grammar> obiekt.  
  
 W każdej aktualizacji program obsługi <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> zdarzeń zapisuje nazwę i stan aktualnie załadowanych <xref:System.Speech.Recognition.Grammar> obiektów do konsoli programu. Ponieważ gramatyki są ładowane i zwalniane, aplikacja najpierw rozpoznaje nazwy zwierząt farmy, a następnie nazwy zwierząt farmy i nazwy owoców, a następnie tylko nazwy owoców.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informacje zdefiniowane przez użytkownika, które zawierają informacje dla operacji.</param>
        <summary>Żądania, które aparat rozpoznawania wstrzymuje w celu zaktualizowania jego stanu i udostępnia token użytkownika dla skojarzonego zdarzenia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Po wygenerowaniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> przez aparat rozpoznawania zdarzeń <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> zawiera wartość `userToken` parametru.  
  
 Aby określić przesunięcie pozycji audio, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metody.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informacje zdefiniowane przez użytkownika, które zawierają informacje dla operacji.</param>
        <param name="audioPositionAheadToRaiseUpdate">Przesunięcie od bieżącego <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> , aby opóźnić żądanie.</param>
        <summary>Żądania, które aparat rozpoznawania wstrzymuje w celu zaktualizowania stanu i zawiera przesunięcie i token użytkownika dla skojarzonego zdarzenia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania nie inicjuje żądania aktualizacji aparatu rozpoznawania do momentu, gdy <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> aparat rozpoznawania jest <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> równy `audioPositionAheadToRaiseUpdate`bieżącemu znakowi Plus.  
  
 Po wygenerowaniu <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> przez aparat rozpoznawania zdarzeń <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Właściwość <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> zawiera wartość `userToken` parametru.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Strumień wejściowy audio.</param>
        <param name="audioFormat">Format wejścia audio.</param>
        <summary>Konfiguruje <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiekt do odbierania danych wejściowych ze strumienia audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania osiągnie koniec strumienia wejściowego podczas operacji rozpoznawania, operacja rozpoznawania kończy się z dostępnymi danymi wejściowymi. Wszelkie kolejne operacje rozpoznawania mogą generować wyjątek, chyba że dane wejściowe zostaną zaktualizowane do aparatu rozpoznawania.  
  
   
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy. W przykładzie używane są dane wejściowe z pliku audio, przykład. wav, który zawiera frazy "testowanie testów 1 2 3" i "Mister Cooper", oddzielone pauzą. Przykład generuje następujące dane wyjściowe.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Konfiguruje <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiekt do odbierania danych wejściowych z domyślnego urządzenia audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Poniższy przykład przedstawia część aplikacji konsolowej, która demonstruje podstawowe rozpoznawanie mowy. W przykładzie jest stosowane wyjście z domyślnego urządzenia audio, wykonuje wiele asynchronicznych operacji rozpoznawania i kończy się, gdy użytkownik zajmuje frazę "Exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Wyłącza dane wejściowe aparatu rozpoznawania mowy.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Skonfiguruj obiekt dla braku danych wejściowych przy <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> użyciu metod i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> lub podczas tymczasowego przełączania aparatu rozpoznawania. <xref:System.Speech.Recognition.SpeechRecognitionEngine>  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Ścieżka pliku do użycia jako dane wejściowe.</param>
        <summary>Konfiguruje <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiekt do odbierania danych wejściowych z pliku Wave audio format (. wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania osiągnie koniec pliku wejściowego podczas operacji rozpoznawania, operacja rozpoznawania kończy się z dostępnymi danymi wejściowymi. Wszelkie kolejne operacje rozpoznawania mogą generować wyjątek, chyba że dane wejściowe zostaną zaktualizowane do aparatu rozpoznawania.  
  
   
  
## Examples  
 Poniższy przykład wykonuje rozpoznawanie dźwięku w pliku WAV i zapisuje rozpoznany tekst w konsoli programu.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Strumień zawierający dane audio.</param>
        <summary>Konfiguruje <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> obiekt do odbierania danych wejściowych ze strumienia zawierającego dane Wave audio (wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania osiągnie koniec strumienia wejściowego podczas operacji rozpoznawania, operacja rozpoznawania kończy się z dostępnymi danymi wejściowymi. Wszelkie kolejne operacje rozpoznawania mogą generować wyjątek, chyba że dane wejściowe zostaną zaktualizowane do aparatu rozpoznawania.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> wykryje dane wejściowe, które mogą identyfikować jako mowę.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Każdy aparat rozpoznawania mowy ma algorytm rozróżniania między ciszą a mową. Gdy wykonuje operację rozpoznawania mowy, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> wywołuje zdarzenie, gdy jego algorytm identyfikuje dane wejściowe jako mowę. <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Właściwość skojarzonego<xref:System.Speech.Recognition.SpeechDetectedEventArgs> obiektu wskazuje lokalizację w strumieniu wejściowym, w którym aparat rozpoznawania wykrył mowę. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Wywołuje zdarzenie przed wyjęciem dowolnego zdarzenia, lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>Aby uzyskać więcej informacji <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> , zobacz metody, ,,i.<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej służącej do wybierania miejscowości początkowych i docelowych dla lotu. Aplikacja rozpoznaje frazy, takie jak "chcę mieć Miami do Chicago".  W przykładzie używa <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> się zdarzenia do <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> raportowania każdej wykrytej mowy.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> rozpoznany wyraz lub wyraz, który może być składnikiem wielu kompletnych fraz w gramatyce.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Generuje wiele<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> zdarzeń podczas próby zidentyfikowania frazy wejściowej. Można uzyskać dostęp do tekstu częściowo rozpoznanych fraz we <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> właściwości <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> obiektu <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> w programie obsługi zdarzenia. Zwykle obsługa tych zdarzeń jest przydatna tylko w przypadku debugowania.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs>pochodzi od <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Aby uzyskać więcej informacji, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Zobacz właściwości <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>i metody <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> i.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje zwroty, takie jak "Wyświetlanie listy artystów w kategorii Jazz". W przykładzie używa <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> się zdarzenia, aby wyświetlić niekompletne fragmenty frazy w konsoli w miarę ich rozpoznania.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> odbiera dane wejściowe, które nie są zgodne z żadnym z załadowanych i włączonych <see cref="T:System.Speech.Recognition.Grammar" /> obiektów.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Aparat rozpoznawania wywołuje to zdarzenie, jeśli określa, że dane wejściowe nie pasują do wystarczającej pewności żadnego z załadowanych i włączonych <xref:System.Speech.Recognition.Grammar> obiektów. Właściwość zawiera obiekt odrzucony <xref:System.Speech.Recognition.RecognitionResult>. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> Możesz użyć procedury obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> zdarzenia, aby pobrać rozpoznawanie <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> odrzucone i ich <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> wyniki.  
  
 Jeśli aplikacja używa <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia, można zmodyfikować poziom pewności, przy którym dane wejściowe mowy są akceptowane lub odrzucane przy użyciu jednej <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> z metod. Można zmodyfikować sposób, w jaki rozpoznawanie mowy reaguje na dane wejściowe inne niż mowę <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>przy <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>właściwości, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> , i.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład rozpoznaje frazy, takie jak "Wyświetlanie listy artystów w kategorii Jazz" lub "Wyświetlanie albumów gospel". W przykładzie zastosowano procedurę obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> zdarzenia, aby wyświetlić powiadomienie w konsoli, gdy dane wejściowe mowy nie można dopasować do zawartości gramatyki wystarczającej <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> do wygenerowania pomyślnego rozpoznania. Program obsługi wyświetla również wynik <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> rozpoznania, który został odrzucony z powodu oceny o niskiej pewności.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Uruchamiany, gdy <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> odbiera dane wejściowe, które pasują do dowolnego z <see cref="T:System.Speech.Recognition.Grammar" /> załadowanych i włączonych obiektów.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Można zainicjować operację rozpoznawania przy użyciu jednej z <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metod lub. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Aparat rozpoznawania wywołuje <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenie, jeśli określa, że dane wejściowe są zgodne z jednym <xref:System.Speech.Recognition.Grammar> z załadowanych obiektów z wystarczającym poziomem pewności, aby stanowić uznanie. Właściwość zawiera zaakceptowany <xref:System.Speech.Recognition.RecognitionResult>obiekt. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> Procedury obsługi <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> zdarzeń mogą uzyskać rozpoznaną frazę, a także listę rozpoznawania z niższymi wynikami pewności. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>  
  
 Jeśli aplikacja używa <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia, można zmodyfikować poziom pewności, przy którym dane wejściowe mowy są akceptowane lub odrzucane przy użyciu jednej <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> z metod.  Można zmodyfikować sposób, w jaki rozpoznawanie mowy reaguje na dane wejściowe inne niż mowę <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>przy <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>właściwości, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> , i.  
  
 Gdy aparat rozpoznawania odbiera dane wejściowe, które pasują do <xref:System.Speech.Recognition.Grammar> gramatyki, obiekt <xref:System.Speech.Recognition.Grammar.SpeechRecognized> może zgłosić swoje zdarzenie. Zdarzenie obiektu jest zgłaszane przed <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzeniem aparatu rozpoznawania mowy. <xref:System.Speech.Recognition.Grammar> <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Wszystkie zadania specyficzne dla określonej gramatyki powinny być zawsze wykonywane przez program obsługi <xref:System.Speech.Recognition.Grammar.SpeechRecognized> zdarzenia.  
  
 Podczas tworzenia <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> delegata należy określić metodę, która będzie obsługiwać zdarzenie. Aby skojarzyć zdarzenie z programem obsługi zdarzeń, Dodaj wystąpienie delegata do zdarzenia. Program obsługi zdarzeń jest wywoływany przy każdym wystąpieniu zdarzenia, o ile nie usunięto delegata. Aby uzyskać więcej informacji na temat delegatów obsługi zdarzeń, zobacz [Events and delegats](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która tworzy gramatykę rozpoznawania mowy, konstruuje <xref:System.Speech.Recognition.Grammar> obiekt i ładuje go do programu <xref:System.Speech.Recognition.SpeechRecognitionEngine> w celu przeprowadzenia rozpoznawania. Przykład ilustruje dane wejściowe mowy do <xref:System.Speech.Recognition.SpeechRecognitionEngine>, skojarzone wyniki rozpoznawania i skojarzone zdarzenia zgłoszone przez aparat rozpoznawania mowy.  
  
 Wypowiadane dane wejściowe, takie jak "chcę mieć od Chicago do Miami" spowodują <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> wyzwolenie zdarzenia. Mówiąc, że fraza "przylot ja z Houston do Chicago" nie spowoduje <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> wyzwolenia zdarzenia.  
  
 W przykładzie zastosowano procedurę obsługi dla <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> zdarzenia, aby wyświetlić pomyślnie rozpoznane frazy i semantykę, która zawiera w konsoli programu.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Zwalnia wszystkie <see cref="T:System.Speech.Recognition.Grammar" /> obiekty z aparatu rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania <xref:System.Speech.Recognition.Grammar> jest obecnie ładowany asynchronicznie, ta metoda czeka <xref:System.Speech.Recognition.Grammar> do momentu załadowania, zanim <xref:System.Speech.Recognition.Grammar> odładuje wszystkie obiekty z <xref:System.Speech.Recognition.SpeechRecognitionEngine> wystąpienia.  
  
 Aby zwolnić określoną gramatykę, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> metody.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono część aplikacji konsolowej, która pokazuje synchroniczne ładowanie i wyładowywanie gramatyki rozpoznawania mowy.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Obiekt gramatyki do zwolnienia.</param>
        <summary>Zwalnia określony <see cref="T:System.Speech.Recognition.Grammar" /> obiekt <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> z wystąpienia.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jeśli aparat rozpoznawania jest uruchomiony, aplikacje muszą użyć <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> do <xref:System.Speech.Recognition.SpeechRecognitionEngine> wstrzymania wystąpienia przed załadowaniem, wyładowaniem, <xref:System.Speech.Recognition.Grammar> włączeniem lub wyłączeniem obiektu. Aby zwolnić wszystkie <xref:System.Speech.Recognition.Grammar> obiekty, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> Użyj metody.  
  
   
  
## Examples  
 W poniższym przykładzie przedstawiono część aplikacji konsolowej, która pokazuje synchroniczne ładowanie i wyładowywanie gramatyki rozpoznawania mowy.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" />jest <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">Gramatyka nie została załadowana w tym aparacie rozpoznawania lub aparat rozpoznawania obecnie ładuje gramatykę asynchronicznie.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Aktualizuje wartość ustawienia dla aparatu rozpoznawania.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ustawienia aparatu rozpoznawania mogą zawierać ciąg, 64-bitową liczbę całkowitą lub dane adresu pamięci. W poniższej tabeli opisano ustawienia, które są zdefiniowane dla aparatu rozpoznawania zgodnego z programem Microsoft Speech API (SAPI). Poniższe ustawienia muszą mieć ten sam zakres dla każdego aparatu rozpoznawania, który obsługuje ustawienie. Aparat rozpoznawania zgodny z interfejsem SAPI nie jest wymagany do obsługi tych ustawień i może obsługiwać inne ustawienia.  
  
|Nazwa|Opis|  
|----------|-----------------|  
|`ResourceUsage`|Określa użycie procesora przez aparat rozpoznawania. Zakresem jest z zakresu od 0 do 100. Wartość domyślna to 50.|  
|`ResponseSpeed`|Wskazuje długość wyciszenia na końcu jednoznacznego wejścia, zanim aparat rozpoznawania mowy ukończy operację rozpoznawania. Zakresem jest z zakresu od 0 do 10 000 milisekund (MS). To ustawienie odpowiada <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> właściwości aparatu rozpoznawania. Wartość domyślna = 150ms.|  
|`ComplexResponseSpeed`|Wskazuje długość wyciszenia w milisekundach (MS) na końcu niejednoznacznych danych wejściowych przed ukończeniem operacji rozpoznawania przez aparat rozpoznawania mowy. Zakresem jest z zakresu od 0 do 10, 000ms. To ustawienie odpowiada <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> właściwości aparatu rozpoznawania. Wartość domyślna = 500 ms.|  
|`AdaptationOn`|Wskazuje, czy adaptacja modelu akustycznego jest włączona (wartość `1`=) czy wyłączona (wartość `0`=). Wartość domyślna to `1` (włączone).|  
|`PersistedBackgroundAdaptation`|Wskazuje, czy adaptacja w tle jest włączona `1`(wartość =) czy wyłączona `0`(wartość =) i utrzymuje ustawienie w rejestrze. Wartość domyślna to `1` (włączone).|  
  
 Aby zwrócić jeden z ustawień aparatu rozpoznawania, użyj <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> metody.  
  
 Z wyjątkiem `PersistedBackgroundAdaptation`wartości właściwości ustawionych <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> przy użyciu metod pozostają w efekcie tylko <xref:System.Speech.Recognition.SpeechRecognitionEngine>dla bieżącego wystąpienia, po którym przywracają ustawienia domyślne.  
  
 Można zmodyfikować sposób, w jaki rozpoznawanie mowy reaguje na dane wejściowe inne niż mowę <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>przy <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>użyciu <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>właściwości, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> , i.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Nazwa ustawienia do zaktualizowania.</param>
        <param name="updatedValue">Nowa wartość ustawienia.</param>
        <summary>Aktualizuje określone ustawienie dla <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> z określoną wartością całkowitą.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Z wyjątkiem `PersistedBackgroundAdaptation`wartości właściwości ustawionych <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> przy użyciu metody pozostają w efekcie tylko <xref:System.Speech.Recognition.SpeechRecognitionEngine>dla bieżącego wystąpienia, po którym przywracają ustawienia domyślne. Opis <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> obsługiwanych ustawień można znaleźć w temacie.  
  
   
  
## Examples  
 Poniższy przykład jest częścią aplikacji konsolowej, która wyprowadza wartości dla wielu ustawień zdefiniowanych dla aparatu rozpoznawania, który obsługuje ustawienia regionalne en-US. Przykład aktualizuje ustawienia poziomu ufności, a następnie wysyła zapytanie do aparatu rozpoznawania w celu sprawdzenia zaktualizowanych wartości. Przykład generuje następujące dane wyjściowe.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" />jest ciągiem pustym ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Aparat rozpoznawania nie ma ustawienia o tej nazwie.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nazwa ustawienia do zaktualizowania.</param>
        <param name="updatedValue">Nowa wartość ustawienia.</param>
        <summary>Aktualizuje określone ustawienie aparatu rozpoznawania mowy o określonej wartości ciągu.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Z wyjątkiem `PersistedBackgroundAdaptation`wartości właściwości ustawionych <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> przy użyciu metody pozostają w efekcie tylko <xref:System.Speech.Recognition.SpeechRecognitionEngine>dla bieżącego wystąpienia, po którym przywracają ustawienia domyślne. Opis <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> obsługiwanych ustawień można znaleźć w temacie.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" />jest <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" />jest ciągiem pustym ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Aparat rozpoznawania nie ma ustawienia o tej nazwie.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>
